{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def text_to_sentence_dataset(data):\n",
    "    # List to store dictionaries representing sentences with metadata\n",
    "    sentence_data = []\n",
    "\n",
    "    for sample in data:\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = re.findall(r'[^.!?]*[.!?]', sample['text'])\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Calculate the start and end indices of the sentence\n",
    "            start_index = sample['text'].index(sentence)\n",
    "            end_index = start_index + len(sentence)\n",
    "\n",
    "            # Adjust spans to sentence-level\n",
    "            adjusted_sentence_spans = []\n",
    "            for anno in sample['annotations']:\n",
    "                span_start = anno['start']\n",
    "                span_end = anno['end']\n",
    "                if span_start >= start_index and span_end <= end_index:\n",
    "                    adjusted_start = span_start - start_index -1\n",
    "                    adjusted_end = span_end - start_index -1\n",
    "                    adjusted_sentence_spans.append({'start':adjusted_start, 'end':adjusted_end, 'tag':anno['tag']})\n",
    "\n",
    "            #If there are no annotations for the sentence\n",
    "            if not adjusted_sentence_spans:\n",
    "                sentence_dict = {\n",
    "                    'text': sentence,\n",
    "                    'article_id': sample['article_id'],\n",
    "                    'lang': sample['lang'],\n",
    "                    'annotations': [],\n",
    "                    'label':0\n",
    "                }\n",
    "            else:\n",
    "                # Create a dictionary representing the sentence with metadata\n",
    "                sentence_dict = {\n",
    "                    'text': sentence,\n",
    "                    'article_id': sample['article_id'],\n",
    "                    'lang': sample['lang'],\n",
    "                    'annotations': adjusted_sentence_spans,\n",
    "                    'label':1\n",
    "                }\n",
    "\n",
    "            sentence_data.append(sentence_dict)\n",
    "\n",
    "    return sentence_data\n",
    "\n",
    "data = json.load(open('./data/train_gold/train_gold.json', 'r', encoding='utf8'))\n",
    "sentence_data = text_to_sentence_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_data)\n",
    "\n",
    "#Frasi in totale = 60686\n",
    "#Escludendo frasi senza annotazioni = 18108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train_gold/train_gold_sentences_TOTAL.json\", \"w\", encoding='utf-8') as outfile: \n",
    "    json.dump(sentence_data, outfile, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def text_to_sentence_dataset(sample):\n",
    "    print(sample)\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = re.findall(r'[^.!?]*[.!?]', sample['text'])\n",
    "\n",
    "    adjusted_spans = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Calculate the start and end indices of the sentence\n",
    "        start_index = sample['text'].index(sentence)\n",
    "        end_index = start_index + len(sentence)\n",
    "\n",
    "        # Adjust spans to sentence-level\n",
    "        adjusted_sentence_spans = []\n",
    "        for anno in sample['annotations']:\n",
    "            span_start = anno['start']\n",
    "            span_end = anno['end']\n",
    "            if span_start >= start_index and span_end <= end_index:\n",
    "                adjusted_start = span_start - start_index\n",
    "                adjusted_end = span_end - start_index\n",
    "                adjusted_sentence_spans.append((adjusted_start, adjusted_end))\n",
    "        adjusted_spans.append(adjusted_sentence_spans)\n",
    "\n",
    "    return sentences, adjusted_spans\n",
    "\n",
    "\n",
    "data = json.load(open('./data/train_gold/annotated_train_luca.json', 'r', encoding='utf8'))\n",
    "sentences, adjusted_spans = text_to_sentence_dataset(data[4])\n",
    "print(\"Adjusted Spans:\", adjusted_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def text_to_sentence_dataset(data):\n",
    "    # List to store dictionaries representing sentences with metadata\n",
    "    sentence_data = []\n",
    "\n",
    "    for sample in data:\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = re.findall(r'[^.!?]*[.!?]', sample['text'])\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Calculate the start and end indices of the sentence\n",
    "            start_index = sample['text'].index(sentence)\n",
    "            end_index = start_index + len(sentence)\n",
    "\n",
    "            # Adjust spans to sentence-level\n",
    "            adjusted_sentence_spans = []\n",
    "            for anno in sample['annotations']:\n",
    "                span_start = anno['start']\n",
    "                span_end = anno['end']\n",
    "                if span_start >= start_index and span_end <= end_index:\n",
    "                    adjusted_start = span_start - start_index -1\n",
    "                    adjusted_end = span_end - start_index -1\n",
    "                    adjusted_sentence_spans.append({'start':adjusted_start, 'end':adjusted_end, 'tag':anno['tag']})\n",
    "\n",
    "            # Create a dictionary representing the sentence with metadata\n",
    "            sentence_dict = {\n",
    "                'sentence': sentence,\n",
    "                'article_id': sample['article_id'],\n",
    "                'lang': sample['lang'],\n",
    "                'annotations': adjusted_sentence_spans\n",
    "            }\n",
    "\n",
    "            sentence_data.append(sentence_dict)\n",
    "\n",
    "    return sentence_data\n",
    "\n",
    "data = json.load(open('./data/train_gold/annotated_train_luca.json', 'r', encoding='utf8'))\n",
    "sentence_data = text_to_sentence_dataset(data)\n",
    "\n",
    "# Print the resulting sentence data\n",
    "for sentence_dict in sentence_data:\n",
    "    print(\"Sentence:\", sentence_dict['sentence'])\n",
    "    print(\"Article ID:\", sentence_dict['article_id'])\n",
    "    print(\"Language:\", sentence_dict['lang'])\n",
    "    print(\"Annotations:\", sentence_dict['annotations'])\n",
    "    print()  # Add an empty line for readability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
