{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Loaded_Language_precision', 0.06386554621848739),\n",
       "  ('Loaded_Language_recall', 0.11094890510948906),\n",
       "  ('Loaded_Language_f1-score', 0.08106666666666666),\n",
       "  ('Loaded_Language_support', 685)],\n",
       " [('Name_Calling-Labeling_precision', 0.12568306010928962),\n",
       "  ('Name_Calling-Labeling_recall', 0.20795660036166366),\n",
       "  ('Name_Calling-Labeling_f1-score', 0.1566757493188011),\n",
       "  ('Name_Calling-Labeling_support', 553)],\n",
       " [('Appeal_to_Popularity_precision', 0.13953488372093023),\n",
       "  ('Appeal_to_Popularity_recall', 0.06666666666666667),\n",
       "  ('Appeal_to_Popularity_f1-score', 0.09022556390977443),\n",
       "  ('Appeal_to_Popularity_support', 90)],\n",
       " [('Straw_Man_precision', 0.12903225806451613),\n",
       "  ('Straw_Man_recall', 0.05405405405405406),\n",
       "  ('Straw_Man_f1-score', 0.0761904761904762),\n",
       "  ('Straw_Man_support', 74)],\n",
       " [('Appeal_to_Fear-Prejudice_precision', 0.05389221556886228),\n",
       "  ('Appeal_to_Fear-Prejudice_recall', 0.04072398190045249),\n",
       "  ('Appeal_to_Fear-Prejudice_f1-score', 0.04639175257731958),\n",
       "  ('Appeal_to_Fear-Prejudice_support', 221)],\n",
       " [('Whataboutism_precision', 0.2727272727272727),\n",
       "  ('Whataboutism_recall', 0.14285714285714285),\n",
       "  ('Whataboutism_f1-score', 0.18749999999999997),\n",
       "  ('Whataboutism_support', 21)],\n",
       " [('Repetition_precision', 0.02857142857142857),\n",
       "  ('Repetition_recall', 0.0273972602739726),\n",
       "  ('Repetition_f1-score', 0.027972027972027972),\n",
       "  ('Repetition_support', 146)],\n",
       " [('Questioning_the_Reputation_precision', 0.21634615384615385),\n",
       "  ('Questioning_the_Reputation_recall', 0.17110266159695817),\n",
       "  ('Questioning_the_Reputation_f1-score', 0.19108280254777069),\n",
       "  ('Questioning_the_Reputation_support', 263.0)],\n",
       " [('False_Dilemma-No_Choice_precision', 0.125),\n",
       "  ('False_Dilemma-No_Choice_recall', 0.075),\n",
       "  ('False_Dilemma-No_Choice_f1-score', 0.09374999999999999),\n",
       "  ('False_Dilemma-No_Choice_support', 80)],\n",
       " [('Appeal_to_Time_precision', 0),\n",
       "  ('Appeal_to_Time_recall', 0),\n",
       "  ('Appeal_to_Time_f1-score', 0),\n",
       "  ('Appeal_to_Time_support', 46)],\n",
       " [('Exaggeration-Minimisation_precision', 0.05025125628140704),\n",
       "  ('Exaggeration-Minimisation_recall', 0.05434782608695652),\n",
       "  ('Exaggeration-Minimisation_f1-score', 0.05221932114882506),\n",
       "  ('Exaggeration-Minimisation_support', 184)],\n",
       " [('Appeal_to_Authority_precision', 0.0945945945945946),\n",
       "  ('Appeal_to_Authority_recall', 0.05785123966942149),\n",
       "  ('Appeal_to_Authority_f1-score', 0.07179487179487179),\n",
       "  ('Appeal_to_Authority_support', 121.0)],\n",
       " [('Flag_Waving_precision', 0.04878048780487805),\n",
       "  ('Flag_Waving_recall', 0.035398230088495575),\n",
       "  ('Flag_Waving_f1-score', 0.041025641025641026),\n",
       "  ('Flag_Waving_support', 113)],\n",
       " [('Appeal_to_Hypocrisy_precision', 0.15294117647058825),\n",
       "  ('Appeal_to_Hypocrisy_recall', 0.10833333333333334),\n",
       "  ('Appeal_to_Hypocrisy_f1-score', 0.12682926829268293),\n",
       "  ('Appeal_to_Hypocrisy_support', 120.0)],\n",
       " [('Appeal_to_Values_precision', 0.14457831325301204),\n",
       "  ('Appeal_to_Values_recall', 0.07947019867549669),\n",
       "  ('Appeal_to_Values_f1-score', 0.10256410256410257),\n",
       "  ('Appeal_to_Values_support', 151)],\n",
       " [('Guilt_by_Association_precision', 0.09090909090909091),\n",
       "  ('Guilt_by_Association_recall', 0.045454545454545456),\n",
       "  ('Guilt_by_Association_f1-score', 0.060606060606060615),\n",
       "  ('Guilt_by_Association_support', 110)],\n",
       " [('Conversation_Killer_precision', 0.23636363636363636),\n",
       "  ('Conversation_Killer_recall', 0.21666666666666667),\n",
       "  ('Conversation_Killer_f1-score', 0.22608695652173916),\n",
       "  ('Conversation_Killer_support', 120)],\n",
       " [('Red_Herring_precision', 0),\n",
       "  ('Red_Herring_recall', 0),\n",
       "  ('Red_Herring_f1-score', 0),\n",
       "  ('Red_Herring_support', 53)],\n",
       " [('Consequential_Oversimplification_precision', 0.2222222222222222),\n",
       "  ('Consequential_Oversimplification_recall', 0.09900990099009901),\n",
       "  ('Consequential_Oversimplification_f1-score', 0.13698630136986303),\n",
       "  ('Consequential_Oversimplification_support', 101)],\n",
       " [('Doubt_precision', 0.21074380165289255),\n",
       "  ('Doubt_recall', 0.1608832807570978),\n",
       "  ('Doubt_f1-score', 0.18246869409660108),\n",
       "  ('Doubt_support', 634)],\n",
       " [('Slogans_precision', 0.2926829268292683),\n",
       "  ('Slogans_recall', 0.19834710743801653),\n",
       "  ('Slogans_f1-score', 0.23645320197044334),\n",
       "  ('Slogans_support', 121)],\n",
       " [('Causal_Oversimplification_precision', 0.038461538461538464),\n",
       "  ('Causal_Oversimplification_recall', 0.013513513513513514),\n",
       "  ('Causal_Oversimplification_f1-score', 0.02),\n",
       "  ('Causal_Oversimplification_support', 148)],\n",
       " [('Obfuscation-Vagueness-Confusion_precision', 0.1282051282051282),\n",
       "  ('Obfuscation-Vagueness-Confusion_recall', 0.05154639175257732),\n",
       "  ('Obfuscation-Vagueness-Confusion_f1-score', 0.07352941176470588),\n",
       "  ('Obfuscation-Vagueness-Confusion_support', 97)]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = '/home/pgajo/checkthat24/checkthat24_DIT/models/M2/'\n",
    "results = []\n",
    "for subdir in os.listdir(path):\n",
    "    res = json.load(open(path + subdir + '/results.json'))\n",
    "    metrics = list(res.items())[1:5]\n",
    "    results.append(metrics)\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persuasion_technique</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>n_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Loaded_Language</td>\n",
       "      <td>0.063866</td>\n",
       "      <td>0.110949</td>\n",
       "      <td>0.081067</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Name_Calling-Labeling</td>\n",
       "      <td>0.125683</td>\n",
       "      <td>0.207957</td>\n",
       "      <td>0.156676</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Appeal_to_Popularity</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.090226</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Straw_Man</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Appeal_to_Fear-Prejudice</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.046392</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Whataboutism</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Repetition</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Questioning_the_Reputation</td>\n",
       "      <td>0.216346</td>\n",
       "      <td>0.171103</td>\n",
       "      <td>0.191083</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False_Dilemma-No_Choice</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Appeal_to_Time</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Exaggeration-Minimisation</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Appeal_to_Authority</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.057851</td>\n",
       "      <td>0.071795</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Flag_Waving</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>0.041026</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Appeal_to_Hypocrisy</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.126829</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Appeal_to_Values</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.079470</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Guilt_by_Association</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Conversation_Killer</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.226087</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Red_Herring</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Consequential_Oversimplification</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Doubt</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.160883</td>\n",
       "      <td>0.182469</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Slogans</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>0.236453</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Causal_Oversimplification</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Obfuscation-Vagueness-Confusion</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                persuasion_technique  precision    recall        f1  n_samples\n",
       "0                    Loaded_Language   0.063866  0.110949  0.081067        685\n",
       "1              Name_Calling-Labeling   0.125683  0.207957  0.156676        553\n",
       "2               Appeal_to_Popularity   0.139535  0.066667  0.090226         90\n",
       "3                          Straw_Man   0.129032  0.054054  0.076190         74\n",
       "4           Appeal_to_Fear-Prejudice   0.053892  0.040724  0.046392        221\n",
       "5                       Whataboutism   0.272727  0.142857  0.187500         21\n",
       "6                         Repetition   0.028571  0.027397  0.027972        146\n",
       "7         Questioning_the_Reputation   0.216346  0.171103  0.191083        263\n",
       "8            False_Dilemma-No_Choice   0.125000  0.075000  0.093750         80\n",
       "9                     Appeal_to_Time   0.000000  0.000000  0.000000         46\n",
       "10         Exaggeration-Minimisation   0.050251  0.054348  0.052219        184\n",
       "11               Appeal_to_Authority   0.094595  0.057851  0.071795        121\n",
       "12                       Flag_Waving   0.048780  0.035398  0.041026        113\n",
       "13               Appeal_to_Hypocrisy   0.152941  0.108333  0.126829        120\n",
       "14                  Appeal_to_Values   0.144578  0.079470  0.102564        151\n",
       "15              Guilt_by_Association   0.090909  0.045455  0.060606        110\n",
       "16               Conversation_Killer   0.236364  0.216667  0.226087        120\n",
       "17                       Red_Herring   0.000000  0.000000  0.000000         53\n",
       "18  Consequential_Oversimplification   0.222222  0.099010  0.136986        101\n",
       "19                             Doubt   0.210744  0.160883  0.182469        634\n",
       "20                           Slogans   0.292683  0.198347  0.236453        121\n",
       "21         Causal_Oversimplification   0.038462  0.013514  0.020000        148\n",
       "22   Obfuscation-Vagueness-Confusion   0.128205  0.051546  0.073529         97"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column names\n",
    "columns = ['persuasion_technique', 'precision', 'recall', 'f1', 'n_samples']\n",
    "\n",
    "# Initialize an empty list to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate through the original data and transform it\n",
    "for sublist in results:\n",
    "    technique_data = {}\n",
    "    for item in sublist:\n",
    "        key, value = item\n",
    "        if '_precision' in key:\n",
    "            technique = key.split('_precision')[0]\n",
    "            metric = 'precision'\n",
    "        elif '_recall' in key:\n",
    "            technique = key.split('_recall')[0]\n",
    "            metric = 'recall'\n",
    "        elif '_f1-score' in key:\n",
    "            technique = key.split('_f1-score')[0]\n",
    "            metric = 'f1-score'\n",
    "        elif '_support' in key:\n",
    "            technique = key.split('_support')[0]\n",
    "            metric = 'support'\n",
    "        if technique not in technique_data:\n",
    "            technique_data[technique] = {}\n",
    "        technique_data[technique][metric] = value\n",
    "    for technique, metrics in technique_data.items():\n",
    "        transformed_data.append([technique, metrics.get('precision', None), metrics.get('recall', None), metrics.get('f1-score', None), int(metrics.get('support', None))])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(transformed_data, columns=columns)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/pgajo/checkthat24/checkthat24_DIT/misc/results2.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
