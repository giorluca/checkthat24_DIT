Training model no. 14 of 23 for (14, 'Loaded_Language') persuasion technique...
{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}]}
{'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.1449814126394052
{'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.1802525832376579
{'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.265695067264574
{'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}, {'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.2969215491559086
{'micro_f1': 0.9208972604139002, 'precision': 0.9208972604139002, 'Loaded_Language_precision': 0.27763272410791995, 'Loaded_Language_recall': 0.38902439024390245, 'Loaded_Language_f1-score': 0.3240223463687151, 'Loaded_Language_support': 820.0, 'micro avg_precision': 0.27763272410791995, 'micro avg_recall': 0.38902439024390245, 'micro avg_f1-score': 0.3240223463687151, 'micro avg_support': 820.0, 'macro avg_precision': 0.27763272410791995, 'macro avg_recall': 0.38902439024390245, 'macro avg_f1-score': 0.3240223463687151, 'macro avg_support': 820.0, 'weighted avg_precision': 0.27763272410791995, 'weighted avg_recall': 0.38902439024390245, 'weighted avg_f1-score': 0.3240223463687151, 'weighted avg_support': 820.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 5}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}, {'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}, {'micro_f1': 0.9208972604139002, 'precision': 0.9208972604139002, 'Loaded_Language_precision': 0.27763272410791995, 'Loaded_Language_recall': 0.38902439024390245, 'Loaded_Language_f1-score': 0.3240223463687151, 'Loaded_Language_support': 820.0, 'micro avg_precision': 0.27763272410791995, 'micro avg_recall': 0.38902439024390245, 'micro avg_f1-score': 0.3240223463687151, 'micro avg_support': 820.0, 'macro avg_precision': 0.27763272410791995, 'macro avg_recall': 0.38902439024390245, 'macro avg_f1-score': 0.3240223463687151, 'macro avg_support': 820.0, 'weighted avg_precision': 0.27763272410791995, 'weighted avg_recall': 0.38902439024390245, 'weighted avg_f1-score': 0.3240223463687151, 'weighted avg_support': 820.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 5}]}
Best model updated: current epoch macro f1 = 0.3240223463687151
{'micro_f1': 0.9231036384808678, 'precision': 0.9231036384808678, 'Loaded_Language_precision': 0.29416884247171454, 'Loaded_Language_recall': 0.4300254452926209, 'Loaded_Language_f1-score': 0.34935400516795867, 'Loaded_Language_support': 786.0, 'micro avg_precision': 0.29416884247171454, 'micro avg_recall': 0.4300254452926209, 'micro avg_f1-score': 0.34935400516795867, 'micro avg_support': 786.0, 'macro avg_precision': 0.29416884247171454, 'macro avg_recall': 0.4300254452926209, 'macro avg_f1-score': 0.34935400516795867, 'macro avg_support': 786.0, 'weighted avg_precision': 0.29416884247171454, 'weighted avg_recall': 0.4300254452926209, 'weighted avg_f1-score': 0.34935400516795867, 'weighted avg_support': 786.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 6}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}, {'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}, {'micro_f1': 0.9208972604139002, 'precision': 0.9208972604139002, 'Loaded_Language_precision': 0.27763272410791995, 'Loaded_Language_recall': 0.38902439024390245, 'Loaded_Language_f1-score': 0.3240223463687151, 'Loaded_Language_support': 820.0, 'micro avg_precision': 0.27763272410791995, 'micro avg_recall': 0.38902439024390245, 'micro avg_f1-score': 0.3240223463687151, 'micro avg_support': 820.0, 'macro avg_precision': 0.27763272410791995, 'macro avg_recall': 0.38902439024390245, 'macro avg_f1-score': 0.3240223463687151, 'macro avg_support': 820.0, 'weighted avg_precision': 0.27763272410791995, 'weighted avg_recall': 0.38902439024390245, 'weighted avg_f1-score': 0.3240223463687151, 'weighted avg_support': 820.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 5}, {'micro_f1': 0.9231036384808678, 'precision': 0.9231036384808678, 'Loaded_Language_precision': 0.29416884247171454, 'Loaded_Language_recall': 0.4300254452926209, 'Loaded_Language_f1-score': 0.34935400516795867, 'Loaded_Language_support': 786.0, 'micro avg_precision': 0.29416884247171454, 'micro avg_recall': 0.4300254452926209, 'micro avg_f1-score': 0.34935400516795867, 'micro avg_support': 786.0, 'macro avg_precision': 0.29416884247171454, 'macro avg_recall': 0.4300254452926209, 'macro avg_f1-score': 0.34935400516795867, 'macro avg_support': 786.0, 'weighted avg_precision': 0.29416884247171454, 'weighted avg_recall': 0.4300254452926209, 'weighted avg_f1-score': 0.34935400516795867, 'weighted avg_support': 786.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 6}]}
Best model updated: current epoch macro f1 = 0.34935400516795867
{'micro_f1': 0.9187113117734785, 'precision': 0.9187113117734785, 'Loaded_Language_precision': 0.31505657093124456, 'Loaded_Language_recall': 0.4108967082860386, 'Loaded_Language_f1-score': 0.35665024630541875, 'Loaded_Language_support': 881.0, 'micro avg_precision': 0.31505657093124456, 'micro avg_recall': 0.4108967082860386, 'micro avg_f1-score': 0.35665024630541875, 'micro avg_support': 881.0, 'macro avg_precision': 0.31505657093124456, 'macro avg_recall': 0.4108967082860386, 'macro avg_f1-score': 0.35665024630541875, 'macro avg_support': 881.0, 'weighted avg_precision': 0.31505657093124456, 'weighted avg_recall': 0.4108967082860386, 'weighted avg_f1-score': 0.3566502463054187, 'weighted avg_support': 881.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 7}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}, {'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}, {'micro_f1': 0.9208972604139002, 'precision': 0.9208972604139002, 'Loaded_Language_precision': 0.27763272410791995, 'Loaded_Language_recall': 0.38902439024390245, 'Loaded_Language_f1-score': 0.3240223463687151, 'Loaded_Language_support': 820.0, 'micro avg_precision': 0.27763272410791995, 'micro avg_recall': 0.38902439024390245, 'micro avg_f1-score': 0.3240223463687151, 'micro avg_support': 820.0, 'macro avg_precision': 0.27763272410791995, 'macro avg_recall': 0.38902439024390245, 'macro avg_f1-score': 0.3240223463687151, 'macro avg_support': 820.0, 'weighted avg_precision': 0.27763272410791995, 'weighted avg_recall': 0.38902439024390245, 'weighted avg_f1-score': 0.3240223463687151, 'weighted avg_support': 820.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 5}, {'micro_f1': 0.9231036384808678, 'precision': 0.9231036384808678, 'Loaded_Language_precision': 0.29416884247171454, 'Loaded_Language_recall': 0.4300254452926209, 'Loaded_Language_f1-score': 0.34935400516795867, 'Loaded_Language_support': 786.0, 'micro avg_precision': 0.29416884247171454, 'micro avg_recall': 0.4300254452926209, 'micro avg_f1-score': 0.34935400516795867, 'micro avg_support': 786.0, 'macro avg_precision': 0.29416884247171454, 'macro avg_recall': 0.4300254452926209, 'macro avg_f1-score': 0.34935400516795867, 'macro avg_support': 786.0, 'weighted avg_precision': 0.29416884247171454, 'weighted avg_recall': 0.4300254452926209, 'weighted avg_f1-score': 0.34935400516795867, 'weighted avg_support': 786.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 6}, {'micro_f1': 0.9187113117734785, 'precision': 0.9187113117734785, 'Loaded_Language_precision': 0.31505657093124456, 'Loaded_Language_recall': 0.4108967082860386, 'Loaded_Language_f1-score': 0.35665024630541875, 'Loaded_Language_support': 881.0, 'micro avg_precision': 0.31505657093124456, 'micro avg_recall': 0.4108967082860386, 'micro avg_f1-score': 0.35665024630541875, 'micro avg_support': 881.0, 'macro avg_precision': 0.31505657093124456, 'macro avg_recall': 0.4108967082860386, 'macro avg_f1-score': 0.35665024630541875, 'macro avg_support': 881.0, 'weighted avg_precision': 0.31505657093124456, 'weighted avg_recall': 0.4108967082860386, 'weighted avg_f1-score': 0.3566502463054187, 'weighted avg_support': 881.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 7}]}
Best model updated: current epoch macro f1 = 0.35665024630541875
{'micro_f1': 0.9210402663997221, 'precision': 0.9210402663997221, 'Loaded_Language_precision': 0.2819843342036554, 'Loaded_Language_recall': 0.4432284541723666, 'Loaded_Language_f1-score': 0.3446808510638298, 'Loaded_Language_support': 731.0, 'micro avg_precision': 0.2819843342036554, 'micro avg_recall': 0.4432284541723666, 'micro avg_f1-score': 0.3446808510638298, 'micro avg_support': 731.0, 'macro avg_precision': 0.2819843342036554, 'macro avg_recall': 0.4432284541723666, 'macro avg_f1-score': 0.3446808510638298, 'macro avg_support': 731.0, 'weighted avg_precision': 0.2819843342036554, 'weighted avg_recall': 0.4432284541723666, 'weighted avg_f1-score': 0.3446808510638298, 'weighted avg_support': 731.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 8}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}, {'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}, {'micro_f1': 0.9208972604139002, 'precision': 0.9208972604139002, 'Loaded_Language_precision': 0.27763272410791995, 'Loaded_Language_recall': 0.38902439024390245, 'Loaded_Language_f1-score': 0.3240223463687151, 'Loaded_Language_support': 820.0, 'micro avg_precision': 0.27763272410791995, 'micro avg_recall': 0.38902439024390245, 'micro avg_f1-score': 0.3240223463687151, 'micro avg_support': 820.0, 'macro avg_precision': 0.27763272410791995, 'macro avg_recall': 0.38902439024390245, 'macro avg_f1-score': 0.3240223463687151, 'macro avg_support': 820.0, 'weighted avg_precision': 0.27763272410791995, 'weighted avg_recall': 0.38902439024390245, 'weighted avg_f1-score': 0.3240223463687151, 'weighted avg_support': 820.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 5}, {'micro_f1': 0.9231036384808678, 'precision': 0.9231036384808678, 'Loaded_Language_precision': 0.29416884247171454, 'Loaded_Language_recall': 0.4300254452926209, 'Loaded_Language_f1-score': 0.34935400516795867, 'Loaded_Language_support': 786.0, 'micro avg_precision': 0.29416884247171454, 'micro avg_recall': 0.4300254452926209, 'micro avg_f1-score': 0.34935400516795867, 'micro avg_support': 786.0, 'macro avg_precision': 0.29416884247171454, 'macro avg_recall': 0.4300254452926209, 'macro avg_f1-score': 0.34935400516795867, 'macro avg_support': 786.0, 'weighted avg_precision': 0.29416884247171454, 'weighted avg_recall': 0.4300254452926209, 'weighted avg_f1-score': 0.34935400516795867, 'weighted avg_support': 786.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 6}, {'micro_f1': 0.9187113117734785, 'precision': 0.9187113117734785, 'Loaded_Language_precision': 0.31505657093124456, 'Loaded_Language_recall': 0.4108967082860386, 'Loaded_Language_f1-score': 0.35665024630541875, 'Loaded_Language_support': 881.0, 'micro avg_precision': 0.31505657093124456, 'micro avg_recall': 0.4108967082860386, 'micro avg_f1-score': 0.35665024630541875, 'micro avg_support': 881.0, 'macro avg_precision': 0.31505657093124456, 'macro avg_recall': 0.4108967082860386, 'macro avg_f1-score': 0.35665024630541875, 'macro avg_support': 881.0, 'weighted avg_precision': 0.31505657093124456, 'weighted avg_recall': 0.4108967082860386, 'weighted avg_f1-score': 0.3566502463054187, 'weighted avg_support': 881.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 7}, {'micro_f1': 0.9210402663997221, 'precision': 0.9210402663997221, 'Loaded_Language_precision': 0.2819843342036554, 'Loaded_Language_recall': 0.4432284541723666, 'Loaded_Language_f1-score': 0.3446808510638298, 'Loaded_Language_support': 731.0, 'micro avg_precision': 0.2819843342036554, 'micro avg_recall': 0.4432284541723666, 'micro avg_f1-score': 0.3446808510638298, 'micro avg_support': 731.0, 'macro avg_precision': 0.2819843342036554, 'macro avg_recall': 0.4432284541723666, 'macro avg_f1-score': 0.3446808510638298, 'macro avg_support': 731.0, 'weighted avg_precision': 0.2819843342036554, 'weighted avg_recall': 0.4432284541723666, 'weighted avg_f1-score': 0.3446808510638298, 'weighted avg_support': 731.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 8}]}
{'micro_f1': 0.9235122270117878, 'precision': 0.9235122270117878, 'Loaded_Language_precision': 0.30548302872062666, 'Loaded_Language_recall': 0.41835518474374256, 'Loaded_Language_f1-score': 0.35311871227364183, 'Loaded_Language_support': 839.0, 'micro avg_precision': 0.30548302872062666, 'micro avg_recall': 0.41835518474374256, 'micro avg_f1-score': 0.35311871227364183, 'micro avg_support': 839.0, 'macro avg_precision': 0.30548302872062666, 'macro avg_recall': 0.41835518474374256, 'macro avg_f1-score': 0.35311871227364183, 'macro avg_support': 839.0, 'weighted avg_precision': 0.30548302872062666, 'weighted avg_recall': 0.41835518474374256, 'weighted avg_f1-score': 0.3531187122736419, 'weighted avg_support': 839.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 9}
{'results': [{'micro_f1': 0.8941755704917363, 'precision': 0.8941755704917363, 'Loaded_Language_precision': 0.0, 'Loaded_Language_recall': 0.0, 'Loaded_Language_f1-score': 0.0, 'Loaded_Language_support': 139.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 139.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 139.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 139.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 0}, {'micro_f1': 0.9071278269218983, 'precision': 0.9071278269218983, 'Loaded_Language_precision': 0.10182767624020887, 'Loaded_Language_recall': 0.25161290322580643, 'Loaded_Language_f1-score': 0.1449814126394052, 'Loaded_Language_support': 465.0, 'micro avg_precision': 0.10182767624020887, 'micro avg_recall': 0.25161290322580643, 'micro avg_f1-score': 0.1449814126394052, 'micro avg_support': 465.0, 'macro avg_precision': 0.10182767624020887, 'macro avg_recall': 0.25161290322580643, 'macro avg_f1-score': 0.1449814126394052, 'macro avg_support': 465.0, 'weighted avg_precision': 0.10182767624020887, 'weighted avg_recall': 0.25161290322580643, 'weighted avg_f1-score': 0.1449814126394052, 'weighted avg_support': 465.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 1}, {'micro_f1': 0.9135835257104333, 'precision': 0.9135835257104333, 'Loaded_Language_precision': 0.13664055700609226, 'Loaded_Language_recall': 0.26475548060708265, 'Loaded_Language_f1-score': 0.1802525832376579, 'Loaded_Language_support': 593.0, 'micro avg_precision': 0.13664055700609226, 'micro avg_recall': 0.26475548060708265, 'micro avg_f1-score': 0.1802525832376579, 'micro avg_support': 593.0, 'macro avg_precision': 0.13664055700609226, 'macro avg_recall': 0.26475548060708265, 'macro avg_f1-score': 0.1802525832376579, 'macro avg_support': 593.0, 'weighted avg_precision': 0.13664055700609226, 'weighted avg_recall': 0.26475548060708265, 'weighted avg_f1-score': 0.1802525832376579, 'weighted avg_support': 593.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 2}, {'micro_f1': 0.9162597805879589, 'precision': 0.9162597805879589, 'Loaded_Language_precision': 0.206266318537859, 'Loaded_Language_recall': 0.3732283464566929, 'Loaded_Language_f1-score': 0.265695067264574, 'Loaded_Language_support': 635.0, 'micro avg_precision': 0.206266318537859, 'micro avg_recall': 0.3732283464566929, 'micro avg_f1-score': 0.265695067264574, 'micro avg_support': 635.0, 'macro avg_precision': 0.206266318537859, 'macro avg_recall': 0.3732283464566929, 'macro avg_f1-score': 0.265695067264574, 'macro avg_support': 635.0, 'weighted avg_precision': 0.20626631853785898, 'weighted avg_recall': 0.3732283464566929, 'weighted avg_f1-score': 0.265695067264574, 'weighted avg_support': 635.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 3}, {'micro_f1': 0.9184252998018344, 'precision': 0.9184252998018345, 'Loaded_Language_precision': 0.26022628372497825, 'Loaded_Language_recall': 0.34566473988439306, 'Loaded_Language_f1-score': 0.2969215491559086, 'Loaded_Language_support': 865.0, 'micro avg_precision': 0.26022628372497825, 'micro avg_recall': 0.34566473988439306, 'micro avg_f1-score': 0.2969215491559086, 'micro avg_support': 865.0, 'macro avg_precision': 0.26022628372497825, 'macro avg_recall': 0.34566473988439306, 'macro avg_f1-score': 0.2969215491559086, 'macro avg_support': 865.0, 'weighted avg_precision': 0.26022628372497825, 'weighted avg_recall': 0.34566473988439306, 'weighted avg_f1-score': 0.2969215491559086, 'weighted avg_support': 865.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 4}, {'micro_f1': 0.9208972604139002, 'precision': 0.9208972604139002, 'Loaded_Language_precision': 0.27763272410791995, 'Loaded_Language_recall': 0.38902439024390245, 'Loaded_Language_f1-score': 0.3240223463687151, 'Loaded_Language_support': 820.0, 'micro avg_precision': 0.27763272410791995, 'micro avg_recall': 0.38902439024390245, 'micro avg_f1-score': 0.3240223463687151, 'micro avg_support': 820.0, 'macro avg_precision': 0.27763272410791995, 'macro avg_recall': 0.38902439024390245, 'macro avg_f1-score': 0.3240223463687151, 'macro avg_support': 820.0, 'weighted avg_precision': 0.27763272410791995, 'weighted avg_recall': 0.38902439024390245, 'weighted avg_f1-score': 0.3240223463687151, 'weighted avg_support': 820.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 5}, {'micro_f1': 0.9231036384808678, 'precision': 0.9231036384808678, 'Loaded_Language_precision': 0.29416884247171454, 'Loaded_Language_recall': 0.4300254452926209, 'Loaded_Language_f1-score': 0.34935400516795867, 'Loaded_Language_support': 786.0, 'micro avg_precision': 0.29416884247171454, 'micro avg_recall': 0.4300254452926209, 'micro avg_f1-score': 0.34935400516795867, 'micro avg_support': 786.0, 'macro avg_precision': 0.29416884247171454, 'macro avg_recall': 0.4300254452926209, 'macro avg_f1-score': 0.34935400516795867, 'macro avg_support': 786.0, 'weighted avg_precision': 0.29416884247171454, 'weighted avg_recall': 0.4300254452926209, 'weighted avg_f1-score': 0.34935400516795867, 'weighted avg_support': 786.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 6}, {'micro_f1': 0.9187113117734785, 'precision': 0.9187113117734785, 'Loaded_Language_precision': 0.31505657093124456, 'Loaded_Language_recall': 0.4108967082860386, 'Loaded_Language_f1-score': 0.35665024630541875, 'Loaded_Language_support': 881.0, 'micro avg_precision': 0.31505657093124456, 'micro avg_recall': 0.4108967082860386, 'micro avg_f1-score': 0.35665024630541875, 'micro avg_support': 881.0, 'macro avg_precision': 0.31505657093124456, 'macro avg_recall': 0.4108967082860386, 'macro avg_f1-score': 0.35665024630541875, 'macro avg_support': 881.0, 'weighted avg_precision': 0.31505657093124456, 'weighted avg_recall': 0.4108967082860386, 'weighted avg_f1-score': 0.3566502463054187, 'weighted avg_support': 881.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 7}, {'micro_f1': 0.9210402663997221, 'precision': 0.9210402663997221, 'Loaded_Language_precision': 0.2819843342036554, 'Loaded_Language_recall': 0.4432284541723666, 'Loaded_Language_f1-score': 0.3446808510638298, 'Loaded_Language_support': 731.0, 'micro avg_precision': 0.2819843342036554, 'micro avg_recall': 0.4432284541723666, 'micro avg_f1-score': 0.3446808510638298, 'micro avg_support': 731.0, 'macro avg_precision': 0.2819843342036554, 'macro avg_recall': 0.4432284541723666, 'macro avg_f1-score': 0.3446808510638298, 'macro avg_support': 731.0, 'weighted avg_precision': 0.2819843342036554, 'weighted avg_recall': 0.4432284541723666, 'weighted avg_f1-score': 0.3446808510638298, 'weighted avg_support': 731.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 8}, {'micro_f1': 0.9235122270117878, 'precision': 0.9235122270117878, 'Loaded_Language_precision': 0.30548302872062666, 'Loaded_Language_recall': 0.41835518474374256, 'Loaded_Language_f1-score': 0.35311871227364183, 'Loaded_Language_support': 839.0, 'micro avg_precision': 0.30548302872062666, 'micro avg_recall': 0.41835518474374256, 'micro avg_f1-score': 0.35311871227364183, 'micro avg_support': 839.0, 'macro avg_precision': 0.30548302872062666, 'macro avg_recall': 0.41835518474374256, 'macro avg_f1-score': 0.35311871227364183, 'macro avg_support': 839.0, 'weighted avg_precision': 0.30548302872062666, 'weighted avg_recall': 0.41835518474374256, 'weighted avg_f1-score': 0.3531187122736419, 'weighted avg_support': 839.0, 'O_support': 43326, 'B-Loaded_Language_support': 1149, 'I-Loaded_Language_support': 4474, 'epoch': 9}]}
Early stopping triggered.
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_14_ME10_target=Loaded_Language_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 15 of 23 for (15, 'Repetition') persuasion technique...
{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}]}
Best model updated: current epoch macro f1 = 0.02197802197802198
{'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.36296296296296293
{'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.46441947565543074
{'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.529209621993127
{'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}, {'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.5714285714285715
{'micro_f1': 0.9297865759356635, 'precision': 0.9297865759356635, 'Repetition_precision': 0.538961038961039, 'Repetition_recall': 0.5724137931034483, 'Repetition_f1-score': 0.5551839464882944, 'Repetition_support': 145.0, 'micro avg_precision': 0.538961038961039, 'micro avg_recall': 0.5724137931034483, 'micro avg_f1-score': 0.5551839464882944, 'micro avg_support': 145.0, 'macro avg_precision': 0.538961038961039, 'macro avg_recall': 0.5724137931034483, 'macro avg_f1-score': 0.5551839464882944, 'macro avg_support': 145.0, 'weighted avg_precision': 0.538961038961039, 'weighted avg_recall': 0.5724137931034483, 'weighted avg_f1-score': 0.5551839464882944, 'weighted avg_support': 145.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 5}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}, {'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}, {'micro_f1': 0.9297865759356635, 'precision': 0.9297865759356635, 'Repetition_precision': 0.538961038961039, 'Repetition_recall': 0.5724137931034483, 'Repetition_f1-score': 0.5551839464882944, 'Repetition_support': 145.0, 'micro avg_precision': 0.538961038961039, 'micro avg_recall': 0.5724137931034483, 'micro avg_f1-score': 0.5551839464882944, 'micro avg_support': 145.0, 'macro avg_precision': 0.538961038961039, 'macro avg_recall': 0.5724137931034483, 'macro avg_f1-score': 0.5551839464882944, 'macro avg_support': 145.0, 'weighted avg_precision': 0.538961038961039, 'weighted avg_recall': 0.5724137931034483, 'weighted avg_f1-score': 0.5551839464882944, 'weighted avg_support': 145.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 5}]}
{'micro_f1': 0.9381379523662233, 'precision': 0.9381379523662233, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.60431654676259, 'Repetition_f1-score': 0.5733788395904437, 'Repetition_support': 139.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.60431654676259, 'micro avg_f1-score': 0.5733788395904437, 'micro avg_support': 139.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.60431654676259, 'macro avg_f1-score': 0.5733788395904437, 'macro avg_support': 139.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.60431654676259, 'weighted avg_f1-score': 0.5733788395904437, 'weighted avg_support': 139.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 6}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}, {'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}, {'micro_f1': 0.9297865759356635, 'precision': 0.9297865759356635, 'Repetition_precision': 0.538961038961039, 'Repetition_recall': 0.5724137931034483, 'Repetition_f1-score': 0.5551839464882944, 'Repetition_support': 145.0, 'micro avg_precision': 0.538961038961039, 'micro avg_recall': 0.5724137931034483, 'micro avg_f1-score': 0.5551839464882944, 'micro avg_support': 145.0, 'macro avg_precision': 0.538961038961039, 'macro avg_recall': 0.5724137931034483, 'macro avg_f1-score': 0.5551839464882944, 'macro avg_support': 145.0, 'weighted avg_precision': 0.538961038961039, 'weighted avg_recall': 0.5724137931034483, 'weighted avg_f1-score': 0.5551839464882944, 'weighted avg_support': 145.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 5}, {'micro_f1': 0.9381379523662233, 'precision': 0.9381379523662233, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.60431654676259, 'Repetition_f1-score': 0.5733788395904437, 'Repetition_support': 139.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.60431654676259, 'micro avg_f1-score': 0.5733788395904437, 'micro avg_support': 139.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.60431654676259, 'macro avg_f1-score': 0.5733788395904437, 'macro avg_support': 139.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.60431654676259, 'weighted avg_f1-score': 0.5733788395904437, 'weighted avg_support': 139.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 6}]}
Best model updated: current epoch macro f1 = 0.5733788395904437
{'micro_f1': 0.9341169192700278, 'precision': 0.9341169192700278, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.6885245901639344, 'Repetition_f1-score': 0.6086956521739131, 'Repetition_support': 122.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.6885245901639344, 'micro avg_f1-score': 0.6086956521739131, 'micro avg_support': 122.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.6885245901639344, 'macro avg_f1-score': 0.6086956521739131, 'macro avg_support': 122.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.6885245901639344, 'weighted avg_f1-score': 0.6086956521739131, 'weighted avg_support': 122.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 7}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}, {'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}, {'micro_f1': 0.9297865759356635, 'precision': 0.9297865759356635, 'Repetition_precision': 0.538961038961039, 'Repetition_recall': 0.5724137931034483, 'Repetition_f1-score': 0.5551839464882944, 'Repetition_support': 145.0, 'micro avg_precision': 0.538961038961039, 'micro avg_recall': 0.5724137931034483, 'micro avg_f1-score': 0.5551839464882944, 'micro avg_support': 145.0, 'macro avg_precision': 0.538961038961039, 'macro avg_recall': 0.5724137931034483, 'macro avg_f1-score': 0.5551839464882944, 'macro avg_support': 145.0, 'weighted avg_precision': 0.538961038961039, 'weighted avg_recall': 0.5724137931034483, 'weighted avg_f1-score': 0.5551839464882944, 'weighted avg_support': 145.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 5}, {'micro_f1': 0.9381379523662233, 'precision': 0.9381379523662233, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.60431654676259, 'Repetition_f1-score': 0.5733788395904437, 'Repetition_support': 139.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.60431654676259, 'micro avg_f1-score': 0.5733788395904437, 'micro avg_support': 139.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.60431654676259, 'macro avg_f1-score': 0.5733788395904437, 'macro avg_support': 139.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.60431654676259, 'weighted avg_f1-score': 0.5733788395904437, 'weighted avg_support': 139.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 6}, {'micro_f1': 0.9341169192700278, 'precision': 0.9341169192700278, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.6885245901639344, 'Repetition_f1-score': 0.6086956521739131, 'Repetition_support': 122.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.6885245901639344, 'micro avg_f1-score': 0.6086956521739131, 'micro avg_support': 122.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.6885245901639344, 'macro avg_f1-score': 0.6086956521739131, 'macro avg_support': 122.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.6885245901639344, 'weighted avg_f1-score': 0.6086956521739131, 'weighted avg_support': 122.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 7}]}
Best model updated: current epoch macro f1 = 0.6086956521739131
{'micro_f1': 0.9382926074853077, 'precision': 0.9382926074853077, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.6564885496183206, 'Repetition_f1-score': 0.6035087719298246, 'Repetition_support': 131.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.6564885496183206, 'micro avg_f1-score': 0.6035087719298246, 'micro avg_support': 131.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.6564885496183206, 'macro avg_f1-score': 0.6035087719298246, 'macro avg_support': 131.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.6564885496183206, 'weighted avg_f1-score': 0.6035087719298246, 'weighted avg_support': 131.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 8}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}, {'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}, {'micro_f1': 0.9297865759356635, 'precision': 0.9297865759356635, 'Repetition_precision': 0.538961038961039, 'Repetition_recall': 0.5724137931034483, 'Repetition_f1-score': 0.5551839464882944, 'Repetition_support': 145.0, 'micro avg_precision': 0.538961038961039, 'micro avg_recall': 0.5724137931034483, 'micro avg_f1-score': 0.5551839464882944, 'micro avg_support': 145.0, 'macro avg_precision': 0.538961038961039, 'macro avg_recall': 0.5724137931034483, 'macro avg_f1-score': 0.5551839464882944, 'macro avg_support': 145.0, 'weighted avg_precision': 0.538961038961039, 'weighted avg_recall': 0.5724137931034483, 'weighted avg_f1-score': 0.5551839464882944, 'weighted avg_support': 145.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 5}, {'micro_f1': 0.9381379523662233, 'precision': 0.9381379523662233, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.60431654676259, 'Repetition_f1-score': 0.5733788395904437, 'Repetition_support': 139.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.60431654676259, 'micro avg_f1-score': 0.5733788395904437, 'micro avg_support': 139.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.60431654676259, 'macro avg_f1-score': 0.5733788395904437, 'macro avg_support': 139.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.60431654676259, 'weighted avg_f1-score': 0.5733788395904437, 'weighted avg_support': 139.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 6}, {'micro_f1': 0.9341169192700278, 'precision': 0.9341169192700278, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.6885245901639344, 'Repetition_f1-score': 0.6086956521739131, 'Repetition_support': 122.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.6885245901639344, 'micro avg_f1-score': 0.6086956521739131, 'micro avg_support': 122.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.6885245901639344, 'macro avg_f1-score': 0.6086956521739131, 'macro avg_support': 122.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.6885245901639344, 'weighted avg_f1-score': 0.6086956521739131, 'weighted avg_support': 122.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 7}, {'micro_f1': 0.9382926074853077, 'precision': 0.9382926074853077, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.6564885496183206, 'Repetition_f1-score': 0.6035087719298246, 'Repetition_support': 131.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.6564885496183206, 'micro avg_f1-score': 0.6035087719298246, 'micro avg_support': 131.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.6564885496183206, 'macro avg_f1-score': 0.6035087719298246, 'macro avg_support': 131.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.6564885496183206, 'weighted avg_f1-score': 0.6035087719298246, 'weighted avg_support': 131.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 8}]}
{'micro_f1': 0.9362820909372099, 'precision': 0.93628209093721, 'Repetition_precision': 0.5714285714285714, 'Repetition_recall': 0.5945945945945946, 'Repetition_f1-score': 0.5827814569536424, 'Repetition_support': 148.0, 'micro avg_precision': 0.5714285714285714, 'micro avg_recall': 0.5945945945945946, 'micro avg_f1-score': 0.5827814569536424, 'micro avg_support': 148.0, 'macro avg_precision': 0.5714285714285714, 'macro avg_recall': 0.5945945945945946, 'macro avg_f1-score': 0.5827814569536424, 'macro avg_support': 148.0, 'weighted avg_precision': 0.5714285714285714, 'weighted avg_recall': 0.5945945945945946, 'weighted avg_f1-score': 0.5827814569536424, 'weighted avg_support': 148.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 9}
{'results': [{'micro_f1': 0.8909681410454686, 'precision': 0.8909681410454686, 'Repetition_precision': 0.012987012987012988, 'Repetition_recall': 0.07142857142857142, 'Repetition_f1-score': 0.02197802197802198, 'Repetition_support': 28.0, 'micro avg_precision': 0.012987012987012988, 'micro avg_recall': 0.07142857142857142, 'micro avg_f1-score': 0.02197802197802198, 'micro avg_support': 28.0, 'macro avg_precision': 0.012987012987012988, 'macro avg_recall': 0.07142857142857142, 'macro avg_f1-score': 0.02197802197802198, 'macro avg_support': 28.0, 'weighted avg_precision': 0.012987012987012988, 'weighted avg_recall': 0.07142857142857142, 'weighted avg_f1-score': 0.02197802197802198, 'weighted avg_support': 28.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 0}, {'micro_f1': 0.9189607175997525, 'precision': 0.9189607175997525, 'Repetition_precision': 0.3181818181818182, 'Repetition_recall': 0.4224137931034483, 'Repetition_f1-score': 0.36296296296296293, 'Repetition_support': 116.0, 'micro avg_precision': 0.3181818181818182, 'micro avg_recall': 0.4224137931034483, 'micro avg_f1-score': 0.36296296296296293, 'micro avg_support': 116.0, 'macro avg_precision': 0.3181818181818182, 'macro avg_recall': 0.4224137931034483, 'macro avg_f1-score': 0.36296296296296293, 'macro avg_support': 116.0, 'weighted avg_precision': 0.3181818181818182, 'weighted avg_recall': 0.4224137931034483, 'weighted avg_f1-score': 0.36296296296296293, 'weighted avg_support': 116.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 1}, {'micro_f1': 0.9302505412929168, 'precision': 0.9302505412929168, 'Repetition_precision': 0.4025974025974026, 'Repetition_recall': 0.5486725663716814, 'Repetition_f1-score': 0.46441947565543074, 'Repetition_support': 113.0, 'micro avg_precision': 0.4025974025974026, 'micro avg_recall': 0.5486725663716814, 'micro avg_f1-score': 0.46441947565543074, 'micro avg_support': 113.0, 'macro avg_precision': 0.4025974025974026, 'macro avg_recall': 0.5486725663716814, 'macro avg_f1-score': 0.46441947565543074, 'macro avg_support': 113.0, 'weighted avg_precision': 0.4025974025974026, 'weighted avg_recall': 0.5486725663716814, 'weighted avg_f1-score': 0.46441947565543074, 'weighted avg_support': 113.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 2}, {'micro_f1': 0.9319517476028456, 'precision': 0.9319517476028456, 'Repetition_precision': 0.5, 'Repetition_recall': 0.5620437956204379, 'Repetition_f1-score': 0.529209621993127, 'Repetition_support': 137.0, 'micro avg_precision': 0.5, 'micro avg_recall': 0.5620437956204379, 'micro avg_f1-score': 0.529209621993127, 'micro avg_support': 137.0, 'macro avg_precision': 0.5, 'macro avg_recall': 0.5620437956204379, 'macro avg_f1-score': 0.529209621993127, 'macro avg_support': 137.0, 'weighted avg_precision': 0.5, 'weighted avg_recall': 0.5620437956204379, 'weighted avg_f1-score': 0.529209621993127, 'weighted avg_support': 137.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 3}, {'micro_f1': 0.9356634704608723, 'precision': 0.9356634704608723, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.5850340136054422, 'Repetition_f1-score': 0.5714285714285715, 'Repetition_support': 147.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.5850340136054422, 'micro avg_f1-score': 0.5714285714285715, 'micro avg_support': 147.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.5850340136054422, 'macro avg_f1-score': 0.5714285714285715, 'macro avg_support': 147.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.5850340136054422, 'weighted avg_f1-score': 0.5714285714285715, 'weighted avg_support': 147.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 4}, {'micro_f1': 0.9297865759356635, 'precision': 0.9297865759356635, 'Repetition_precision': 0.538961038961039, 'Repetition_recall': 0.5724137931034483, 'Repetition_f1-score': 0.5551839464882944, 'Repetition_support': 145.0, 'micro avg_precision': 0.538961038961039, 'micro avg_recall': 0.5724137931034483, 'micro avg_f1-score': 0.5551839464882944, 'micro avg_support': 145.0, 'macro avg_precision': 0.538961038961039, 'macro avg_recall': 0.5724137931034483, 'macro avg_f1-score': 0.5551839464882944, 'macro avg_support': 145.0, 'weighted avg_precision': 0.538961038961039, 'weighted avg_recall': 0.5724137931034483, 'weighted avg_f1-score': 0.5551839464882944, 'weighted avg_support': 145.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 5}, {'micro_f1': 0.9381379523662233, 'precision': 0.9381379523662233, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.60431654676259, 'Repetition_f1-score': 0.5733788395904437, 'Repetition_support': 139.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.60431654676259, 'micro avg_f1-score': 0.5733788395904437, 'micro avg_support': 139.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.60431654676259, 'macro avg_f1-score': 0.5733788395904437, 'macro avg_support': 139.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.60431654676259, 'weighted avg_f1-score': 0.5733788395904437, 'weighted avg_support': 139.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 6}, {'micro_f1': 0.9341169192700278, 'precision': 0.9341169192700278, 'Repetition_precision': 0.5454545454545454, 'Repetition_recall': 0.6885245901639344, 'Repetition_f1-score': 0.6086956521739131, 'Repetition_support': 122.0, 'micro avg_precision': 0.5454545454545454, 'micro avg_recall': 0.6885245901639344, 'micro avg_f1-score': 0.6086956521739131, 'micro avg_support': 122.0, 'macro avg_precision': 0.5454545454545454, 'macro avg_recall': 0.6885245901639344, 'macro avg_f1-score': 0.6086956521739131, 'macro avg_support': 122.0, 'weighted avg_precision': 0.5454545454545454, 'weighted avg_recall': 0.6885245901639344, 'weighted avg_f1-score': 0.6086956521739131, 'weighted avg_support': 122.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 7}, {'micro_f1': 0.9382926074853077, 'precision': 0.9382926074853077, 'Repetition_precision': 0.5584415584415584, 'Repetition_recall': 0.6564885496183206, 'Repetition_f1-score': 0.6035087719298246, 'Repetition_support': 131.0, 'micro avg_precision': 0.5584415584415584, 'micro avg_recall': 0.6564885496183206, 'micro avg_f1-score': 0.6035087719298246, 'micro avg_support': 131.0, 'macro avg_precision': 0.5584415584415584, 'macro avg_recall': 0.6564885496183206, 'macro avg_f1-score': 0.6035087719298246, 'macro avg_support': 131.0, 'weighted avg_precision': 0.5584415584415584, 'weighted avg_recall': 0.6564885496183206, 'weighted avg_f1-score': 0.6035087719298246, 'weighted avg_support': 131.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 8}, {'micro_f1': 0.9362820909372099, 'precision': 0.93628209093721, 'Repetition_precision': 0.5714285714285714, 'Repetition_recall': 0.5945945945945946, 'Repetition_f1-score': 0.5827814569536424, 'Repetition_support': 148.0, 'micro avg_precision': 0.5714285714285714, 'micro avg_recall': 0.5945945945945946, 'micro avg_f1-score': 0.5827814569536424, 'micro avg_support': 148.0, 'macro avg_precision': 0.5714285714285714, 'macro avg_recall': 0.5945945945945946, 'macro avg_f1-score': 0.5827814569536424, 'macro avg_support': 148.0, 'weighted avg_precision': 0.5714285714285714, 'weighted avg_recall': 0.5945945945945946, 'weighted avg_f1-score': 0.5827814569536424, 'weighted avg_support': 148.0, 'O_support': 5721, 'B-Repetition_support': 154, 'I-Repetition_support': 591, 'epoch': 9}]}
Early stopping triggered.
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_15_ME10_target=Repetition_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 16 of 23 for (16, 'Exaggeration-Minimisation') persuasion technique...
{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}]}
Best model updated: current epoch macro f1 = 0.022038567493112945
{'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.10989010989010989
{'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.13089005235602097
{'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.18798955613577023
{'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}, {'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.2288329519450801
{'micro_f1': 0.8702616123865456, 'precision': 0.8702616123865456, 'Exaggeration-Minimisation_precision': 0.32038834951456313, 'Exaggeration-Minimisation_recall': 0.3548387096774194, 'Exaggeration-Minimisation_f1-score': 0.33673469387755106, 'Exaggeration-Minimisation_support': 186.0, 'micro avg_precision': 0.32038834951456313, 'micro avg_recall': 0.3548387096774194, 'micro avg_f1-score': 0.33673469387755106, 'micro avg_support': 186.0, 'macro avg_precision': 0.32038834951456313, 'macro avg_recall': 0.3548387096774194, 'macro avg_f1-score': 0.33673469387755106, 'macro avg_support': 186.0, 'weighted avg_precision': 0.32038834951456313, 'weighted avg_recall': 0.3548387096774194, 'weighted avg_f1-score': 0.33673469387755106, 'weighted avg_support': 186.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 5}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}, {'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}, {'micro_f1': 0.8702616123865456, 'precision': 0.8702616123865456, 'Exaggeration-Minimisation_precision': 0.32038834951456313, 'Exaggeration-Minimisation_recall': 0.3548387096774194, 'Exaggeration-Minimisation_f1-score': 0.33673469387755106, 'Exaggeration-Minimisation_support': 186.0, 'micro avg_precision': 0.32038834951456313, 'micro avg_recall': 0.3548387096774194, 'micro avg_f1-score': 0.33673469387755106, 'micro avg_support': 186.0, 'macro avg_precision': 0.32038834951456313, 'macro avg_recall': 0.3548387096774194, 'macro avg_f1-score': 0.33673469387755106, 'macro avg_support': 186.0, 'weighted avg_precision': 0.32038834951456313, 'weighted avg_recall': 0.3548387096774194, 'weighted avg_f1-score': 0.33673469387755106, 'weighted avg_support': 186.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 5}]}
Best model updated: current epoch macro f1 = 0.33673469387755106
{'micro_f1': 0.8680192205018686, 'precision': 0.8680192205018686, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.35467980295566504, 'Exaggeration-Minimisation_f1-score': 0.3520782396088019, 'Exaggeration-Minimisation_support': 203.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.35467980295566504, 'micro avg_f1-score': 0.3520782396088019, 'micro avg_support': 203.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.35467980295566504, 'macro avg_f1-score': 0.3520782396088019, 'macro avg_support': 203.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.35467980295566504, 'weighted avg_f1-score': 0.3520782396088019, 'weighted avg_support': 203.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 6}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}, {'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}, {'micro_f1': 0.8702616123865456, 'precision': 0.8702616123865456, 'Exaggeration-Minimisation_precision': 0.32038834951456313, 'Exaggeration-Minimisation_recall': 0.3548387096774194, 'Exaggeration-Minimisation_f1-score': 0.33673469387755106, 'Exaggeration-Minimisation_support': 186.0, 'micro avg_precision': 0.32038834951456313, 'micro avg_recall': 0.3548387096774194, 'micro avg_f1-score': 0.33673469387755106, 'micro avg_support': 186.0, 'macro avg_precision': 0.32038834951456313, 'macro avg_recall': 0.3548387096774194, 'macro avg_f1-score': 0.33673469387755106, 'macro avg_support': 186.0, 'weighted avg_precision': 0.32038834951456313, 'weighted avg_recall': 0.3548387096774194, 'weighted avg_f1-score': 0.33673469387755106, 'weighted avg_support': 186.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 5}, {'micro_f1': 0.8680192205018686, 'precision': 0.8680192205018686, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.35467980295566504, 'Exaggeration-Minimisation_f1-score': 0.3520782396088019, 'Exaggeration-Minimisation_support': 203.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.35467980295566504, 'micro avg_f1-score': 0.3520782396088019, 'micro avg_support': 203.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.35467980295566504, 'macro avg_f1-score': 0.3520782396088019, 'macro avg_support': 203.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.35467980295566504, 'weighted avg_f1-score': 0.3520782396088019, 'weighted avg_support': 203.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 6}]}
Best model updated: current epoch macro f1 = 0.3520782396088019
{'micro_f1': 0.8468766684463428, 'precision': 0.8468766684463428, 'Exaggeration-Minimisation_precision': 0.21359223300970873, 'Exaggeration-Minimisation_recall': 0.32116788321167883, 'Exaggeration-Minimisation_f1-score': 0.2565597667638484, 'Exaggeration-Minimisation_support': 137.0, 'micro avg_precision': 0.21359223300970873, 'micro avg_recall': 0.32116788321167883, 'micro avg_f1-score': 0.2565597667638484, 'micro avg_support': 137.0, 'macro avg_precision': 0.21359223300970873, 'macro avg_recall': 0.32116788321167883, 'macro avg_f1-score': 0.2565597667638484, 'macro avg_support': 137.0, 'weighted avg_precision': 0.21359223300970873, 'weighted avg_recall': 0.32116788321167883, 'weighted avg_f1-score': 0.2565597667638484, 'weighted avg_support': 137.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 7}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}, {'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}, {'micro_f1': 0.8702616123865456, 'precision': 0.8702616123865456, 'Exaggeration-Minimisation_precision': 0.32038834951456313, 'Exaggeration-Minimisation_recall': 0.3548387096774194, 'Exaggeration-Minimisation_f1-score': 0.33673469387755106, 'Exaggeration-Minimisation_support': 186.0, 'micro avg_precision': 0.32038834951456313, 'micro avg_recall': 0.3548387096774194, 'micro avg_f1-score': 0.33673469387755106, 'micro avg_support': 186.0, 'macro avg_precision': 0.32038834951456313, 'macro avg_recall': 0.3548387096774194, 'macro avg_f1-score': 0.33673469387755106, 'macro avg_support': 186.0, 'weighted avg_precision': 0.32038834951456313, 'weighted avg_recall': 0.3548387096774194, 'weighted avg_f1-score': 0.33673469387755106, 'weighted avg_support': 186.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 5}, {'micro_f1': 0.8680192205018686, 'precision': 0.8680192205018686, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.35467980295566504, 'Exaggeration-Minimisation_f1-score': 0.3520782396088019, 'Exaggeration-Minimisation_support': 203.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.35467980295566504, 'micro avg_f1-score': 0.3520782396088019, 'micro avg_support': 203.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.35467980295566504, 'macro avg_f1-score': 0.3520782396088019, 'macro avg_support': 203.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.35467980295566504, 'weighted avg_f1-score': 0.3520782396088019, 'weighted avg_support': 203.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 6}, {'micro_f1': 0.8468766684463428, 'precision': 0.8468766684463428, 'Exaggeration-Minimisation_precision': 0.21359223300970873, 'Exaggeration-Minimisation_recall': 0.32116788321167883, 'Exaggeration-Minimisation_f1-score': 0.2565597667638484, 'Exaggeration-Minimisation_support': 137.0, 'micro avg_precision': 0.21359223300970873, 'micro avg_recall': 0.32116788321167883, 'micro avg_f1-score': 0.2565597667638484, 'micro avg_support': 137.0, 'macro avg_precision': 0.21359223300970873, 'macro avg_recall': 0.32116788321167883, 'macro avg_f1-score': 0.2565597667638484, 'macro avg_support': 137.0, 'weighted avg_precision': 0.21359223300970873, 'weighted avg_recall': 0.32116788321167883, 'weighted avg_f1-score': 0.2565597667638484, 'weighted avg_support': 137.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 7}]}
{'micro_f1': 0.8676988788040577, 'precision': 0.8676988788040577, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.36180904522613067, 'Exaggeration-Minimisation_f1-score': 0.35555555555555557, 'Exaggeration-Minimisation_support': 199.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.36180904522613067, 'micro avg_f1-score': 0.35555555555555557, 'micro avg_support': 199.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.36180904522613067, 'macro avg_f1-score': 0.35555555555555557, 'macro avg_support': 199.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.36180904522613067, 'weighted avg_f1-score': 0.35555555555555557, 'weighted avg_support': 199.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 8}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}, {'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}, {'micro_f1': 0.8702616123865456, 'precision': 0.8702616123865456, 'Exaggeration-Minimisation_precision': 0.32038834951456313, 'Exaggeration-Minimisation_recall': 0.3548387096774194, 'Exaggeration-Minimisation_f1-score': 0.33673469387755106, 'Exaggeration-Minimisation_support': 186.0, 'micro avg_precision': 0.32038834951456313, 'micro avg_recall': 0.3548387096774194, 'micro avg_f1-score': 0.33673469387755106, 'micro avg_support': 186.0, 'macro avg_precision': 0.32038834951456313, 'macro avg_recall': 0.3548387096774194, 'macro avg_f1-score': 0.33673469387755106, 'macro avg_support': 186.0, 'weighted avg_precision': 0.32038834951456313, 'weighted avg_recall': 0.3548387096774194, 'weighted avg_f1-score': 0.33673469387755106, 'weighted avg_support': 186.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 5}, {'micro_f1': 0.8680192205018686, 'precision': 0.8680192205018686, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.35467980295566504, 'Exaggeration-Minimisation_f1-score': 0.3520782396088019, 'Exaggeration-Minimisation_support': 203.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.35467980295566504, 'micro avg_f1-score': 0.3520782396088019, 'micro avg_support': 203.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.35467980295566504, 'macro avg_f1-score': 0.3520782396088019, 'macro avg_support': 203.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.35467980295566504, 'weighted avg_f1-score': 0.3520782396088019, 'weighted avg_support': 203.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 6}, {'micro_f1': 0.8468766684463428, 'precision': 0.8468766684463428, 'Exaggeration-Minimisation_precision': 0.21359223300970873, 'Exaggeration-Minimisation_recall': 0.32116788321167883, 'Exaggeration-Minimisation_f1-score': 0.2565597667638484, 'Exaggeration-Minimisation_support': 137.0, 'micro avg_precision': 0.21359223300970873, 'micro avg_recall': 0.32116788321167883, 'micro avg_f1-score': 0.2565597667638484, 'micro avg_support': 137.0, 'macro avg_precision': 0.21359223300970873, 'macro avg_recall': 0.32116788321167883, 'macro avg_f1-score': 0.2565597667638484, 'macro avg_support': 137.0, 'weighted avg_precision': 0.21359223300970873, 'weighted avg_recall': 0.32116788321167883, 'weighted avg_f1-score': 0.2565597667638484, 'weighted avg_support': 137.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 7}, {'micro_f1': 0.8676988788040577, 'precision': 0.8676988788040577, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.36180904522613067, 'Exaggeration-Minimisation_f1-score': 0.35555555555555557, 'Exaggeration-Minimisation_support': 199.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.36180904522613067, 'micro avg_f1-score': 0.35555555555555557, 'micro avg_support': 199.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.36180904522613067, 'macro avg_f1-score': 0.35555555555555557, 'macro avg_support': 199.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.36180904522613067, 'weighted avg_f1-score': 0.35555555555555557, 'weighted avg_support': 199.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 8}]}
Best model updated: current epoch macro f1 = 0.35555555555555557
{'micro_f1': 0.8595835557928457, 'precision': 0.8595835557928457, 'Exaggeration-Minimisation_precision': 0.3883495145631068, 'Exaggeration-Minimisation_recall': 0.365296803652968, 'Exaggeration-Minimisation_f1-score': 0.3764705882352941, 'Exaggeration-Minimisation_support': 219.0, 'micro avg_precision': 0.3883495145631068, 'micro avg_recall': 0.365296803652968, 'micro avg_f1-score': 0.3764705882352941, 'micro avg_support': 219.0, 'macro avg_precision': 0.3883495145631068, 'macro avg_recall': 0.365296803652968, 'macro avg_f1-score': 0.3764705882352941, 'macro avg_support': 219.0, 'weighted avg_precision': 0.3883495145631068, 'weighted avg_recall': 0.365296803652968, 'weighted avg_f1-score': 0.37647058823529417, 'weighted avg_support': 219.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 9}
{'results': [{'micro_f1': 0.8363053924185798, 'precision': 0.8363053924185798, 'Exaggeration-Minimisation_precision': 0.019417475728155338, 'Exaggeration-Minimisation_recall': 0.025477707006369428, 'Exaggeration-Minimisation_f1-score': 0.022038567493112945, 'Exaggeration-Minimisation_support': 157.0, 'micro avg_precision': 0.019417475728155338, 'micro avg_recall': 0.025477707006369428, 'micro avg_f1-score': 0.022038567493112945, 'micro avg_support': 157.0, 'macro avg_precision': 0.019417475728155338, 'macro avg_recall': 0.025477707006369428, 'macro avg_f1-score': 0.022038567493112945, 'macro avg_support': 157.0, 'weighted avg_precision': 0.019417475728155338, 'weighted avg_recall': 0.025477707006369428, 'weighted avg_f1-score': 0.022038567493112945, 'weighted avg_support': 157.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 0}, {'micro_f1': 0.8463427656166577, 'precision': 0.8463427656166578, 'Exaggeration-Minimisation_precision': 0.0970873786407767, 'Exaggeration-Minimisation_recall': 0.12658227848101267, 'Exaggeration-Minimisation_f1-score': 0.10989010989010989, 'Exaggeration-Minimisation_support': 158.0, 'micro avg_precision': 0.0970873786407767, 'micro avg_recall': 0.12658227848101267, 'micro avg_f1-score': 0.10989010989010989, 'micro avg_support': 158.0, 'macro avg_precision': 0.0970873786407767, 'macro avg_recall': 0.12658227848101267, 'macro avg_f1-score': 0.10989010989010989, 'macro avg_support': 158.0, 'weighted avg_precision': 0.0970873786407767, 'weighted avg_recall': 0.12658227848101267, 'weighted avg_f1-score': 0.10989010989010989, 'weighted avg_support': 158.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 1}, {'micro_f1': 0.8596903363587827, 'precision': 0.8596903363587827, 'Exaggeration-Minimisation_precision': 0.12135922330097088, 'Exaggeration-Minimisation_recall': 0.14204545454545456, 'Exaggeration-Minimisation_f1-score': 0.13089005235602097, 'Exaggeration-Minimisation_support': 176.0, 'micro avg_precision': 0.12135922330097088, 'micro avg_recall': 0.14204545454545456, 'micro avg_f1-score': 0.13089005235602097, 'micro avg_support': 176.0, 'macro avg_precision': 0.12135922330097088, 'macro avg_recall': 0.14204545454545456, 'macro avg_f1-score': 0.13089005235602097, 'macro avg_support': 176.0, 'weighted avg_precision': 0.12135922330097088, 'weighted avg_recall': 0.14204545454545456, 'weighted avg_f1-score': 0.13089005235602097, 'weighted avg_support': 176.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 2}, {'micro_f1': 0.8475173518419649, 'precision': 0.8475173518419648, 'Exaggeration-Minimisation_precision': 0.17475728155339806, 'Exaggeration-Minimisation_recall': 0.2033898305084746, 'Exaggeration-Minimisation_f1-score': 0.18798955613577023, 'Exaggeration-Minimisation_support': 177.0, 'micro avg_precision': 0.17475728155339806, 'micro avg_recall': 0.2033898305084746, 'micro avg_f1-score': 0.18798955613577023, 'micro avg_support': 177.0, 'macro avg_precision': 0.17475728155339806, 'macro avg_recall': 0.2033898305084746, 'macro avg_f1-score': 0.18798955613577023, 'macro avg_support': 177.0, 'weighted avg_precision': 0.17475728155339806, 'weighted avg_recall': 0.2033898305084746, 'weighted avg_f1-score': 0.18798955613577023, 'weighted avg_support': 177.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 3}, {'micro_f1': 0.8607581420181527, 'precision': 0.8607581420181527, 'Exaggeration-Minimisation_precision': 0.24271844660194175, 'Exaggeration-Minimisation_recall': 0.21645021645021645, 'Exaggeration-Minimisation_f1-score': 0.2288329519450801, 'Exaggeration-Minimisation_support': 231.0, 'micro avg_precision': 0.24271844660194175, 'micro avg_recall': 0.21645021645021645, 'micro avg_f1-score': 0.2288329519450801, 'micro avg_support': 231.0, 'macro avg_precision': 0.24271844660194175, 'macro avg_recall': 0.21645021645021645, 'macro avg_f1-score': 0.2288329519450801, 'macro avg_support': 231.0, 'weighted avg_precision': 0.24271844660194175, 'weighted avg_recall': 0.21645021645021645, 'weighted avg_f1-score': 0.2288329519450801, 'weighted avg_support': 231.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 4}, {'micro_f1': 0.8702616123865456, 'precision': 0.8702616123865456, 'Exaggeration-Minimisation_precision': 0.32038834951456313, 'Exaggeration-Minimisation_recall': 0.3548387096774194, 'Exaggeration-Minimisation_f1-score': 0.33673469387755106, 'Exaggeration-Minimisation_support': 186.0, 'micro avg_precision': 0.32038834951456313, 'micro avg_recall': 0.3548387096774194, 'micro avg_f1-score': 0.33673469387755106, 'micro avg_support': 186.0, 'macro avg_precision': 0.32038834951456313, 'macro avg_recall': 0.3548387096774194, 'macro avg_f1-score': 0.33673469387755106, 'macro avg_support': 186.0, 'weighted avg_precision': 0.32038834951456313, 'weighted avg_recall': 0.3548387096774194, 'weighted avg_f1-score': 0.33673469387755106, 'weighted avg_support': 186.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 5}, {'micro_f1': 0.8680192205018686, 'precision': 0.8680192205018686, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.35467980295566504, 'Exaggeration-Minimisation_f1-score': 0.3520782396088019, 'Exaggeration-Minimisation_support': 203.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.35467980295566504, 'micro avg_f1-score': 0.3520782396088019, 'micro avg_support': 203.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.35467980295566504, 'macro avg_f1-score': 0.3520782396088019, 'macro avg_support': 203.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.35467980295566504, 'weighted avg_f1-score': 0.3520782396088019, 'weighted avg_support': 203.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 6}, {'micro_f1': 0.8468766684463428, 'precision': 0.8468766684463428, 'Exaggeration-Minimisation_precision': 0.21359223300970873, 'Exaggeration-Minimisation_recall': 0.32116788321167883, 'Exaggeration-Minimisation_f1-score': 0.2565597667638484, 'Exaggeration-Minimisation_support': 137.0, 'micro avg_precision': 0.21359223300970873, 'micro avg_recall': 0.32116788321167883, 'micro avg_f1-score': 0.2565597667638484, 'micro avg_support': 137.0, 'macro avg_precision': 0.21359223300970873, 'macro avg_recall': 0.32116788321167883, 'macro avg_f1-score': 0.2565597667638484, 'macro avg_support': 137.0, 'weighted avg_precision': 0.21359223300970873, 'weighted avg_recall': 0.32116788321167883, 'weighted avg_f1-score': 0.2565597667638484, 'weighted avg_support': 137.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 7}, {'micro_f1': 0.8676988788040577, 'precision': 0.8676988788040577, 'Exaggeration-Minimisation_precision': 0.34951456310679613, 'Exaggeration-Minimisation_recall': 0.36180904522613067, 'Exaggeration-Minimisation_f1-score': 0.35555555555555557, 'Exaggeration-Minimisation_support': 199.0, 'micro avg_precision': 0.34951456310679613, 'micro avg_recall': 0.36180904522613067, 'micro avg_f1-score': 0.35555555555555557, 'micro avg_support': 199.0, 'macro avg_precision': 0.34951456310679613, 'macro avg_recall': 0.36180904522613067, 'macro avg_f1-score': 0.35555555555555557, 'macro avg_support': 199.0, 'weighted avg_precision': 0.34951456310679613, 'weighted avg_recall': 0.36180904522613067, 'weighted avg_f1-score': 0.35555555555555557, 'weighted avg_support': 199.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 8}, {'micro_f1': 0.8595835557928457, 'precision': 0.8595835557928457, 'Exaggeration-Minimisation_precision': 0.3883495145631068, 'Exaggeration-Minimisation_recall': 0.365296803652968, 'Exaggeration-Minimisation_f1-score': 0.3764705882352941, 'Exaggeration-Minimisation_support': 219.0, 'micro avg_precision': 0.3883495145631068, 'micro avg_recall': 0.365296803652968, 'micro avg_f1-score': 0.3764705882352941, 'micro avg_support': 219.0, 'macro avg_precision': 0.3883495145631068, 'macro avg_recall': 0.365296803652968, 'macro avg_f1-score': 0.3764705882352941, 'macro avg_support': 219.0, 'weighted avg_precision': 0.3883495145631068, 'weighted avg_recall': 0.365296803652968, 'weighted avg_f1-score': 0.37647058823529417, 'weighted avg_support': 219.0, 'O_support': 7302, 'B-Exaggeration-Minimisation_support': 206, 'I-Exaggeration-Minimisation_support': 1857, 'epoch': 9}]}
Best model updated: current epoch macro f1 = 0.3764705882352941
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_16_ME10_target=Exaggeration-Minimisation_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 17 of 23 for (17, 'Obfuscation-Vagueness-Confusion') persuasion technique...
{'micro_f1': 0.6424050632911392, 'precision': 0.6424050632911392, 'Obfuscation-Vagueness-Confusion_precision': 0.0, 'Obfuscation-Vagueness-Confusion_recall': 0.0, 'Obfuscation-Vagueness-Confusion_f1-score': 0.0, 'Obfuscation-Vagueness-Confusion_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 1624, 'B-Obfuscation-Vagueness-Confusion_support': 47, 'I-Obfuscation-Vagueness-Confusion_support': 857, 'epoch': 0}
{'results': [{'micro_f1': 0.6424050632911392, 'precision': 0.6424050632911392, 'Obfuscation-Vagueness-Confusion_precision': 0.0, 'Obfuscation-Vagueness-Confusion_recall': 0.0, 'Obfuscation-Vagueness-Confusion_f1-score': 0.0, 'Obfuscation-Vagueness-Confusion_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 1624, 'B-Obfuscation-Vagueness-Confusion_support': 47, 'I-Obfuscation-Vagueness-Confusion_support': 857, 'epoch': 0}]}
{'micro_f1': 0.6443829113924051, 'precision': 0.6443829113924051, 'Obfuscation-Vagueness-Confusion_precision': 0.0, 'Obfuscation-Vagueness-Confusion_recall': 0.0, 'Obfuscation-Vagueness-Confusion_f1-score': 0.0, 'Obfuscation-Vagueness-Confusion_support': 5.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 5.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 5.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 5.0, 'O_support': 1624, 'B-Obfuscation-Vagueness-Confusion_support': 47, 'I-Obfuscation-Vagueness-Confusion_support': 857, 'epoch': 1}
{'results': [{'micro_f1': 0.6424050632911392, 'precision': 0.6424050632911392, 'Obfuscation-Vagueness-Confusion_precision': 0.0, 'Obfuscation-Vagueness-Confusion_recall': 0.0, 'Obfuscation-Vagueness-Confusion_f1-score': 0.0, 'Obfuscation-Vagueness-Confusion_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 1624, 'B-Obfuscation-Vagueness-Confusion_support': 47, 'I-Obfuscation-Vagueness-Confusion_support': 857, 'epoch': 0}, {'micro_f1': 0.6443829113924051, 'precision': 0.6443829113924051, 'Obfuscation-Vagueness-Confusion_precision': 0.0, 'Obfuscation-Vagueness-Confusion_recall': 0.0, 'Obfuscation-Vagueness-Confusion_f1-score': 0.0, 'Obfuscation-Vagueness-Confusion_support': 5.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 5.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 5.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 5.0, 'O_support': 1624, 'B-Obfuscation-Vagueness-Confusion_support': 47, 'I-Obfuscation-Vagueness-Confusion_support': 857, 'epoch': 1}]}
Early stopping triggered.
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_17_ME10_target=Obfuscation-Vagueness-Confusion_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 18 of 23 for (18, 'Name_Calling-Labeling') persuasion technique...
{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}]}
Best model updated: current epoch macro f1 = 0.02093244529019981
{'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.17034700315457416
{'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}, {'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.23736600306278713
{'micro_f1': 0.9432219505569139, 'precision': 0.9432219505569139, 'Name_Calling-Labeling_precision': 0.23161764705882354, 'Name_Calling-Labeling_recall': 0.35660377358490564, 'Name_Calling-Labeling_f1-score': 0.28083209509658247, 'Name_Calling-Labeling_support': 530.0, 'micro avg_precision': 0.23161764705882354, 'micro avg_recall': 0.35660377358490564, 'micro avg_f1-score': 0.28083209509658247, 'micro avg_support': 530.0, 'macro avg_precision': 0.23161764705882354, 'macro avg_recall': 0.35660377358490564, 'macro avg_f1-score': 0.28083209509658247, 'macro avg_support': 530.0, 'weighted avg_precision': 0.23161764705882354, 'weighted avg_recall': 0.35660377358490564, 'weighted avg_f1-score': 0.28083209509658247, 'weighted avg_support': 530.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 3}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}, {'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}, {'micro_f1': 0.9432219505569139, 'precision': 0.9432219505569139, 'Name_Calling-Labeling_precision': 0.23161764705882354, 'Name_Calling-Labeling_recall': 0.35660377358490564, 'Name_Calling-Labeling_f1-score': 0.28083209509658247, 'Name_Calling-Labeling_support': 530.0, 'micro avg_precision': 0.23161764705882354, 'micro avg_recall': 0.35660377358490564, 'micro avg_f1-score': 0.28083209509658247, 'micro avg_support': 530.0, 'macro avg_precision': 0.23161764705882354, 'macro avg_recall': 0.35660377358490564, 'macro avg_f1-score': 0.28083209509658247, 'macro avg_support': 530.0, 'weighted avg_precision': 0.23161764705882354, 'weighted avg_recall': 0.35660377358490564, 'weighted avg_f1-score': 0.28083209509658247, 'weighted avg_support': 530.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.28083209509658247
{'micro_f1': 0.9449123124754747, 'precision': 0.9449123124754747, 'Name_Calling-Labeling_precision': 0.28799019607843135, 'Name_Calling-Labeling_recall': 0.4058721934369603, 'Name_Calling-Labeling_f1-score': 0.3369175627240143, 'Name_Calling-Labeling_support': 579.0, 'micro avg_precision': 0.28799019607843135, 'micro avg_recall': 0.4058721934369603, 'micro avg_f1-score': 0.3369175627240143, 'micro avg_support': 579.0, 'macro avg_precision': 0.28799019607843135, 'macro avg_recall': 0.4058721934369603, 'macro avg_f1-score': 0.3369175627240143, 'macro avg_support': 579.0, 'weighted avg_precision': 0.28799019607843135, 'weighted avg_recall': 0.4058721934369603, 'weighted avg_f1-score': 0.3369175627240143, 'weighted avg_support': 579.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 4}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}, {'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}, {'micro_f1': 0.9432219505569139, 'precision': 0.9432219505569139, 'Name_Calling-Labeling_precision': 0.23161764705882354, 'Name_Calling-Labeling_recall': 0.35660377358490564, 'Name_Calling-Labeling_f1-score': 0.28083209509658247, 'Name_Calling-Labeling_support': 530.0, 'micro avg_precision': 0.23161764705882354, 'micro avg_recall': 0.35660377358490564, 'micro avg_f1-score': 0.28083209509658247, 'micro avg_support': 530.0, 'macro avg_precision': 0.23161764705882354, 'macro avg_recall': 0.35660377358490564, 'macro avg_f1-score': 0.28083209509658247, 'macro avg_support': 530.0, 'weighted avg_precision': 0.23161764705882354, 'weighted avg_recall': 0.35660377358490564, 'weighted avg_f1-score': 0.28083209509658247, 'weighted avg_support': 530.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 3}, {'micro_f1': 0.9449123124754747, 'precision': 0.9449123124754747, 'Name_Calling-Labeling_precision': 0.28799019607843135, 'Name_Calling-Labeling_recall': 0.4058721934369603, 'Name_Calling-Labeling_f1-score': 0.3369175627240143, 'Name_Calling-Labeling_support': 579.0, 'micro avg_precision': 0.28799019607843135, 'micro avg_recall': 0.4058721934369603, 'micro avg_f1-score': 0.3369175627240143, 'micro avg_support': 579.0, 'macro avg_precision': 0.28799019607843135, 'macro avg_recall': 0.4058721934369603, 'macro avg_f1-score': 0.3369175627240143, 'macro avg_support': 579.0, 'weighted avg_precision': 0.28799019607843135, 'weighted avg_recall': 0.4058721934369603, 'weighted avg_f1-score': 0.3369175627240143, 'weighted avg_support': 579.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.3369175627240143
{'micro_f1': 0.9482326662440762, 'precision': 0.9482326662440762, 'Name_Calling-Labeling_precision': 0.40318627450980393, 'Name_Calling-Labeling_recall': 0.4404283801874163, 'Name_Calling-Labeling_f1-score': 0.4209852847088931, 'Name_Calling-Labeling_support': 747.0, 'micro avg_precision': 0.40318627450980393, 'micro avg_recall': 0.4404283801874163, 'micro avg_f1-score': 0.4209852847088931, 'micro avg_support': 747.0, 'macro avg_precision': 0.40318627450980393, 'macro avg_recall': 0.4404283801874163, 'macro avg_f1-score': 0.4209852847088931, 'macro avg_support': 747.0, 'weighted avg_precision': 0.40318627450980393, 'weighted avg_recall': 0.4404283801874163, 'weighted avg_f1-score': 0.4209852847088931, 'weighted avg_support': 747.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 5}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}, {'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}, {'micro_f1': 0.9432219505569139, 'precision': 0.9432219505569139, 'Name_Calling-Labeling_precision': 0.23161764705882354, 'Name_Calling-Labeling_recall': 0.35660377358490564, 'Name_Calling-Labeling_f1-score': 0.28083209509658247, 'Name_Calling-Labeling_support': 530.0, 'micro avg_precision': 0.23161764705882354, 'micro avg_recall': 0.35660377358490564, 'micro avg_f1-score': 0.28083209509658247, 'micro avg_support': 530.0, 'macro avg_precision': 0.23161764705882354, 'macro avg_recall': 0.35660377358490564, 'macro avg_f1-score': 0.28083209509658247, 'macro avg_support': 530.0, 'weighted avg_precision': 0.23161764705882354, 'weighted avg_recall': 0.35660377358490564, 'weighted avg_f1-score': 0.28083209509658247, 'weighted avg_support': 530.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 3}, {'micro_f1': 0.9449123124754747, 'precision': 0.9449123124754747, 'Name_Calling-Labeling_precision': 0.28799019607843135, 'Name_Calling-Labeling_recall': 0.4058721934369603, 'Name_Calling-Labeling_f1-score': 0.3369175627240143, 'Name_Calling-Labeling_support': 579.0, 'micro avg_precision': 0.28799019607843135, 'micro avg_recall': 0.4058721934369603, 'micro avg_f1-score': 0.3369175627240143, 'micro avg_support': 579.0, 'macro avg_precision': 0.28799019607843135, 'macro avg_recall': 0.4058721934369603, 'macro avg_f1-score': 0.3369175627240143, 'macro avg_support': 579.0, 'weighted avg_precision': 0.28799019607843135, 'weighted avg_recall': 0.4058721934369603, 'weighted avg_f1-score': 0.3369175627240143, 'weighted avg_support': 579.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 4}, {'micro_f1': 0.9482326662440762, 'precision': 0.9482326662440762, 'Name_Calling-Labeling_precision': 0.40318627450980393, 'Name_Calling-Labeling_recall': 0.4404283801874163, 'Name_Calling-Labeling_f1-score': 0.4209852847088931, 'Name_Calling-Labeling_support': 747.0, 'micro avg_precision': 0.40318627450980393, 'micro avg_recall': 0.4404283801874163, 'micro avg_f1-score': 0.4209852847088931, 'micro avg_support': 747.0, 'macro avg_precision': 0.40318627450980393, 'macro avg_recall': 0.4404283801874163, 'macro avg_f1-score': 0.4209852847088931, 'macro avg_support': 747.0, 'weighted avg_precision': 0.40318627450980393, 'weighted avg_recall': 0.4404283801874163, 'weighted avg_f1-score': 0.4209852847088931, 'weighted avg_support': 747.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 5}]}
Best model updated: current epoch macro f1 = 0.4209852847088931
{'micro_f1': 0.9470252648736757, 'precision': 0.9470252648736757, 'Name_Calling-Labeling_precision': 0.3235294117647059, 'Name_Calling-Labeling_recall': 0.41379310344827586, 'Name_Calling-Labeling_f1-score': 0.3631361760660248, 'Name_Calling-Labeling_support': 638.0, 'micro avg_precision': 0.3235294117647059, 'micro avg_recall': 0.41379310344827586, 'micro avg_f1-score': 0.3631361760660248, 'micro avg_support': 638.0, 'macro avg_precision': 0.3235294117647059, 'macro avg_recall': 0.41379310344827586, 'macro avg_f1-score': 0.3631361760660248, 'macro avg_support': 638.0, 'weighted avg_precision': 0.3235294117647059, 'weighted avg_recall': 0.41379310344827586, 'weighted avg_f1-score': 0.3631361760660248, 'weighted avg_support': 638.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 6}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}, {'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}, {'micro_f1': 0.9432219505569139, 'precision': 0.9432219505569139, 'Name_Calling-Labeling_precision': 0.23161764705882354, 'Name_Calling-Labeling_recall': 0.35660377358490564, 'Name_Calling-Labeling_f1-score': 0.28083209509658247, 'Name_Calling-Labeling_support': 530.0, 'micro avg_precision': 0.23161764705882354, 'micro avg_recall': 0.35660377358490564, 'micro avg_f1-score': 0.28083209509658247, 'micro avg_support': 530.0, 'macro avg_precision': 0.23161764705882354, 'macro avg_recall': 0.35660377358490564, 'macro avg_f1-score': 0.28083209509658247, 'macro avg_support': 530.0, 'weighted avg_precision': 0.23161764705882354, 'weighted avg_recall': 0.35660377358490564, 'weighted avg_f1-score': 0.28083209509658247, 'weighted avg_support': 530.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 3}, {'micro_f1': 0.9449123124754747, 'precision': 0.9449123124754747, 'Name_Calling-Labeling_precision': 0.28799019607843135, 'Name_Calling-Labeling_recall': 0.4058721934369603, 'Name_Calling-Labeling_f1-score': 0.3369175627240143, 'Name_Calling-Labeling_support': 579.0, 'micro avg_precision': 0.28799019607843135, 'micro avg_recall': 0.4058721934369603, 'micro avg_f1-score': 0.3369175627240143, 'micro avg_support': 579.0, 'macro avg_precision': 0.28799019607843135, 'macro avg_recall': 0.4058721934369603, 'macro avg_f1-score': 0.3369175627240143, 'macro avg_support': 579.0, 'weighted avg_precision': 0.28799019607843135, 'weighted avg_recall': 0.4058721934369603, 'weighted avg_f1-score': 0.3369175627240143, 'weighted avg_support': 579.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 4}, {'micro_f1': 0.9482326662440762, 'precision': 0.9482326662440762, 'Name_Calling-Labeling_precision': 0.40318627450980393, 'Name_Calling-Labeling_recall': 0.4404283801874163, 'Name_Calling-Labeling_f1-score': 0.4209852847088931, 'Name_Calling-Labeling_support': 747.0, 'micro avg_precision': 0.40318627450980393, 'micro avg_recall': 0.4404283801874163, 'micro avg_f1-score': 0.4209852847088931, 'micro avg_support': 747.0, 'macro avg_precision': 0.40318627450980393, 'macro avg_recall': 0.4404283801874163, 'macro avg_f1-score': 0.4209852847088931, 'macro avg_support': 747.0, 'weighted avg_precision': 0.40318627450980393, 'weighted avg_recall': 0.4404283801874163, 'weighted avg_f1-score': 0.4209852847088931, 'weighted avg_support': 747.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 5}, {'micro_f1': 0.9470252648736757, 'precision': 0.9470252648736757, 'Name_Calling-Labeling_precision': 0.3235294117647059, 'Name_Calling-Labeling_recall': 0.41379310344827586, 'Name_Calling-Labeling_f1-score': 0.3631361760660248, 'Name_Calling-Labeling_support': 638.0, 'micro avg_precision': 0.3235294117647059, 'micro avg_recall': 0.41379310344827586, 'micro avg_f1-score': 0.3631361760660248, 'micro avg_support': 638.0, 'macro avg_precision': 0.3235294117647059, 'macro avg_recall': 0.41379310344827586, 'macro avg_f1-score': 0.3631361760660248, 'macro avg_support': 638.0, 'weighted avg_precision': 0.3235294117647059, 'weighted avg_recall': 0.41379310344827586, 'weighted avg_f1-score': 0.3631361760660248, 'weighted avg_support': 638.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 6}]}
{'micro_f1': 0.9442180566874944, 'precision': 0.9442180566874944, 'Name_Calling-Labeling_precision': 0.375, 'Name_Calling-Labeling_recall': 0.4140730717185386, 'Name_Calling-Labeling_f1-score': 0.39356913183279746, 'Name_Calling-Labeling_support': 739.0, 'micro avg_precision': 0.375, 'micro avg_recall': 0.4140730717185386, 'micro avg_f1-score': 0.39356913183279746, 'micro avg_support': 739.0, 'macro avg_precision': 0.375, 'macro avg_recall': 0.4140730717185386, 'macro avg_f1-score': 0.39356913183279746, 'macro avg_support': 739.0, 'weighted avg_precision': 0.375, 'weighted avg_recall': 0.4140730717185386, 'weighted avg_f1-score': 0.39356913183279746, 'weighted avg_support': 739.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 7}
{'results': [{'micro_f1': 0.9193455884572428, 'precision': 0.9193455884572429, 'Name_Calling-Labeling_precision': 0.013480392156862746, 'Name_Calling-Labeling_recall': 0.04680851063829787, 'Name_Calling-Labeling_f1-score': 0.02093244529019981, 'Name_Calling-Labeling_support': 235.0, 'micro avg_precision': 0.013480392156862746, 'micro avg_recall': 0.04680851063829787, 'micro avg_f1-score': 0.02093244529019981, 'micro avg_support': 235.0, 'macro avg_precision': 0.013480392156862746, 'macro avg_recall': 0.04680851063829787, 'macro avg_f1-score': 0.02093244529019981, 'macro avg_support': 235.0, 'weighted avg_precision': 0.013480392156862746, 'weighted avg_recall': 0.04680851063829787, 'weighted avg_f1-score': 0.02093244529019981, 'weighted avg_support': 235.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 0}, {'micro_f1': 0.9371547586706511, 'precision': 0.9371547586706511, 'Name_Calling-Labeling_precision': 0.1323529411764706, 'Name_Calling-Labeling_recall': 0.23893805309734514, 'Name_Calling-Labeling_f1-score': 0.17034700315457416, 'Name_Calling-Labeling_support': 452.0, 'micro avg_precision': 0.1323529411764706, 'micro avg_recall': 0.23893805309734514, 'micro avg_f1-score': 0.17034700315457416, 'micro avg_support': 452.0, 'macro avg_precision': 0.1323529411764706, 'macro avg_recall': 0.23893805309734514, 'macro avg_f1-score': 0.17034700315457416, 'macro avg_support': 452.0, 'weighted avg_precision': 0.1323529411764706, 'weighted avg_recall': 0.23893805309734514, 'weighted avg_f1-score': 0.17034700315457416, 'weighted avg_support': 452.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 1}, {'micro_f1': 0.9379395695614114, 'precision': 0.9379395695614114, 'Name_Calling-Labeling_precision': 0.18995098039215685, 'Name_Calling-Labeling_recall': 0.3163265306122449, 'Name_Calling-Labeling_f1-score': 0.23736600306278713, 'Name_Calling-Labeling_support': 490.0, 'micro avg_precision': 0.18995098039215685, 'micro avg_recall': 0.3163265306122449, 'micro avg_f1-score': 0.23736600306278713, 'micro avg_support': 490.0, 'macro avg_precision': 0.18995098039215685, 'macro avg_recall': 0.3163265306122449, 'macro avg_f1-score': 0.23736600306278713, 'macro avg_support': 490.0, 'weighted avg_precision': 0.18995098039215685, 'weighted avg_recall': 0.3163265306122449, 'weighted avg_f1-score': 0.23736600306278713, 'weighted avg_support': 490.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 2}, {'micro_f1': 0.9432219505569139, 'precision': 0.9432219505569139, 'Name_Calling-Labeling_precision': 0.23161764705882354, 'Name_Calling-Labeling_recall': 0.35660377358490564, 'Name_Calling-Labeling_f1-score': 0.28083209509658247, 'Name_Calling-Labeling_support': 530.0, 'micro avg_precision': 0.23161764705882354, 'micro avg_recall': 0.35660377358490564, 'micro avg_f1-score': 0.28083209509658247, 'micro avg_support': 530.0, 'macro avg_precision': 0.23161764705882354, 'macro avg_recall': 0.35660377358490564, 'macro avg_f1-score': 0.28083209509658247, 'macro avg_support': 530.0, 'weighted avg_precision': 0.23161764705882354, 'weighted avg_recall': 0.35660377358490564, 'weighted avg_f1-score': 0.28083209509658247, 'weighted avg_support': 530.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 3}, {'micro_f1': 0.9449123124754747, 'precision': 0.9449123124754747, 'Name_Calling-Labeling_precision': 0.28799019607843135, 'Name_Calling-Labeling_recall': 0.4058721934369603, 'Name_Calling-Labeling_f1-score': 0.3369175627240143, 'Name_Calling-Labeling_support': 579.0, 'micro avg_precision': 0.28799019607843135, 'micro avg_recall': 0.4058721934369603, 'micro avg_f1-score': 0.3369175627240143, 'micro avg_support': 579.0, 'macro avg_precision': 0.28799019607843135, 'macro avg_recall': 0.4058721934369603, 'macro avg_f1-score': 0.3369175627240143, 'macro avg_support': 579.0, 'weighted avg_precision': 0.28799019607843135, 'weighted avg_recall': 0.4058721934369603, 'weighted avg_f1-score': 0.3369175627240143, 'weighted avg_support': 579.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 4}, {'micro_f1': 0.9482326662440762, 'precision': 0.9482326662440762, 'Name_Calling-Labeling_precision': 0.40318627450980393, 'Name_Calling-Labeling_recall': 0.4404283801874163, 'Name_Calling-Labeling_f1-score': 0.4209852847088931, 'Name_Calling-Labeling_support': 747.0, 'micro avg_precision': 0.40318627450980393, 'micro avg_recall': 0.4404283801874163, 'micro avg_f1-score': 0.4209852847088931, 'micro avg_support': 747.0, 'macro avg_precision': 0.40318627450980393, 'macro avg_recall': 0.4404283801874163, 'macro avg_f1-score': 0.4209852847088931, 'macro avg_support': 747.0, 'weighted avg_precision': 0.40318627450980393, 'weighted avg_recall': 0.4404283801874163, 'weighted avg_f1-score': 0.4209852847088931, 'weighted avg_support': 747.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 5}, {'micro_f1': 0.9470252648736757, 'precision': 0.9470252648736757, 'Name_Calling-Labeling_precision': 0.3235294117647059, 'Name_Calling-Labeling_recall': 0.41379310344827586, 'Name_Calling-Labeling_f1-score': 0.3631361760660248, 'Name_Calling-Labeling_support': 638.0, 'micro avg_precision': 0.3235294117647059, 'micro avg_recall': 0.41379310344827586, 'micro avg_f1-score': 0.3631361760660248, 'micro avg_support': 638.0, 'macro avg_precision': 0.3235294117647059, 'macro avg_recall': 0.41379310344827586, 'macro avg_f1-score': 0.3631361760660248, 'macro avg_support': 638.0, 'weighted avg_precision': 0.3235294117647059, 'weighted avg_recall': 0.41379310344827586, 'weighted avg_f1-score': 0.3631361760660248, 'weighted avg_support': 638.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 6}, {'micro_f1': 0.9442180566874944, 'precision': 0.9442180566874944, 'Name_Calling-Labeling_precision': 0.375, 'Name_Calling-Labeling_recall': 0.4140730717185386, 'Name_Calling-Labeling_f1-score': 0.39356913183279746, 'Name_Calling-Labeling_support': 739.0, 'micro avg_precision': 0.375, 'micro avg_recall': 0.4140730717185386, 'micro avg_f1-score': 0.39356913183279746, 'micro avg_support': 739.0, 'macro avg_precision': 0.375, 'macro avg_recall': 0.4140730717185386, 'macro avg_f1-score': 0.39356913183279746, 'macro avg_support': 739.0, 'weighted avg_precision': 0.375, 'weighted avg_recall': 0.4140730717185386, 'weighted avg_f1-score': 0.39356913183279746, 'weighted avg_support': 739.0, 'O_support': 30096, 'B-Name_Calling-Labeling_support': 816, 'I-Name_Calling-Labeling_support': 2217, 'epoch': 7}]}
Early stopping triggered.
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_18_ME10_target=Name_Calling-Labeling_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 19 of 23 for (19, 'Doubt') persuasion technique...
{'micro_f1': 0.5840220385674931, 'precision': 0.5840220385674931, 'Doubt_precision': 0.0, 'Doubt_recall': 0.0, 'Doubt_f1-score': 0.0, 'Doubt_support': 189.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 189.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 189.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 189.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 0}
{'results': [{'micro_f1': 0.5840220385674931, 'precision': 0.5840220385674931, 'Doubt_precision': 0.0, 'Doubt_recall': 0.0, 'Doubt_f1-score': 0.0, 'Doubt_support': 189.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 189.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 189.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 189.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 0}]}
{'micro_f1': 0.6465808836349688, 'precision': 0.6465808836349688, 'Doubt_precision': 0.03203342618384401, 'Doubt_recall': 0.07033639143730887, 'Doubt_f1-score': 0.04401913875598087, 'Doubt_support': 327.0, 'micro avg_precision': 0.03203342618384401, 'micro avg_recall': 0.07033639143730887, 'micro avg_f1-score': 0.04401913875598087, 'micro avg_support': 327.0, 'macro avg_precision': 0.03203342618384401, 'macro avg_recall': 0.07033639143730887, 'macro avg_f1-score': 0.04401913875598087, 'macro avg_support': 327.0, 'weighted avg_precision': 0.03203342618384401, 'weighted avg_recall': 0.07033639143730887, 'weighted avg_f1-score': 0.04401913875598087, 'weighted avg_support': 327.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 1}
{'results': [{'micro_f1': 0.5840220385674931, 'precision': 0.5840220385674931, 'Doubt_precision': 0.0, 'Doubt_recall': 0.0, 'Doubt_f1-score': 0.0, 'Doubt_support': 189.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 189.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 189.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 189.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 0}, {'micro_f1': 0.6465808836349688, 'precision': 0.6465808836349688, 'Doubt_precision': 0.03203342618384401, 'Doubt_recall': 0.07033639143730887, 'Doubt_f1-score': 0.04401913875598087, 'Doubt_support': 327.0, 'micro avg_precision': 0.03203342618384401, 'micro avg_recall': 0.07033639143730887, 'micro avg_f1-score': 0.04401913875598087, 'micro avg_support': 327.0, 'macro avg_precision': 0.03203342618384401, 'macro avg_recall': 0.07033639143730887, 'macro avg_f1-score': 0.04401913875598087, 'macro avg_support': 327.0, 'weighted avg_precision': 0.03203342618384401, 'weighted avg_recall': 0.07033639143730887, 'weighted avg_f1-score': 0.04401913875598087, 'weighted avg_support': 327.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.04401913875598087
{'micro_f1': 0.7303065174181401, 'precision': 0.73030651741814, 'Doubt_precision': 0.14345403899721448, 'Doubt_recall': 0.15944272445820434, 'Doubt_f1-score': 0.15102639296187684, 'Doubt_support': 646.0, 'micro avg_precision': 0.14345403899721448, 'micro avg_recall': 0.15944272445820434, 'micro avg_f1-score': 0.15102639296187684, 'micro avg_support': 646.0, 'macro avg_precision': 0.14345403899721448, 'macro avg_recall': 0.15944272445820434, 'macro avg_f1-score': 0.15102639296187684, 'macro avg_support': 646.0, 'weighted avg_precision': 0.14345403899721448, 'weighted avg_recall': 0.15944272445820434, 'weighted avg_f1-score': 0.15102639296187684, 'weighted avg_support': 646.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 2}
{'results': [{'micro_f1': 0.5840220385674931, 'precision': 0.5840220385674931, 'Doubt_precision': 0.0, 'Doubt_recall': 0.0, 'Doubt_f1-score': 0.0, 'Doubt_support': 189.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 189.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 189.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 189.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 0}, {'micro_f1': 0.6465808836349688, 'precision': 0.6465808836349688, 'Doubt_precision': 0.03203342618384401, 'Doubt_recall': 0.07033639143730887, 'Doubt_f1-score': 0.04401913875598087, 'Doubt_support': 327.0, 'micro avg_precision': 0.03203342618384401, 'micro avg_recall': 0.07033639143730887, 'micro avg_f1-score': 0.04401913875598087, 'micro avg_support': 327.0, 'macro avg_precision': 0.03203342618384401, 'macro avg_recall': 0.07033639143730887, 'macro avg_f1-score': 0.04401913875598087, 'macro avg_support': 327.0, 'weighted avg_precision': 0.03203342618384401, 'weighted avg_recall': 0.07033639143730887, 'weighted avg_f1-score': 0.04401913875598087, 'weighted avg_support': 327.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 1}, {'micro_f1': 0.7303065174181401, 'precision': 0.73030651741814, 'Doubt_precision': 0.14345403899721448, 'Doubt_recall': 0.15944272445820434, 'Doubt_f1-score': 0.15102639296187684, 'Doubt_support': 646.0, 'micro avg_precision': 0.14345403899721448, 'micro avg_recall': 0.15944272445820434, 'micro avg_f1-score': 0.15102639296187684, 'micro avg_support': 646.0, 'macro avg_precision': 0.14345403899721448, 'macro avg_recall': 0.15944272445820434, 'macro avg_f1-score': 0.15102639296187684, 'macro avg_support': 646.0, 'weighted avg_precision': 0.14345403899721448, 'weighted avg_recall': 0.15944272445820434, 'weighted avg_f1-score': 0.15102639296187684, 'weighted avg_support': 646.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.15102639296187684
{'micro_f1': 0.7308295846845906, 'precision': 0.7308295846845905, 'Doubt_precision': 0.11142061281337047, 'Doubt_recall': 0.12213740458015267, 'Doubt_f1-score': 0.11653313911143481, 'Doubt_support': 655.0, 'micro avg_precision': 0.11142061281337047, 'micro avg_recall': 0.12213740458015267, 'micro avg_f1-score': 0.11653313911143481, 'micro avg_support': 655.0, 'macro avg_precision': 0.11142061281337047, 'macro avg_recall': 0.12213740458015267, 'macro avg_f1-score': 0.11653313911143481, 'macro avg_support': 655.0, 'weighted avg_precision': 0.11142061281337047, 'weighted avg_recall': 0.12213740458015267, 'weighted avg_f1-score': 0.11653313911143481, 'weighted avg_support': 655.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 3}
{'results': [{'micro_f1': 0.5840220385674931, 'precision': 0.5840220385674931, 'Doubt_precision': 0.0, 'Doubt_recall': 0.0, 'Doubt_f1-score': 0.0, 'Doubt_support': 189.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 189.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 189.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 189.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 0}, {'micro_f1': 0.6465808836349688, 'precision': 0.6465808836349688, 'Doubt_precision': 0.03203342618384401, 'Doubt_recall': 0.07033639143730887, 'Doubt_f1-score': 0.04401913875598087, 'Doubt_support': 327.0, 'micro avg_precision': 0.03203342618384401, 'micro avg_recall': 0.07033639143730887, 'micro avg_f1-score': 0.04401913875598087, 'micro avg_support': 327.0, 'macro avg_precision': 0.03203342618384401, 'macro avg_recall': 0.07033639143730887, 'macro avg_f1-score': 0.04401913875598087, 'macro avg_support': 327.0, 'weighted avg_precision': 0.03203342618384401, 'weighted avg_recall': 0.07033639143730887, 'weighted avg_f1-score': 0.04401913875598087, 'weighted avg_support': 327.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 1}, {'micro_f1': 0.7303065174181401, 'precision': 0.73030651741814, 'Doubt_precision': 0.14345403899721448, 'Doubt_recall': 0.15944272445820434, 'Doubt_f1-score': 0.15102639296187684, 'Doubt_support': 646.0, 'micro avg_precision': 0.14345403899721448, 'micro avg_recall': 0.15944272445820434, 'micro avg_f1-score': 0.15102639296187684, 'micro avg_support': 646.0, 'macro avg_precision': 0.14345403899721448, 'macro avg_recall': 0.15944272445820434, 'macro avg_f1-score': 0.15102639296187684, 'macro avg_support': 646.0, 'weighted avg_precision': 0.14345403899721448, 'weighted avg_recall': 0.15944272445820434, 'weighted avg_f1-score': 0.15102639296187684, 'weighted avg_support': 646.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 2}, {'micro_f1': 0.7308295846845906, 'precision': 0.7308295846845905, 'Doubt_precision': 0.11142061281337047, 'Doubt_recall': 0.12213740458015267, 'Doubt_f1-score': 0.11653313911143481, 'Doubt_support': 655.0, 'micro avg_precision': 0.11142061281337047, 'micro avg_recall': 0.12213740458015267, 'micro avg_f1-score': 0.11653313911143481, 'micro avg_support': 655.0, 'macro avg_precision': 0.11142061281337047, 'macro avg_recall': 0.12213740458015267, 'macro avg_f1-score': 0.11653313911143481, 'macro avg_support': 655.0, 'weighted avg_precision': 0.11142061281337047, 'weighted avg_recall': 0.12213740458015267, 'weighted avg_f1-score': 0.11653313911143481, 'weighted avg_support': 655.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 3}]}
{'micro_f1': 0.6922969627227394, 'precision': 0.6922969627227394, 'Doubt_precision': 0.08635097493036212, 'Doubt_recall': 0.11946050096339114, 'Doubt_f1-score': 0.10024252223120453, 'Doubt_support': 519.0, 'micro avg_precision': 0.08635097493036212, 'micro avg_recall': 0.11946050096339114, 'micro avg_f1-score': 0.10024252223120453, 'micro avg_support': 519.0, 'macro avg_precision': 0.08635097493036212, 'macro avg_recall': 0.11946050096339114, 'macro avg_f1-score': 0.10024252223120453, 'macro avg_support': 519.0, 'weighted avg_precision': 0.08635097493036212, 'weighted avg_recall': 0.11946050096339114, 'weighted avg_f1-score': 0.10024252223120453, 'weighted avg_support': 519.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 4}
{'results': [{'micro_f1': 0.5840220385674931, 'precision': 0.5840220385674931, 'Doubt_precision': 0.0, 'Doubt_recall': 0.0, 'Doubt_f1-score': 0.0, 'Doubt_support': 189.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 189.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 189.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 189.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 0}, {'micro_f1': 0.6465808836349688, 'precision': 0.6465808836349688, 'Doubt_precision': 0.03203342618384401, 'Doubt_recall': 0.07033639143730887, 'Doubt_f1-score': 0.04401913875598087, 'Doubt_support': 327.0, 'micro avg_precision': 0.03203342618384401, 'micro avg_recall': 0.07033639143730887, 'micro avg_f1-score': 0.04401913875598087, 'micro avg_support': 327.0, 'macro avg_precision': 0.03203342618384401, 'macro avg_recall': 0.07033639143730887, 'macro avg_f1-score': 0.04401913875598087, 'macro avg_support': 327.0, 'weighted avg_precision': 0.03203342618384401, 'weighted avg_recall': 0.07033639143730887, 'weighted avg_f1-score': 0.04401913875598087, 'weighted avg_support': 327.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 1}, {'micro_f1': 0.7303065174181401, 'precision': 0.73030651741814, 'Doubt_precision': 0.14345403899721448, 'Doubt_recall': 0.15944272445820434, 'Doubt_f1-score': 0.15102639296187684, 'Doubt_support': 646.0, 'micro avg_precision': 0.14345403899721448, 'micro avg_recall': 0.15944272445820434, 'micro avg_f1-score': 0.15102639296187684, 'micro avg_support': 646.0, 'macro avg_precision': 0.14345403899721448, 'macro avg_recall': 0.15944272445820434, 'macro avg_f1-score': 0.15102639296187684, 'macro avg_support': 646.0, 'weighted avg_precision': 0.14345403899721448, 'weighted avg_recall': 0.15944272445820434, 'weighted avg_f1-score': 0.15102639296187684, 'weighted avg_support': 646.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 2}, {'micro_f1': 0.7308295846845906, 'precision': 0.7308295846845905, 'Doubt_precision': 0.11142061281337047, 'Doubt_recall': 0.12213740458015267, 'Doubt_f1-score': 0.11653313911143481, 'Doubt_support': 655.0, 'micro avg_precision': 0.11142061281337047, 'micro avg_recall': 0.12213740458015267, 'micro avg_f1-score': 0.11653313911143481, 'micro avg_support': 655.0, 'macro avg_precision': 0.11142061281337047, 'macro avg_recall': 0.12213740458015267, 'macro avg_f1-score': 0.11653313911143481, 'macro avg_support': 655.0, 'weighted avg_precision': 0.11142061281337047, 'weighted avg_recall': 0.12213740458015267, 'weighted avg_f1-score': 0.11653313911143481, 'weighted avg_support': 655.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 3}, {'micro_f1': 0.6922969627227394, 'precision': 0.6922969627227394, 'Doubt_precision': 0.08635097493036212, 'Doubt_recall': 0.11946050096339114, 'Doubt_f1-score': 0.10024252223120453, 'Doubt_support': 519.0, 'micro avg_precision': 0.08635097493036212, 'micro avg_recall': 0.11946050096339114, 'micro avg_f1-score': 0.10024252223120453, 'micro avg_support': 519.0, 'macro avg_precision': 0.08635097493036212, 'macro avg_recall': 0.11946050096339114, 'macro avg_f1-score': 0.10024252223120453, 'macro avg_support': 519.0, 'weighted avg_precision': 0.08635097493036212, 'weighted avg_recall': 0.11946050096339114, 'weighted avg_f1-score': 0.10024252223120453, 'weighted avg_support': 519.0, 'B-Doubt_support': 718, 'I-Doubt_support': 11709, 'O_support': 16250, 'epoch': 4}]}
Early stopping triggered.
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_19_ME10_target=Doubt_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 20 of 23 for (20, 'Guilt_by_Association') persuasion technique...
{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}]}
Best model updated: current epoch macro f1 = 0.011627906976744186
{'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}, {'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.02352941176470588
{'micro_f1': 0.7994938132733408, 'precision': 0.7994938132733408, 'Guilt_by_Association_precision': 0.11764705882352941, 'Guilt_by_Association_recall': 0.05847953216374269, 'Guilt_by_Association_f1-score': 0.07812499999999999, 'Guilt_by_Association_support': 171.0, 'micro avg_precision': 0.11764705882352941, 'micro avg_recall': 0.05847953216374269, 'micro avg_f1-score': 0.07812499999999999, 'micro avg_support': 171.0, 'macro avg_precision': 0.11764705882352941, 'macro avg_recall': 0.05847953216374269, 'macro avg_f1-score': 0.07812499999999999, 'macro avg_support': 171.0, 'weighted avg_precision': 0.11764705882352941, 'weighted avg_recall': 0.05847953216374269, 'weighted avg_f1-score': 0.07812499999999999, 'weighted avg_support': 171.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 2}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}, {'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}, {'micro_f1': 0.7994938132733408, 'precision': 0.7994938132733408, 'Guilt_by_Association_precision': 0.11764705882352941, 'Guilt_by_Association_recall': 0.05847953216374269, 'Guilt_by_Association_f1-score': 0.07812499999999999, 'Guilt_by_Association_support': 171.0, 'micro avg_precision': 0.11764705882352941, 'micro avg_recall': 0.05847953216374269, 'micro avg_f1-score': 0.07812499999999999, 'micro avg_support': 171.0, 'macro avg_precision': 0.11764705882352941, 'macro avg_recall': 0.05847953216374269, 'macro avg_f1-score': 0.07812499999999999, 'macro avg_support': 171.0, 'weighted avg_precision': 0.11764705882352941, 'weighted avg_recall': 0.05847953216374269, 'weighted avg_f1-score': 0.07812499999999999, 'weighted avg_support': 171.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.07812499999999999
{'micro_f1': 0.796119235095613, 'precision': 0.796119235095613, 'Guilt_by_Association_precision': 0.12941176470588237, 'Guilt_by_Association_recall': 0.06358381502890173, 'Guilt_by_Association_f1-score': 0.08527131782945736, 'Guilt_by_Association_support': 173.0, 'micro avg_precision': 0.12941176470588237, 'micro avg_recall': 0.06358381502890173, 'micro avg_f1-score': 0.08527131782945736, 'micro avg_support': 173.0, 'macro avg_precision': 0.12941176470588237, 'macro avg_recall': 0.06358381502890173, 'macro avg_f1-score': 0.08527131782945736, 'macro avg_support': 173.0, 'weighted avg_precision': 0.12941176470588237, 'weighted avg_recall': 0.06358381502890173, 'weighted avg_f1-score': 0.08527131782945736, 'weighted avg_support': 173.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 3}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}, {'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}, {'micro_f1': 0.7994938132733408, 'precision': 0.7994938132733408, 'Guilt_by_Association_precision': 0.11764705882352941, 'Guilt_by_Association_recall': 0.05847953216374269, 'Guilt_by_Association_f1-score': 0.07812499999999999, 'Guilt_by_Association_support': 171.0, 'micro avg_precision': 0.11764705882352941, 'micro avg_recall': 0.05847953216374269, 'micro avg_f1-score': 0.07812499999999999, 'micro avg_support': 171.0, 'macro avg_precision': 0.11764705882352941, 'macro avg_recall': 0.05847953216374269, 'macro avg_f1-score': 0.07812499999999999, 'macro avg_support': 171.0, 'weighted avg_precision': 0.11764705882352941, 'weighted avg_recall': 0.05847953216374269, 'weighted avg_f1-score': 0.07812499999999999, 'weighted avg_support': 171.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 2}, {'micro_f1': 0.796119235095613, 'precision': 0.796119235095613, 'Guilt_by_Association_precision': 0.12941176470588237, 'Guilt_by_Association_recall': 0.06358381502890173, 'Guilt_by_Association_f1-score': 0.08527131782945736, 'Guilt_by_Association_support': 173.0, 'micro avg_precision': 0.12941176470588237, 'micro avg_recall': 0.06358381502890173, 'micro avg_f1-score': 0.08527131782945736, 'micro avg_support': 173.0, 'macro avg_precision': 0.12941176470588237, 'macro avg_recall': 0.06358381502890173, 'macro avg_f1-score': 0.08527131782945736, 'macro avg_support': 173.0, 'weighted avg_precision': 0.12941176470588237, 'weighted avg_recall': 0.06358381502890173, 'weighted avg_f1-score': 0.08527131782945736, 'weighted avg_support': 173.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.08527131782945736
{'micro_f1': 0.8079302587176603, 'precision': 0.8079302587176603, 'Guilt_by_Association_precision': 0.21176470588235294, 'Guilt_by_Association_recall': 0.12857142857142856, 'Guilt_by_Association_f1-score': 0.15999999999999998, 'Guilt_by_Association_support': 140.0, 'micro avg_precision': 0.21176470588235294, 'micro avg_recall': 0.12857142857142856, 'micro avg_f1-score': 0.15999999999999998, 'micro avg_support': 140.0, 'macro avg_precision': 0.21176470588235294, 'macro avg_recall': 0.12857142857142856, 'macro avg_f1-score': 0.15999999999999998, 'macro avg_support': 140.0, 'weighted avg_precision': 0.21176470588235294, 'weighted avg_recall': 0.12857142857142856, 'weighted avg_f1-score': 0.15999999999999998, 'weighted avg_support': 140.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 4}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}, {'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}, {'micro_f1': 0.7994938132733408, 'precision': 0.7994938132733408, 'Guilt_by_Association_precision': 0.11764705882352941, 'Guilt_by_Association_recall': 0.05847953216374269, 'Guilt_by_Association_f1-score': 0.07812499999999999, 'Guilt_by_Association_support': 171.0, 'micro avg_precision': 0.11764705882352941, 'micro avg_recall': 0.05847953216374269, 'micro avg_f1-score': 0.07812499999999999, 'micro avg_support': 171.0, 'macro avg_precision': 0.11764705882352941, 'macro avg_recall': 0.05847953216374269, 'macro avg_f1-score': 0.07812499999999999, 'macro avg_support': 171.0, 'weighted avg_precision': 0.11764705882352941, 'weighted avg_recall': 0.05847953216374269, 'weighted avg_f1-score': 0.07812499999999999, 'weighted avg_support': 171.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 2}, {'micro_f1': 0.796119235095613, 'precision': 0.796119235095613, 'Guilt_by_Association_precision': 0.12941176470588237, 'Guilt_by_Association_recall': 0.06358381502890173, 'Guilt_by_Association_f1-score': 0.08527131782945736, 'Guilt_by_Association_support': 173.0, 'micro avg_precision': 0.12941176470588237, 'micro avg_recall': 0.06358381502890173, 'micro avg_f1-score': 0.08527131782945736, 'micro avg_support': 173.0, 'macro avg_precision': 0.12941176470588237, 'macro avg_recall': 0.06358381502890173, 'macro avg_f1-score': 0.08527131782945736, 'macro avg_support': 173.0, 'weighted avg_precision': 0.12941176470588237, 'weighted avg_recall': 0.06358381502890173, 'weighted avg_f1-score': 0.08527131782945736, 'weighted avg_support': 173.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 3}, {'micro_f1': 0.8079302587176603, 'precision': 0.8079302587176603, 'Guilt_by_Association_precision': 0.21176470588235294, 'Guilt_by_Association_recall': 0.12857142857142856, 'Guilt_by_Association_f1-score': 0.15999999999999998, 'Guilt_by_Association_support': 140.0, 'micro avg_precision': 0.21176470588235294, 'micro avg_recall': 0.12857142857142856, 'micro avg_f1-score': 0.15999999999999998, 'micro avg_support': 140.0, 'macro avg_precision': 0.21176470588235294, 'macro avg_recall': 0.12857142857142856, 'macro avg_f1-score': 0.15999999999999998, 'macro avg_support': 140.0, 'weighted avg_precision': 0.21176470588235294, 'weighted avg_recall': 0.12857142857142856, 'weighted avg_f1-score': 0.15999999999999998, 'weighted avg_support': 140.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.15999999999999998
{'micro_f1': 0.7789651293588301, 'precision': 0.7789651293588301, 'Guilt_by_Association_precision': 0.07058823529411765, 'Guilt_by_Association_recall': 0.05555555555555555, 'Guilt_by_Association_f1-score': 0.06217616580310881, 'Guilt_by_Association_support': 108.0, 'micro avg_precision': 0.07058823529411765, 'micro avg_recall': 0.05555555555555555, 'micro avg_f1-score': 0.06217616580310881, 'micro avg_support': 108.0, 'macro avg_precision': 0.07058823529411765, 'macro avg_recall': 0.05555555555555555, 'macro avg_f1-score': 0.06217616580310881, 'macro avg_support': 108.0, 'weighted avg_precision': 0.07058823529411765, 'weighted avg_recall': 0.05555555555555555, 'weighted avg_f1-score': 0.06217616580310881, 'weighted avg_support': 108.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 5}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}, {'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}, {'micro_f1': 0.7994938132733408, 'precision': 0.7994938132733408, 'Guilt_by_Association_precision': 0.11764705882352941, 'Guilt_by_Association_recall': 0.05847953216374269, 'Guilt_by_Association_f1-score': 0.07812499999999999, 'Guilt_by_Association_support': 171.0, 'micro avg_precision': 0.11764705882352941, 'micro avg_recall': 0.05847953216374269, 'micro avg_f1-score': 0.07812499999999999, 'micro avg_support': 171.0, 'macro avg_precision': 0.11764705882352941, 'macro avg_recall': 0.05847953216374269, 'macro avg_f1-score': 0.07812499999999999, 'macro avg_support': 171.0, 'weighted avg_precision': 0.11764705882352941, 'weighted avg_recall': 0.05847953216374269, 'weighted avg_f1-score': 0.07812499999999999, 'weighted avg_support': 171.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 2}, {'micro_f1': 0.796119235095613, 'precision': 0.796119235095613, 'Guilt_by_Association_precision': 0.12941176470588237, 'Guilt_by_Association_recall': 0.06358381502890173, 'Guilt_by_Association_f1-score': 0.08527131782945736, 'Guilt_by_Association_support': 173.0, 'micro avg_precision': 0.12941176470588237, 'micro avg_recall': 0.06358381502890173, 'micro avg_f1-score': 0.08527131782945736, 'micro avg_support': 173.0, 'macro avg_precision': 0.12941176470588237, 'macro avg_recall': 0.06358381502890173, 'macro avg_f1-score': 0.08527131782945736, 'macro avg_support': 173.0, 'weighted avg_precision': 0.12941176470588237, 'weighted avg_recall': 0.06358381502890173, 'weighted avg_f1-score': 0.08527131782945736, 'weighted avg_support': 173.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 3}, {'micro_f1': 0.8079302587176603, 'precision': 0.8079302587176603, 'Guilt_by_Association_precision': 0.21176470588235294, 'Guilt_by_Association_recall': 0.12857142857142856, 'Guilt_by_Association_f1-score': 0.15999999999999998, 'Guilt_by_Association_support': 140.0, 'micro avg_precision': 0.21176470588235294, 'micro avg_recall': 0.12857142857142856, 'micro avg_f1-score': 0.15999999999999998, 'micro avg_support': 140.0, 'macro avg_precision': 0.21176470588235294, 'macro avg_recall': 0.12857142857142856, 'macro avg_f1-score': 0.15999999999999998, 'macro avg_support': 140.0, 'weighted avg_precision': 0.21176470588235294, 'weighted avg_recall': 0.12857142857142856, 'weighted avg_f1-score': 0.15999999999999998, 'weighted avg_support': 140.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 4}, {'micro_f1': 0.7789651293588301, 'precision': 0.7789651293588301, 'Guilt_by_Association_precision': 0.07058823529411765, 'Guilt_by_Association_recall': 0.05555555555555555, 'Guilt_by_Association_f1-score': 0.06217616580310881, 'Guilt_by_Association_support': 108.0, 'micro avg_precision': 0.07058823529411765, 'micro avg_recall': 0.05555555555555555, 'micro avg_f1-score': 0.06217616580310881, 'micro avg_support': 108.0, 'macro avg_precision': 0.07058823529411765, 'macro avg_recall': 0.05555555555555555, 'macro avg_f1-score': 0.06217616580310881, 'macro avg_support': 108.0, 'weighted avg_precision': 0.07058823529411765, 'weighted avg_recall': 0.05555555555555555, 'weighted avg_f1-score': 0.06217616580310881, 'weighted avg_support': 108.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 5}]}
{'micro_f1': 0.8082114735658043, 'precision': 0.8082114735658043, 'Guilt_by_Association_precision': 0.17647058823529413, 'Guilt_by_Association_recall': 0.09259259259259259, 'Guilt_by_Association_f1-score': 0.12145748987854252, 'Guilt_by_Association_support': 162.0, 'micro avg_precision': 0.17647058823529413, 'micro avg_recall': 0.09259259259259259, 'micro avg_f1-score': 0.12145748987854252, 'micro avg_support': 162.0, 'macro avg_precision': 0.17647058823529413, 'macro avg_recall': 0.09259259259259259, 'macro avg_f1-score': 0.12145748987854252, 'macro avg_support': 162.0, 'weighted avg_precision': 0.17647058823529413, 'weighted avg_recall': 0.09259259259259259, 'weighted avg_f1-score': 0.12145748987854252, 'weighted avg_support': 162.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 6}
{'results': [{'micro_f1': 0.7019122609673791, 'precision': 0.7019122609673791, 'Guilt_by_Association_precision': 0.011764705882352941, 'Guilt_by_Association_recall': 0.011494252873563218, 'Guilt_by_Association_f1-score': 0.011627906976744186, 'Guilt_by_Association_support': 87.0, 'micro avg_precision': 0.011764705882352941, 'micro avg_recall': 0.011494252873563218, 'micro avg_f1-score': 0.011627906976744186, 'micro avg_support': 87.0, 'macro avg_precision': 0.011764705882352941, 'macro avg_recall': 0.011494252873563218, 'macro avg_f1-score': 0.011627906976744186, 'macro avg_support': 87.0, 'weighted avg_precision': 0.011764705882352941, 'weighted avg_recall': 0.011494252873563218, 'weighted avg_f1-score': 0.011627906976744186, 'weighted avg_support': 87.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 0}, {'micro_f1': 0.7336895388076491, 'precision': 0.7336895388076491, 'Guilt_by_Association_precision': 0.023529411764705882, 'Guilt_by_Association_recall': 0.023529411764705882, 'Guilt_by_Association_f1-score': 0.02352941176470588, 'Guilt_by_Association_support': 85.0, 'micro avg_precision': 0.023529411764705882, 'micro avg_recall': 0.023529411764705882, 'micro avg_f1-score': 0.02352941176470588, 'micro avg_support': 85.0, 'macro avg_precision': 0.023529411764705882, 'macro avg_recall': 0.023529411764705882, 'macro avg_f1-score': 0.02352941176470588, 'macro avg_support': 85.0, 'weighted avg_precision': 0.023529411764705882, 'weighted avg_recall': 0.023529411764705882, 'weighted avg_f1-score': 0.02352941176470588, 'weighted avg_support': 85.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 1}, {'micro_f1': 0.7994938132733408, 'precision': 0.7994938132733408, 'Guilt_by_Association_precision': 0.11764705882352941, 'Guilt_by_Association_recall': 0.05847953216374269, 'Guilt_by_Association_f1-score': 0.07812499999999999, 'Guilt_by_Association_support': 171.0, 'micro avg_precision': 0.11764705882352941, 'micro avg_recall': 0.05847953216374269, 'micro avg_f1-score': 0.07812499999999999, 'micro avg_support': 171.0, 'macro avg_precision': 0.11764705882352941, 'macro avg_recall': 0.05847953216374269, 'macro avg_f1-score': 0.07812499999999999, 'macro avg_support': 171.0, 'weighted avg_precision': 0.11764705882352941, 'weighted avg_recall': 0.05847953216374269, 'weighted avg_f1-score': 0.07812499999999999, 'weighted avg_support': 171.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 2}, {'micro_f1': 0.796119235095613, 'precision': 0.796119235095613, 'Guilt_by_Association_precision': 0.12941176470588237, 'Guilt_by_Association_recall': 0.06358381502890173, 'Guilt_by_Association_f1-score': 0.08527131782945736, 'Guilt_by_Association_support': 173.0, 'micro avg_precision': 0.12941176470588237, 'micro avg_recall': 0.06358381502890173, 'micro avg_f1-score': 0.08527131782945736, 'micro avg_support': 173.0, 'macro avg_precision': 0.12941176470588237, 'macro avg_recall': 0.06358381502890173, 'macro avg_f1-score': 0.08527131782945736, 'macro avg_support': 173.0, 'weighted avg_precision': 0.12941176470588237, 'weighted avg_recall': 0.06358381502890173, 'weighted avg_f1-score': 0.08527131782945736, 'weighted avg_support': 173.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 3}, {'micro_f1': 0.8079302587176603, 'precision': 0.8079302587176603, 'Guilt_by_Association_precision': 0.21176470588235294, 'Guilt_by_Association_recall': 0.12857142857142856, 'Guilt_by_Association_f1-score': 0.15999999999999998, 'Guilt_by_Association_support': 140.0, 'micro avg_precision': 0.21176470588235294, 'micro avg_recall': 0.12857142857142856, 'micro avg_f1-score': 0.15999999999999998, 'micro avg_support': 140.0, 'macro avg_precision': 0.21176470588235294, 'macro avg_recall': 0.12857142857142856, 'macro avg_f1-score': 0.15999999999999998, 'macro avg_support': 140.0, 'weighted avg_precision': 0.21176470588235294, 'weighted avg_recall': 0.12857142857142856, 'weighted avg_f1-score': 0.15999999999999998, 'weighted avg_support': 140.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 4}, {'micro_f1': 0.7789651293588301, 'precision': 0.7789651293588301, 'Guilt_by_Association_precision': 0.07058823529411765, 'Guilt_by_Association_recall': 0.05555555555555555, 'Guilt_by_Association_f1-score': 0.06217616580310881, 'Guilt_by_Association_support': 108.0, 'micro avg_precision': 0.07058823529411765, 'micro avg_recall': 0.05555555555555555, 'micro avg_f1-score': 0.06217616580310881, 'micro avg_support': 108.0, 'macro avg_precision': 0.07058823529411765, 'macro avg_recall': 0.05555555555555555, 'macro avg_f1-score': 0.06217616580310881, 'macro avg_support': 108.0, 'weighted avg_precision': 0.07058823529411765, 'weighted avg_recall': 0.05555555555555555, 'weighted avg_f1-score': 0.06217616580310881, 'weighted avg_support': 108.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 5}, {'micro_f1': 0.8082114735658043, 'precision': 0.8082114735658043, 'Guilt_by_Association_precision': 0.17647058823529413, 'Guilt_by_Association_recall': 0.09259259259259259, 'Guilt_by_Association_f1-score': 0.12145748987854252, 'Guilt_by_Association_support': 162.0, 'micro avg_precision': 0.17647058823529413, 'micro avg_recall': 0.09259259259259259, 'micro avg_f1-score': 0.12145748987854252, 'micro avg_support': 162.0, 'macro avg_precision': 0.17647058823529413, 'macro avg_recall': 0.09259259259259259, 'macro avg_f1-score': 0.12145748987854252, 'macro avg_support': 162.0, 'weighted avg_precision': 0.17647058823529413, 'weighted avg_recall': 0.09259259259259259, 'weighted avg_f1-score': 0.12145748987854252, 'weighted avg_support': 162.0, 'O_support': 2201, 'B-Guilt_by_Association_support': 85, 'I-Guilt_by_Association_support': 1270, 'epoch': 6}]}
Early stopping triggered.
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_20_ME10_target=Guilt_by_Association_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 21 of 23 for (21, 'Appeal_to_Hypocrisy') persuasion technique...
{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}]}
{'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.005319148936170213
{'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.05934718100890208
{'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.15882352941176472
{'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}, {'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.17506631299734748
{'micro_f1': 0.7670120898100171, 'precision': 0.7670120898100172, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1459227467811159, 'Appeal_to_Hypocrisy_f1-score': 0.1842818428184282, 'Appeal_to_Hypocrisy_support': 233.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1459227467811159, 'micro avg_f1-score': 0.1842818428184282, 'micro avg_support': 233.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1459227467811159, 'macro avg_f1-score': 0.1842818428184282, 'macro avg_support': 233.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1459227467811159, 'weighted avg_f1-score': 0.1842818428184282, 'weighted avg_support': 233.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 5}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}, {'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}, {'micro_f1': 0.7670120898100171, 'precision': 0.7670120898100172, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1459227467811159, 'Appeal_to_Hypocrisy_f1-score': 0.1842818428184282, 'Appeal_to_Hypocrisy_support': 233.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1459227467811159, 'micro avg_f1-score': 0.1842818428184282, 'micro avg_support': 233.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1459227467811159, 'macro avg_f1-score': 0.1842818428184282, 'macro avg_support': 233.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1459227467811159, 'weighted avg_f1-score': 0.1842818428184282, 'weighted avg_support': 233.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 5}]}
Best model updated: current epoch macro f1 = 0.1842818428184282
{'micro_f1': 0.7789291882556131, 'precision': 0.7789291882556131, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1588785046728972, 'Appeal_to_Hypocrisy_f1-score': 0.19428571428571426, 'Appeal_to_Hypocrisy_support': 214.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1588785046728972, 'micro avg_f1-score': 0.19428571428571426, 'micro avg_support': 214.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1588785046728972, 'macro avg_f1-score': 0.19428571428571426, 'macro avg_support': 214.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1588785046728972, 'weighted avg_f1-score': 0.19428571428571426, 'weighted avg_support': 214.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 6}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}, {'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}, {'micro_f1': 0.7670120898100171, 'precision': 0.7670120898100172, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1459227467811159, 'Appeal_to_Hypocrisy_f1-score': 0.1842818428184282, 'Appeal_to_Hypocrisy_support': 233.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1459227467811159, 'micro avg_f1-score': 0.1842818428184282, 'micro avg_support': 233.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1459227467811159, 'macro avg_f1-score': 0.1842818428184282, 'macro avg_support': 233.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1459227467811159, 'weighted avg_f1-score': 0.1842818428184282, 'weighted avg_support': 233.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 5}, {'micro_f1': 0.7789291882556131, 'precision': 0.7789291882556131, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1588785046728972, 'Appeal_to_Hypocrisy_f1-score': 0.19428571428571426, 'Appeal_to_Hypocrisy_support': 214.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1588785046728972, 'micro avg_f1-score': 0.19428571428571426, 'micro avg_support': 214.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1588785046728972, 'macro avg_f1-score': 0.19428571428571426, 'macro avg_support': 214.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1588785046728972, 'weighted avg_f1-score': 0.19428571428571426, 'weighted avg_support': 214.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 6}]}
Best model updated: current epoch macro f1 = 0.19428571428571426
{'micro_f1': 0.7711571675302246, 'precision': 0.7711571675302246, 'Appeal_to_Hypocrisy_precision': 0.21323529411764705, 'Appeal_to_Hypocrisy_recall': 0.1380952380952381, 'Appeal_to_Hypocrisy_f1-score': 0.1676300578034682, 'Appeal_to_Hypocrisy_support': 210.0, 'micro avg_precision': 0.21323529411764705, 'micro avg_recall': 0.1380952380952381, 'micro avg_f1-score': 0.1676300578034682, 'micro avg_support': 210.0, 'macro avg_precision': 0.21323529411764705, 'macro avg_recall': 0.1380952380952381, 'macro avg_f1-score': 0.1676300578034682, 'macro avg_support': 210.0, 'weighted avg_precision': 0.21323529411764708, 'weighted avg_recall': 0.1380952380952381, 'weighted avg_f1-score': 0.1676300578034682, 'weighted avg_support': 210.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 7}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}, {'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}, {'micro_f1': 0.7670120898100171, 'precision': 0.7670120898100172, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1459227467811159, 'Appeal_to_Hypocrisy_f1-score': 0.1842818428184282, 'Appeal_to_Hypocrisy_support': 233.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1459227467811159, 'micro avg_f1-score': 0.1842818428184282, 'micro avg_support': 233.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1459227467811159, 'macro avg_f1-score': 0.1842818428184282, 'macro avg_support': 233.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1459227467811159, 'weighted avg_f1-score': 0.1842818428184282, 'weighted avg_support': 233.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 5}, {'micro_f1': 0.7789291882556131, 'precision': 0.7789291882556131, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1588785046728972, 'Appeal_to_Hypocrisy_f1-score': 0.19428571428571426, 'Appeal_to_Hypocrisy_support': 214.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1588785046728972, 'micro avg_f1-score': 0.19428571428571426, 'micro avg_support': 214.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1588785046728972, 'macro avg_f1-score': 0.19428571428571426, 'macro avg_support': 214.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1588785046728972, 'weighted avg_f1-score': 0.19428571428571426, 'weighted avg_support': 214.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 6}, {'micro_f1': 0.7711571675302246, 'precision': 0.7711571675302246, 'Appeal_to_Hypocrisy_precision': 0.21323529411764705, 'Appeal_to_Hypocrisy_recall': 0.1380952380952381, 'Appeal_to_Hypocrisy_f1-score': 0.1676300578034682, 'Appeal_to_Hypocrisy_support': 210.0, 'micro avg_precision': 0.21323529411764705, 'micro avg_recall': 0.1380952380952381, 'micro avg_f1-score': 0.1676300578034682, 'micro avg_support': 210.0, 'macro avg_precision': 0.21323529411764705, 'macro avg_recall': 0.1380952380952381, 'macro avg_f1-score': 0.1676300578034682, 'macro avg_support': 210.0, 'weighted avg_precision': 0.21323529411764708, 'weighted avg_recall': 0.1380952380952381, 'weighted avg_f1-score': 0.1676300578034682, 'weighted avg_support': 210.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 7}]}
{'micro_f1': 0.7746113989637305, 'precision': 0.7746113989637305, 'Appeal_to_Hypocrisy_precision': 0.2647058823529412, 'Appeal_to_Hypocrisy_recall': 0.15789473684210525, 'Appeal_to_Hypocrisy_f1-score': 0.1978021978021978, 'Appeal_to_Hypocrisy_support': 228.0, 'micro avg_precision': 0.2647058823529412, 'micro avg_recall': 0.15789473684210525, 'micro avg_f1-score': 0.1978021978021978, 'micro avg_support': 228.0, 'macro avg_precision': 0.2647058823529412, 'macro avg_recall': 0.15789473684210525, 'macro avg_f1-score': 0.1978021978021978, 'macro avg_support': 228.0, 'weighted avg_precision': 0.2647058823529412, 'weighted avg_recall': 0.15789473684210525, 'weighted avg_f1-score': 0.1978021978021978, 'weighted avg_support': 228.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 8}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}, {'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}, {'micro_f1': 0.7670120898100171, 'precision': 0.7670120898100172, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1459227467811159, 'Appeal_to_Hypocrisy_f1-score': 0.1842818428184282, 'Appeal_to_Hypocrisy_support': 233.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1459227467811159, 'micro avg_f1-score': 0.1842818428184282, 'micro avg_support': 233.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1459227467811159, 'macro avg_f1-score': 0.1842818428184282, 'macro avg_support': 233.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1459227467811159, 'weighted avg_f1-score': 0.1842818428184282, 'weighted avg_support': 233.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 5}, {'micro_f1': 0.7789291882556131, 'precision': 0.7789291882556131, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1588785046728972, 'Appeal_to_Hypocrisy_f1-score': 0.19428571428571426, 'Appeal_to_Hypocrisy_support': 214.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1588785046728972, 'micro avg_f1-score': 0.19428571428571426, 'micro avg_support': 214.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1588785046728972, 'macro avg_f1-score': 0.19428571428571426, 'macro avg_support': 214.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1588785046728972, 'weighted avg_f1-score': 0.19428571428571426, 'weighted avg_support': 214.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 6}, {'micro_f1': 0.7711571675302246, 'precision': 0.7711571675302246, 'Appeal_to_Hypocrisy_precision': 0.21323529411764705, 'Appeal_to_Hypocrisy_recall': 0.1380952380952381, 'Appeal_to_Hypocrisy_f1-score': 0.1676300578034682, 'Appeal_to_Hypocrisy_support': 210.0, 'micro avg_precision': 0.21323529411764705, 'micro avg_recall': 0.1380952380952381, 'micro avg_f1-score': 0.1676300578034682, 'micro avg_support': 210.0, 'macro avg_precision': 0.21323529411764705, 'macro avg_recall': 0.1380952380952381, 'macro avg_f1-score': 0.1676300578034682, 'macro avg_support': 210.0, 'weighted avg_precision': 0.21323529411764708, 'weighted avg_recall': 0.1380952380952381, 'weighted avg_f1-score': 0.1676300578034682, 'weighted avg_support': 210.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 7}, {'micro_f1': 0.7746113989637305, 'precision': 0.7746113989637305, 'Appeal_to_Hypocrisy_precision': 0.2647058823529412, 'Appeal_to_Hypocrisy_recall': 0.15789473684210525, 'Appeal_to_Hypocrisy_f1-score': 0.1978021978021978, 'Appeal_to_Hypocrisy_support': 228.0, 'micro avg_precision': 0.2647058823529412, 'micro avg_recall': 0.15789473684210525, 'micro avg_f1-score': 0.1978021978021978, 'micro avg_support': 228.0, 'macro avg_precision': 0.2647058823529412, 'macro avg_recall': 0.15789473684210525, 'macro avg_f1-score': 0.1978021978021978, 'macro avg_support': 228.0, 'weighted avg_precision': 0.2647058823529412, 'weighted avg_recall': 0.15789473684210525, 'weighted avg_f1-score': 0.1978021978021978, 'weighted avg_support': 228.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 8}]}
Best model updated: current epoch macro f1 = 0.1978021978021978
{'micro_f1': 0.7744386873920552, 'precision': 0.7744386873920552, 'Appeal_to_Hypocrisy_precision': 0.21323529411764705, 'Appeal_to_Hypocrisy_recall': 0.14795918367346939, 'Appeal_to_Hypocrisy_f1-score': 0.17469879518072287, 'Appeal_to_Hypocrisy_support': 196.0, 'micro avg_precision': 0.21323529411764705, 'micro avg_recall': 0.14795918367346939, 'micro avg_f1-score': 0.17469879518072287, 'micro avg_support': 196.0, 'macro avg_precision': 0.21323529411764705, 'macro avg_recall': 0.14795918367346939, 'macro avg_f1-score': 0.17469879518072287, 'macro avg_support': 196.0, 'weighted avg_precision': 0.21323529411764702, 'weighted avg_recall': 0.14795918367346939, 'weighted avg_f1-score': 0.17469879518072287, 'weighted avg_support': 196.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 9}
{'results': [{'micro_f1': 0.5110535405872193, 'precision': 0.5110535405872193, 'Appeal_to_Hypocrisy_precision': 0.0, 'Appeal_to_Hypocrisy_recall': 0.0, 'Appeal_to_Hypocrisy_f1-score': 0.0, 'Appeal_to_Hypocrisy_support': 109.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 109.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 109.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 109.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 0}, {'micro_f1': 0.6269430051813472, 'precision': 0.6269430051813472, 'Appeal_to_Hypocrisy_precision': 0.007352941176470588, 'Appeal_to_Hypocrisy_recall': 0.004166666666666667, 'Appeal_to_Hypocrisy_f1-score': 0.005319148936170213, 'Appeal_to_Hypocrisy_support': 240.0, 'micro avg_precision': 0.007352941176470588, 'micro avg_recall': 0.004166666666666667, 'micro avg_f1-score': 0.005319148936170213, 'micro avg_support': 240.0, 'macro avg_precision': 0.007352941176470588, 'macro avg_recall': 0.004166666666666667, 'macro avg_f1-score': 0.005319148936170213, 'macro avg_support': 240.0, 'weighted avg_precision': 0.007352941176470588, 'weighted avg_recall': 0.004166666666666667, 'weighted avg_f1-score': 0.005319148936170213, 'weighted avg_support': 240.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 1}, {'micro_f1': 0.7493955094991364, 'precision': 0.7493955094991365, 'Appeal_to_Hypocrisy_precision': 0.07352941176470588, 'Appeal_to_Hypocrisy_recall': 0.04975124378109453, 'Appeal_to_Hypocrisy_f1-score': 0.05934718100890208, 'Appeal_to_Hypocrisy_support': 201.0, 'micro avg_precision': 0.07352941176470588, 'micro avg_recall': 0.04975124378109453, 'micro avg_f1-score': 0.05934718100890208, 'micro avg_support': 201.0, 'macro avg_precision': 0.07352941176470588, 'macro avg_recall': 0.04975124378109453, 'macro avg_f1-score': 0.05934718100890208, 'macro avg_support': 201.0, 'weighted avg_precision': 0.07352941176470588, 'weighted avg_recall': 0.04975124378109453, 'weighted avg_f1-score': 0.05934718100890208, 'weighted avg_support': 201.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 2}, {'micro_f1': 0.7578583765112261, 'precision': 0.7578583765112262, 'Appeal_to_Hypocrisy_precision': 0.19852941176470587, 'Appeal_to_Hypocrisy_recall': 0.1323529411764706, 'Appeal_to_Hypocrisy_f1-score': 0.15882352941176472, 'Appeal_to_Hypocrisy_support': 204.0, 'micro avg_precision': 0.19852941176470587, 'micro avg_recall': 0.1323529411764706, 'micro avg_f1-score': 0.15882352941176472, 'micro avg_support': 204.0, 'macro avg_precision': 0.19852941176470587, 'macro avg_recall': 0.1323529411764706, 'macro avg_f1-score': 0.15882352941176472, 'macro avg_support': 204.0, 'weighted avg_precision': 0.19852941176470587, 'weighted avg_recall': 0.1323529411764706, 'weighted avg_f1-score': 0.15882352941176472, 'weighted avg_support': 204.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 3}, {'micro_f1': 0.7697754749568221, 'precision': 0.7697754749568221, 'Appeal_to_Hypocrisy_precision': 0.2426470588235294, 'Appeal_to_Hypocrisy_recall': 0.13692946058091288, 'Appeal_to_Hypocrisy_f1-score': 0.17506631299734748, 'Appeal_to_Hypocrisy_support': 241.0, 'micro avg_precision': 0.2426470588235294, 'micro avg_recall': 0.13692946058091288, 'micro avg_f1-score': 0.17506631299734748, 'micro avg_support': 241.0, 'macro avg_precision': 0.2426470588235294, 'macro avg_recall': 0.13692946058091288, 'macro avg_f1-score': 0.17506631299734748, 'macro avg_support': 241.0, 'weighted avg_precision': 0.2426470588235294, 'weighted avg_recall': 0.13692946058091288, 'weighted avg_f1-score': 0.17506631299734748, 'weighted avg_support': 241.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 4}, {'micro_f1': 0.7670120898100171, 'precision': 0.7670120898100172, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1459227467811159, 'Appeal_to_Hypocrisy_f1-score': 0.1842818428184282, 'Appeal_to_Hypocrisy_support': 233.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1459227467811159, 'micro avg_f1-score': 0.1842818428184282, 'micro avg_support': 233.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1459227467811159, 'macro avg_f1-score': 0.1842818428184282, 'macro avg_support': 233.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1459227467811159, 'weighted avg_f1-score': 0.1842818428184282, 'weighted avg_support': 233.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 5}, {'micro_f1': 0.7789291882556131, 'precision': 0.7789291882556131, 'Appeal_to_Hypocrisy_precision': 0.25, 'Appeal_to_Hypocrisy_recall': 0.1588785046728972, 'Appeal_to_Hypocrisy_f1-score': 0.19428571428571426, 'Appeal_to_Hypocrisy_support': 214.0, 'micro avg_precision': 0.25, 'micro avg_recall': 0.1588785046728972, 'micro avg_f1-score': 0.19428571428571426, 'micro avg_support': 214.0, 'macro avg_precision': 0.25, 'macro avg_recall': 0.1588785046728972, 'macro avg_f1-score': 0.19428571428571426, 'macro avg_support': 214.0, 'weighted avg_precision': 0.25, 'weighted avg_recall': 0.1588785046728972, 'weighted avg_f1-score': 0.19428571428571426, 'weighted avg_support': 214.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 6}, {'micro_f1': 0.7711571675302246, 'precision': 0.7711571675302246, 'Appeal_to_Hypocrisy_precision': 0.21323529411764705, 'Appeal_to_Hypocrisy_recall': 0.1380952380952381, 'Appeal_to_Hypocrisy_f1-score': 0.1676300578034682, 'Appeal_to_Hypocrisy_support': 210.0, 'micro avg_precision': 0.21323529411764705, 'micro avg_recall': 0.1380952380952381, 'micro avg_f1-score': 0.1676300578034682, 'micro avg_support': 210.0, 'macro avg_precision': 0.21323529411764705, 'macro avg_recall': 0.1380952380952381, 'macro avg_f1-score': 0.1676300578034682, 'macro avg_support': 210.0, 'weighted avg_precision': 0.21323529411764708, 'weighted avg_recall': 0.1380952380952381, 'weighted avg_f1-score': 0.1676300578034682, 'weighted avg_support': 210.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 7}, {'micro_f1': 0.7746113989637305, 'precision': 0.7746113989637305, 'Appeal_to_Hypocrisy_precision': 0.2647058823529412, 'Appeal_to_Hypocrisy_recall': 0.15789473684210525, 'Appeal_to_Hypocrisy_f1-score': 0.1978021978021978, 'Appeal_to_Hypocrisy_support': 228.0, 'micro avg_precision': 0.2647058823529412, 'micro avg_recall': 0.15789473684210525, 'micro avg_f1-score': 0.1978021978021978, 'micro avg_support': 228.0, 'macro avg_precision': 0.2647058823529412, 'macro avg_recall': 0.15789473684210525, 'macro avg_f1-score': 0.1978021978021978, 'macro avg_support': 228.0, 'weighted avg_precision': 0.2647058823529412, 'weighted avg_recall': 0.15789473684210525, 'weighted avg_f1-score': 0.1978021978021978, 'weighted avg_support': 228.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 8}, {'micro_f1': 0.7744386873920552, 'precision': 0.7744386873920552, 'Appeal_to_Hypocrisy_precision': 0.21323529411764705, 'Appeal_to_Hypocrisy_recall': 0.14795918367346939, 'Appeal_to_Hypocrisy_f1-score': 0.17469879518072287, 'Appeal_to_Hypocrisy_support': 196.0, 'micro avg_precision': 0.21323529411764705, 'micro avg_recall': 0.14795918367346939, 'micro avg_f1-score': 0.17469879518072287, 'micro avg_support': 196.0, 'macro avg_precision': 0.21323529411764705, 'macro avg_recall': 0.14795918367346939, 'macro avg_f1-score': 0.17469879518072287, 'macro avg_support': 196.0, 'weighted avg_precision': 0.21323529411764702, 'weighted avg_recall': 0.14795918367346939, 'weighted avg_f1-score': 0.17469879518072287, 'weighted avg_support': 196.0, 'O_support': 2775, 'B-Appeal_to_Hypocrisy_support': 136, 'I-Appeal_to_Hypocrisy_support': 2879, 'epoch': 9}]}
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_21_ME10_target=Appeal_to_Hypocrisy_SUBSAMPLED_2024-05-14-09-56-44
Training model no. 22 of 23 for (22, 'Questioning_the_Reputation') persuasion technique...
{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}]}
{'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}]}
Best model updated: current epoch macro f1 = 0.017673048600883652
{'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}]}
Best model updated: current epoch macro f1 = 0.048
{'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}]}
Best model updated: current epoch macro f1 = 0.0646108663729809
{'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}, {'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}]}
Best model updated: current epoch macro f1 = 0.07810320781032079
{'micro_f1': 0.748501176292024, 'precision': 0.748501176292024, 'Questioning_the_Reputation_precision': 0.10869565217391304, 'Questioning_the_Reputation_recall': 0.07291666666666667, 'Questioning_the_Reputation_f1-score': 0.08728179551122195, 'Questioning_the_Reputation_support': 480.0, 'micro avg_precision': 0.10869565217391304, 'micro avg_recall': 0.07291666666666667, 'micro avg_f1-score': 0.08728179551122195, 'micro avg_support': 480.0, 'macro avg_precision': 0.10869565217391304, 'macro avg_recall': 0.07291666666666667, 'macro avg_f1-score': 0.08728179551122195, 'macro avg_support': 480.0, 'weighted avg_precision': 0.10869565217391304, 'weighted avg_recall': 0.07291666666666667, 'weighted avg_f1-score': 0.08728179551122195, 'weighted avg_support': 480.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 5}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}, {'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}, {'micro_f1': 0.748501176292024, 'precision': 0.748501176292024, 'Questioning_the_Reputation_precision': 0.10869565217391304, 'Questioning_the_Reputation_recall': 0.07291666666666667, 'Questioning_the_Reputation_f1-score': 0.08728179551122195, 'Questioning_the_Reputation_support': 480.0, 'micro avg_precision': 0.10869565217391304, 'micro avg_recall': 0.07291666666666667, 'micro avg_f1-score': 0.08728179551122195, 'micro avg_support': 480.0, 'macro avg_precision': 0.10869565217391304, 'macro avg_recall': 0.07291666666666667, 'macro avg_f1-score': 0.08728179551122195, 'macro avg_support': 480.0, 'weighted avg_precision': 0.10869565217391304, 'weighted avg_recall': 0.07291666666666667, 'weighted avg_f1-score': 0.08728179551122195, 'weighted avg_support': 480.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 5}]}
Best model updated: current epoch macro f1 = 0.08728179551122195
{'micro_f1': 0.7702815511876755, 'precision': 0.7702815511876755, 'Questioning_the_Reputation_precision': 0.18012422360248448, 'Questioning_the_Reputation_recall': 0.11068702290076336, 'Questioning_the_Reputation_f1-score': 0.13711583924349882, 'Questioning_the_Reputation_support': 524.0, 'micro avg_precision': 0.18012422360248448, 'micro avg_recall': 0.11068702290076336, 'micro avg_f1-score': 0.13711583924349882, 'micro avg_support': 524.0, 'macro avg_precision': 0.18012422360248448, 'macro avg_recall': 0.11068702290076336, 'macro avg_f1-score': 0.13711583924349882, 'macro avg_support': 524.0, 'weighted avg_precision': 0.18012422360248448, 'weighted avg_recall': 0.11068702290076336, 'weighted avg_f1-score': 0.13711583924349882, 'weighted avg_support': 524.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 6}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}, {'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}, {'micro_f1': 0.748501176292024, 'precision': 0.748501176292024, 'Questioning_the_Reputation_precision': 0.10869565217391304, 'Questioning_the_Reputation_recall': 0.07291666666666667, 'Questioning_the_Reputation_f1-score': 0.08728179551122195, 'Questioning_the_Reputation_support': 480.0, 'micro avg_precision': 0.10869565217391304, 'micro avg_recall': 0.07291666666666667, 'micro avg_f1-score': 0.08728179551122195, 'micro avg_support': 480.0, 'macro avg_precision': 0.10869565217391304, 'macro avg_recall': 0.07291666666666667, 'macro avg_f1-score': 0.08728179551122195, 'macro avg_support': 480.0, 'weighted avg_precision': 0.10869565217391304, 'weighted avg_recall': 0.07291666666666667, 'weighted avg_f1-score': 0.08728179551122195, 'weighted avg_support': 480.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 5}, {'micro_f1': 0.7702815511876755, 'precision': 0.7702815511876755, 'Questioning_the_Reputation_precision': 0.18012422360248448, 'Questioning_the_Reputation_recall': 0.11068702290076336, 'Questioning_the_Reputation_f1-score': 0.13711583924349882, 'Questioning_the_Reputation_support': 524.0, 'micro avg_precision': 0.18012422360248448, 'micro avg_recall': 0.11068702290076336, 'micro avg_f1-score': 0.13711583924349882, 'micro avg_support': 524.0, 'macro avg_precision': 0.18012422360248448, 'macro avg_recall': 0.11068702290076336, 'macro avg_f1-score': 0.13711583924349882, 'macro avg_support': 524.0, 'weighted avg_precision': 0.18012422360248448, 'weighted avg_recall': 0.11068702290076336, 'weighted avg_f1-score': 0.13711583924349882, 'weighted avg_support': 524.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 6}]}
Best model updated: current epoch macro f1 = 0.13711583924349882
{'micro_f1': 0.733626773924262, 'precision': 0.733626773924262, 'Questioning_the_Reputation_precision': 0.11801242236024845, 'Questioning_the_Reputation_recall': 0.10526315789473684, 'Questioning_the_Reputation_f1-score': 0.11127379209370424, 'Questioning_the_Reputation_support': 361.0, 'micro avg_precision': 0.11801242236024845, 'micro avg_recall': 0.10526315789473684, 'micro avg_f1-score': 0.11127379209370424, 'micro avg_support': 361.0, 'macro avg_precision': 0.11801242236024845, 'macro avg_recall': 0.10526315789473684, 'macro avg_f1-score': 0.11127379209370424, 'macro avg_support': 361.0, 'weighted avg_precision': 0.11801242236024845, 'weighted avg_recall': 0.10526315789473684, 'weighted avg_f1-score': 0.11127379209370424, 'weighted avg_support': 361.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 7}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}, {'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}, {'micro_f1': 0.748501176292024, 'precision': 0.748501176292024, 'Questioning_the_Reputation_precision': 0.10869565217391304, 'Questioning_the_Reputation_recall': 0.07291666666666667, 'Questioning_the_Reputation_f1-score': 0.08728179551122195, 'Questioning_the_Reputation_support': 480.0, 'micro avg_precision': 0.10869565217391304, 'micro avg_recall': 0.07291666666666667, 'micro avg_f1-score': 0.08728179551122195, 'micro avg_support': 480.0, 'macro avg_precision': 0.10869565217391304, 'macro avg_recall': 0.07291666666666667, 'macro avg_f1-score': 0.08728179551122195, 'macro avg_support': 480.0, 'weighted avg_precision': 0.10869565217391304, 'weighted avg_recall': 0.07291666666666667, 'weighted avg_f1-score': 0.08728179551122195, 'weighted avg_support': 480.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 5}, {'micro_f1': 0.7702815511876755, 'precision': 0.7702815511876755, 'Questioning_the_Reputation_precision': 0.18012422360248448, 'Questioning_the_Reputation_recall': 0.11068702290076336, 'Questioning_the_Reputation_f1-score': 0.13711583924349882, 'Questioning_the_Reputation_support': 524.0, 'micro avg_precision': 0.18012422360248448, 'micro avg_recall': 0.11068702290076336, 'micro avg_f1-score': 0.13711583924349882, 'micro avg_support': 524.0, 'macro avg_precision': 0.18012422360248448, 'macro avg_recall': 0.11068702290076336, 'macro avg_f1-score': 0.13711583924349882, 'macro avg_support': 524.0, 'weighted avg_precision': 0.18012422360248448, 'weighted avg_recall': 0.11068702290076336, 'weighted avg_f1-score': 0.13711583924349882, 'weighted avg_support': 524.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 6}, {'micro_f1': 0.733626773924262, 'precision': 0.733626773924262, 'Questioning_the_Reputation_precision': 0.11801242236024845, 'Questioning_the_Reputation_recall': 0.10526315789473684, 'Questioning_the_Reputation_f1-score': 0.11127379209370424, 'Questioning_the_Reputation_support': 361.0, 'micro avg_precision': 0.11801242236024845, 'micro avg_recall': 0.10526315789473684, 'micro avg_f1-score': 0.11127379209370424, 'micro avg_support': 361.0, 'macro avg_precision': 0.11801242236024845, 'macro avg_recall': 0.10526315789473684, 'macro avg_f1-score': 0.11127379209370424, 'macro avg_support': 361.0, 'weighted avg_precision': 0.11801242236024845, 'weighted avg_recall': 0.10526315789473684, 'weighted avg_f1-score': 0.11127379209370424, 'weighted avg_support': 361.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 7}]}
{'micro_f1': 0.7637550276997799, 'precision': 0.7637550276997799, 'Questioning_the_Reputation_precision': 0.1956521739130435, 'Questioning_the_Reputation_recall': 0.14416475972540047, 'Questioning_the_Reputation_f1-score': 0.16600790513833993, 'Questioning_the_Reputation_support': 437.0, 'micro avg_precision': 0.1956521739130435, 'micro avg_recall': 0.14416475972540047, 'micro avg_f1-score': 0.16600790513833993, 'micro avg_support': 437.0, 'macro avg_precision': 0.1956521739130435, 'macro avg_recall': 0.14416475972540047, 'macro avg_f1-score': 0.16600790513833993, 'macro avg_support': 437.0, 'weighted avg_precision': 0.1956521739130435, 'weighted avg_recall': 0.14416475972540047, 'weighted avg_f1-score': 0.16600790513833993, 'weighted avg_support': 437.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 8}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}, {'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}, {'micro_f1': 0.748501176292024, 'precision': 0.748501176292024, 'Questioning_the_Reputation_precision': 0.10869565217391304, 'Questioning_the_Reputation_recall': 0.07291666666666667, 'Questioning_the_Reputation_f1-score': 0.08728179551122195, 'Questioning_the_Reputation_support': 480.0, 'micro avg_precision': 0.10869565217391304, 'micro avg_recall': 0.07291666666666667, 'micro avg_f1-score': 0.08728179551122195, 'micro avg_support': 480.0, 'macro avg_precision': 0.10869565217391304, 'macro avg_recall': 0.07291666666666667, 'macro avg_f1-score': 0.08728179551122195, 'macro avg_support': 480.0, 'weighted avg_precision': 0.10869565217391304, 'weighted avg_recall': 0.07291666666666667, 'weighted avg_f1-score': 0.08728179551122195, 'weighted avg_support': 480.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 5}, {'micro_f1': 0.7702815511876755, 'precision': 0.7702815511876755, 'Questioning_the_Reputation_precision': 0.18012422360248448, 'Questioning_the_Reputation_recall': 0.11068702290076336, 'Questioning_the_Reputation_f1-score': 0.13711583924349882, 'Questioning_the_Reputation_support': 524.0, 'micro avg_precision': 0.18012422360248448, 'micro avg_recall': 0.11068702290076336, 'micro avg_f1-score': 0.13711583924349882, 'micro avg_support': 524.0, 'macro avg_precision': 0.18012422360248448, 'macro avg_recall': 0.11068702290076336, 'macro avg_f1-score': 0.13711583924349882, 'macro avg_support': 524.0, 'weighted avg_precision': 0.18012422360248448, 'weighted avg_recall': 0.11068702290076336, 'weighted avg_f1-score': 0.13711583924349882, 'weighted avg_support': 524.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 6}, {'micro_f1': 0.733626773924262, 'precision': 0.733626773924262, 'Questioning_the_Reputation_precision': 0.11801242236024845, 'Questioning_the_Reputation_recall': 0.10526315789473684, 'Questioning_the_Reputation_f1-score': 0.11127379209370424, 'Questioning_the_Reputation_support': 361.0, 'micro avg_precision': 0.11801242236024845, 'micro avg_recall': 0.10526315789473684, 'micro avg_f1-score': 0.11127379209370424, 'micro avg_support': 361.0, 'macro avg_precision': 0.11801242236024845, 'macro avg_recall': 0.10526315789473684, 'macro avg_f1-score': 0.11127379209370424, 'macro avg_support': 361.0, 'weighted avg_precision': 0.11801242236024845, 'weighted avg_recall': 0.10526315789473684, 'weighted avg_f1-score': 0.11127379209370424, 'weighted avg_support': 361.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 7}, {'micro_f1': 0.7637550276997799, 'precision': 0.7637550276997799, 'Questioning_the_Reputation_precision': 0.1956521739130435, 'Questioning_the_Reputation_recall': 0.14416475972540047, 'Questioning_the_Reputation_f1-score': 0.16600790513833993, 'Questioning_the_Reputation_support': 437.0, 'micro avg_precision': 0.1956521739130435, 'micro avg_recall': 0.14416475972540047, 'micro avg_f1-score': 0.16600790513833993, 'micro avg_support': 437.0, 'macro avg_precision': 0.1956521739130435, 'macro avg_recall': 0.14416475972540047, 'macro avg_f1-score': 0.16600790513833993, 'macro avg_support': 437.0, 'weighted avg_precision': 0.1956521739130435, 'weighted avg_recall': 0.14416475972540047, 'weighted avg_f1-score': 0.16600790513833993, 'weighted avg_support': 437.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 8}]}
Best model updated: current epoch macro f1 = 0.16600790513833993
{'micro_f1': 0.7136677544205813, 'precision': 0.7136677544205813, 'Questioning_the_Reputation_precision': 0.12732919254658384, 'Questioning_the_Reputation_recall': 0.12693498452012383, 'Questioning_the_Reputation_f1-score': 0.12713178294573643, 'Questioning_the_Reputation_support': 323.0, 'micro avg_precision': 0.12732919254658384, 'micro avg_recall': 0.12693498452012383, 'micro avg_f1-score': 0.12713178294573643, 'micro avg_support': 323.0, 'macro avg_precision': 0.12732919254658384, 'macro avg_recall': 0.12693498452012383, 'macro avg_f1-score': 0.12713178294573643, 'macro avg_support': 323.0, 'weighted avg_precision': 0.12732919254658384, 'weighted avg_recall': 0.12693498452012383, 'weighted avg_f1-score': 0.12713178294573643, 'weighted avg_support': 323.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 9}
{'results': [{'micro_f1': 0.5793427942627305, 'precision': 0.5793427942627305, 'Questioning_the_Reputation_precision': 0.0, 'Questioning_the_Reputation_recall': 0.0, 'Questioning_the_Reputation_f1-score': 0.0, 'Questioning_the_Reputation_support': 0.0, 'micro avg_precision': 0.0, 'micro avg_recall': 0.0, 'micro avg_f1-score': 0.0, 'micro avg_support': 0.0, 'macro avg_precision': 0.0, 'macro avg_recall': 0.0, 'macro avg_f1-score': 0.0, 'macro avg_support': 0.0, 'weighted avg_precision': 0.0, 'weighted avg_recall': 0.0, 'weighted avg_f1-score': 0.0, 'weighted avg_support': 0.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 0}, {'micro_f1': 0.664187599605373, 'precision': 0.664187599605373, 'Questioning_the_Reputation_precision': 0.018633540372670808, 'Questioning_the_Reputation_recall': 0.01680672268907563, 'Questioning_the_Reputation_f1-score': 0.017673048600883652, 'Questioning_the_Reputation_support': 357.0, 'micro avg_precision': 0.018633540372670808, 'micro avg_recall': 0.01680672268907563, 'micro avg_f1-score': 0.017673048600883652, 'micro avg_support': 357.0, 'macro avg_precision': 0.018633540372670808, 'macro avg_recall': 0.01680672268907563, 'macro avg_f1-score': 0.017673048600883652, 'macro avg_support': 357.0, 'weighted avg_precision': 0.018633540372670808, 'weighted avg_recall': 0.01680672268907563, 'weighted avg_f1-score': 0.017673048600883652, 'weighted avg_support': 357.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 1}, {'micro_f1': 0.6759505198451848, 'precision': 0.6759505198451848, 'Questioning_the_Reputation_precision': 0.046583850931677016, 'Questioning_the_Reputation_recall': 0.04950495049504951, 'Questioning_the_Reputation_f1-score': 0.048, 'Questioning_the_Reputation_support': 303.0, 'micro avg_precision': 0.046583850931677016, 'micro avg_recall': 0.04950495049504951, 'micro avg_f1-score': 0.048, 'micro avg_support': 303.0, 'macro avg_precision': 0.046583850931677016, 'macro avg_recall': 0.04950495049504951, 'macro avg_f1-score': 0.048, 'macro avg_support': 303.0, 'weighted avg_precision': 0.046583850931677016, 'weighted avg_recall': 0.04950495049504951, 'weighted avg_f1-score': 0.048, 'weighted avg_support': 303.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 2}, {'micro_f1': 0.7107080519086285, 'precision': 0.7107080519086286, 'Questioning_the_Reputation_precision': 0.06832298136645963, 'Questioning_the_Reputation_recall': 0.06128133704735376, 'Questioning_the_Reputation_f1-score': 0.0646108663729809, 'Questioning_the_Reputation_support': 359.0, 'micro avg_precision': 0.06832298136645963, 'micro avg_recall': 0.06128133704735376, 'micro avg_f1-score': 0.0646108663729809, 'micro avg_support': 359.0, 'macro avg_precision': 0.06832298136645963, 'macro avg_recall': 0.06128133704735376, 'macro avg_f1-score': 0.0646108663729809, 'macro avg_support': 359.0, 'weighted avg_precision': 0.06832298136645963, 'weighted avg_recall': 0.06128133704735376, 'weighted avg_f1-score': 0.0646108663729809, 'weighted avg_support': 359.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 3}, {'micro_f1': 0.7283144873643471, 'precision': 0.728314487364347, 'Questioning_the_Reputation_precision': 0.08695652173913043, 'Questioning_the_Reputation_recall': 0.07088607594936709, 'Questioning_the_Reputation_f1-score': 0.07810320781032079, 'Questioning_the_Reputation_support': 395.0, 'micro avg_precision': 0.08695652173913043, 'micro avg_recall': 0.07088607594936709, 'micro avg_f1-score': 0.07810320781032079, 'micro avg_support': 395.0, 'macro avg_precision': 0.08695652173913043, 'macro avg_recall': 0.07088607594936709, 'macro avg_f1-score': 0.07810320781032079, 'macro avg_support': 395.0, 'weighted avg_precision': 0.08695652173913043, 'weighted avg_recall': 0.07088607594936709, 'weighted avg_f1-score': 0.07810320781032079, 'weighted avg_support': 395.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 4}, {'micro_f1': 0.748501176292024, 'precision': 0.748501176292024, 'Questioning_the_Reputation_precision': 0.10869565217391304, 'Questioning_the_Reputation_recall': 0.07291666666666667, 'Questioning_the_Reputation_f1-score': 0.08728179551122195, 'Questioning_the_Reputation_support': 480.0, 'micro avg_precision': 0.10869565217391304, 'micro avg_recall': 0.07291666666666667, 'micro avg_f1-score': 0.08728179551122195, 'micro avg_support': 480.0, 'macro avg_precision': 0.10869565217391304, 'macro avg_recall': 0.07291666666666667, 'macro avg_f1-score': 0.08728179551122195, 'macro avg_support': 480.0, 'weighted avg_precision': 0.10869565217391304, 'weighted avg_recall': 0.07291666666666667, 'weighted avg_f1-score': 0.08728179551122195, 'weighted avg_support': 480.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 5}, {'micro_f1': 0.7702815511876755, 'precision': 0.7702815511876755, 'Questioning_the_Reputation_precision': 0.18012422360248448, 'Questioning_the_Reputation_recall': 0.11068702290076336, 'Questioning_the_Reputation_f1-score': 0.13711583924349882, 'Questioning_the_Reputation_support': 524.0, 'micro avg_precision': 0.18012422360248448, 'micro avg_recall': 0.11068702290076336, 'micro avg_f1-score': 0.13711583924349882, 'micro avg_support': 524.0, 'macro avg_precision': 0.18012422360248448, 'macro avg_recall': 0.11068702290076336, 'macro avg_f1-score': 0.13711583924349882, 'macro avg_support': 524.0, 'weighted avg_precision': 0.18012422360248448, 'weighted avg_recall': 0.11068702290076336, 'weighted avg_f1-score': 0.13711583924349882, 'weighted avg_support': 524.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 6}, {'micro_f1': 0.733626773924262, 'precision': 0.733626773924262, 'Questioning_the_Reputation_precision': 0.11801242236024845, 'Questioning_the_Reputation_recall': 0.10526315789473684, 'Questioning_the_Reputation_f1-score': 0.11127379209370424, 'Questioning_the_Reputation_support': 361.0, 'micro avg_precision': 0.11801242236024845, 'micro avg_recall': 0.10526315789473684, 'micro avg_f1-score': 0.11127379209370424, 'micro avg_support': 361.0, 'macro avg_precision': 0.11801242236024845, 'macro avg_recall': 0.10526315789473684, 'macro avg_f1-score': 0.11127379209370424, 'macro avg_support': 361.0, 'weighted avg_precision': 0.11801242236024845, 'weighted avg_recall': 0.10526315789473684, 'weighted avg_f1-score': 0.11127379209370424, 'weighted avg_support': 361.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 7}, {'micro_f1': 0.7637550276997799, 'precision': 0.7637550276997799, 'Questioning_the_Reputation_precision': 0.1956521739130435, 'Questioning_the_Reputation_recall': 0.14416475972540047, 'Questioning_the_Reputation_f1-score': 0.16600790513833993, 'Questioning_the_Reputation_support': 437.0, 'micro avg_precision': 0.1956521739130435, 'micro avg_recall': 0.14416475972540047, 'micro avg_f1-score': 0.16600790513833993, 'micro avg_support': 437.0, 'macro avg_precision': 0.1956521739130435, 'macro avg_recall': 0.14416475972540047, 'macro avg_f1-score': 0.16600790513833993, 'macro avg_support': 437.0, 'weighted avg_precision': 0.1956521739130435, 'weighted avg_recall': 0.14416475972540047, 'weighted avg_f1-score': 0.16600790513833993, 'weighted avg_support': 437.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 8}, {'micro_f1': 0.7136677544205813, 'precision': 0.7136677544205813, 'Questioning_the_Reputation_precision': 0.12732919254658384, 'Questioning_the_Reputation_recall': 0.12693498452012383, 'Questioning_the_Reputation_f1-score': 0.12713178294573643, 'Questioning_the_Reputation_support': 323.0, 'micro avg_precision': 0.12732919254658384, 'micro avg_recall': 0.12693498452012383, 'micro avg_f1-score': 0.12713178294573643, 'micro avg_support': 323.0, 'macro avg_precision': 0.12732919254658384, 'macro avg_recall': 0.12693498452012383, 'macro avg_f1-score': 0.12713178294573643, 'macro avg_support': 323.0, 'weighted avg_precision': 0.12732919254658384, 'weighted avg_recall': 0.12693498452012383, 'weighted avg_f1-score': 0.12713178294573643, 'weighted avg_support': 323.0, 'O_support': 7634, 'B-Questioning_the_Reputation_support': 322, 'I-Questioning_the_Reputation_support': 5221, 'epoch': 9}]}
Saving model to directory: ./models/M2/2024-05-14-09-56-44_aug_ts0.9/mdeberta-v3-base_22_ME10_target=Questioning_the_Reputation_SUBSAMPLED_2024-05-14-09-56-44
