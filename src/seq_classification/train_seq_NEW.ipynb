{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from train_seq import tokenize_sequence_classification\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/home/lgiordano/LUCA/checkthat_GITHUB/data/formatted/train_sentences.json'\n",
    "with open(train_data_path, 'r', encoding='utf8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code balances positive and negative samples for each language by down-sampling the larger group ###\n",
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "langs = set(sample['data']['lang'] for sample in data)\n",
    "\n",
    "sampled_dfs = []\n",
    "for lang in langs:\n",
    "    df_lang = df[df['data'].apply(lambda x: x['lang'] == lang)]\n",
    "    df_pos_lang = df_lang[df_lang['data'].apply(lambda x: x['label'] == 1)]\n",
    "    df_neg_lang = df_lang[df_lang['data'].apply(lambda x: x['label'] == 0)]\n",
    "    if len(df_neg_lang) > len(df_pos_lang):\n",
    "        df_neg_lang = df_neg_lang.sample(len(df_pos_lang))\n",
    "    df_lang_sampled = pd.concat([df_pos_lang, df_neg_lang])\n",
    "    sampled_dfs.append(df_lang_sampled)\n",
    "df_sampled = pd.concat(sampled_dfs, ignore_index=True)\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/33345 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 33345/33345 [00:11<00:00, 2784.50 examples/s]\n",
      "Map: 100%|██████████| 8337/8337 [00:03<00:00, 2772.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### This code splits the balanced dataset in train/test splits and tokenizes both with dynamic padding\n",
    "\n",
    "#model_name = 'bert-base-multilingual-cased'\n",
    "#model_name = 'xlm-roberta-base'\n",
    "model_name = 'microsoft/mdeberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "split_ratio = 0.2\n",
    "split_seed = 42\n",
    "batch_size = 16\n",
    "\n",
    "datadict = dataset.train_test_split(split_ratio, seed=split_seed)\n",
    "datadict = datadict.map(lambda x: tokenize_sequence_classification(x, tokenizer),\n",
    "                            batch_size=batch_size,\n",
    "                            batched=True\n",
    "                            )\n",
    "\n",
    "columns = [\n",
    "            'input_ids',\n",
    "            'token_type_ids', #non per xlm-roberta\n",
    "            'attention_mask',\n",
    "            'labels'\n",
    "            ]\n",
    "datadict.set_format('torch', columns = columns)\n",
    "\n",
    "train_data = datadict['train']\n",
    "val_data = datadict['test']\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(output_dir='/home/lgiordano/LUCA/checkthat_GITHUB/models/M1/mdeberta-v3-base-NEW_2nd',\n",
    "                                  save_total_limit=2,\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_only_model=True,\n",
    "                                  metric_for_best_model='eval_macro-f1',\n",
    "                                  logging_strategy='epoch',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=5e-5,\n",
    "                                  optim='adamw_torch',\n",
    "                                  num_train_epochs=10)\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    results = classification_report(labels, preds, output_dict=True)\n",
    "    results['macro-f1'] = results['macro avg']['f1-score']\n",
    "\n",
    "    models_dir = '/home/lgiordano/LUCA/checkthat_GITHUB/models/M1'\n",
    "    model_name_simple = model_name.split('/')[-1]\n",
    "    model_save_name = f'{model_name_simple}_{date_time}'\n",
    "    model_save_dir = os.path.join(models_dir, model_save_name)\n",
    "    if not os.path.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir)\n",
    "    with open(os.path.join(model_save_dir, 'results.json'), 'w', encoding='utf8') as f:\n",
    "        json.dump(results, f, ensure_ascii = False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16676' max='41690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16676/41690 38:15 < 57:24, 7.26 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg</th>\n",
       "      <th>Weighted avg</th>\n",
       "      <th>Macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>0.586325</td>\n",
       "      <td>{'precision': 0.6880593756821655, 'recall': 0.7840796019900498, 'f1-score': 0.7329380304615741, 'support': 4020.0}</td>\n",
       "      <td>{'precision': 0.7689030883919062, 'recall': 0.6689830901088719, 'f1-score': 0.7154713241669763, 'support': 4317.0}</td>\n",
       "      <td>0.724481</td>\n",
       "      <td>{'precision': 0.7284812320370359, 'recall': 0.7265313460494609, 'f1-score': 0.7242046773142752, 'support': 8337.0}</td>\n",
       "      <td>{'precision': 0.729921233396925, 'recall': 0.7244812282595658, 'f1-score': 0.7238935575008234, 'support': 8337.0}</td>\n",
       "      <td>0.724205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.562402</td>\n",
       "      <td>{'precision': 0.7642919964819701, 'recall': 0.6485074626865671, 'f1-score': 0.7016552280985063, 'support': 4020.0}</td>\n",
       "      <td>{'precision': 0.7131546894031668, 'recall': 0.8137595552466991, 'f1-score': 0.7601428107757222, 'support': 4317.0}</td>\n",
       "      <td>0.734077</td>\n",
       "      <td>{'precision': 0.7387233429425685, 'recall': 0.7311335089666331, 'f1-score': 0.7308990194371143, 'support': 8337.0}</td>\n",
       "      <td>{'precision': 0.7378124769114779, 'recall': 0.7340770061173084, 'f1-score': 0.7319408097726746, 'support': 8337.0}</td>\n",
       "      <td>0.730899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.600969</td>\n",
       "      <td>{'precision': 0.7872619829284307, 'recall': 0.5965174129353233, 'f1-score': 0.6787432776677045, 'support': 4020.0}</td>\n",
       "      <td>{'precision': 0.6934416934416935, 'recall': 0.8498957609451008, 'f1-score': 0.7637385512073274, 'support': 4317.0}</td>\n",
       "      <td>0.727720</td>\n",
       "      <td>{'precision': 0.7403518381850621, 'recall': 0.723206586940212, 'f1-score': 0.7212409144375159, 'support': 8337.0}</td>\n",
       "      <td>{'precision': 0.738680695929001, 'recall': 0.727719803286554, 'f1-score': 0.7227548640741519, 'support': 8337.0}</td>\n",
       "      <td>0.721241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.640860</td>\n",
       "      <td>{'precision': 0.6888504753673294, 'recall': 0.7930348258706468, 'f1-score': 0.7372802960222017, 'support': 4020.0}</td>\n",
       "      <td>{'precision': 0.7756807764896199, 'recall': 0.6664350243224462, 'f1-score': 0.7169200099676054, 'support': 4317.0}</td>\n",
       "      <td>0.727480</td>\n",
       "      <td>{'precision': 0.7322656259284746, 'recall': 0.7297349250965465, 'f1-score': 0.7271001529949035, 'support': 8337.0}</td>\n",
       "      <td>{'precision': 0.7338122613748774, 'recall': 0.7274799088401104, 'f1-score': 0.7267374922681304, 'support': 8337.0}</td>\n",
       "      <td>0.727100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16676, training_loss=0.5353185077684621, metrics={'train_runtime': 2296.5062, 'train_samples_per_second': 145.199, 'train_steps_per_second': 18.154, 'total_flos': 1.0078943670885072e+16, 'train_loss': 0.5353185077684621, 'epoch': 4.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1043' max='1043' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1043/1043 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5624021887779236,\n",
       " 'eval_0': {'precision': 0.7642919964819701,\n",
       "  'recall': 0.6485074626865671,\n",
       "  'f1-score': 0.7016552280985063,\n",
       "  'support': 4020.0},\n",
       " 'eval_1': {'precision': 0.7131546894031668,\n",
       "  'recall': 0.8137595552466991,\n",
       "  'f1-score': 0.7601428107757222,\n",
       "  'support': 4317.0},\n",
       " 'eval_accuracy': 0.7340770061173084,\n",
       " 'eval_macro avg': {'precision': 0.7387233429425685,\n",
       "  'recall': 0.7311335089666331,\n",
       "  'f1-score': 0.7308990194371143,\n",
       "  'support': 8337.0},\n",
       " 'eval_weighted avg': {'precision': 0.7378124769114779,\n",
       "  'recall': 0.7340770061173084,\n",
       "  'f1-score': 0.7319408097726746,\n",
       "  'support': 8337.0},\n",
       " 'eval_macro-f1': 0.7308990194371143,\n",
       " 'eval_runtime': 20.7636,\n",
       " 'eval_samples_per_second': 401.52,\n",
       " 'eval_steps_per_second': 50.232,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
