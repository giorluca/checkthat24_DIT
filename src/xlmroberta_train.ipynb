{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence_aligner.labelset import LabelSet\n",
    "from sequence_aligner.dataset import TrainingDataset\n",
    "from sequence_aligner.containers import TrainingBatch\n",
    "import json\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('/home/luca/Scrivania/CheckThat/annotated_train.json'))\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "label_set = LabelSet(labels=[\"Appeal_to_Authority\", \"Appeal_to_Popularity\", \"Appeal_to_Values\", \"Appeal_to_Fear-Prejudice\", \"Flag_Waving\",\n",
    "                             \"Causal_Oversimplification\", \"False_Dilemma-No_Choice\", \"Consequential_Oversimplification\", \"Straw_Man\",\n",
    "                             \"Red_Herring\", \"Whataboutism\", \"Slogans\", \"Appeal_to_Time\", \"Conversation_Killer\", \"Loaded_Language\",\n",
    "                             \"Repetition\", \"Exaggeration-Minimisation\", \"Obfuscation-Vagueness-Confusion\", \"Name_Calling-Labeling\", \"Doubt\",\n",
    "                             \"Guilt_by_Association\", \"Appeal_to_Hypocrisy\", \"Questioning_the_Reputation\"])\n",
    "\n",
    "train_dataset = TrainingDataset(data=train_data,tokenizer=tokenizer,label_set=label_set)\n",
    "val_dataset = TrainingDataset(data=val_data,tokenizer=tokenizer,label_set=label_set)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, collate_fn=TrainingBatch, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, collate_fn=TrainingBatch, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=len(train_dataset.label_set.ids_to_label.values()))\n",
    "lr = 5e-6\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 5\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    tot_loss_train = 0\n",
    "    tot_acc_train = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_masks'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs['logits'].transpose(1, 2), labels)\n",
    "        tot_loss_train += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs['logits'], 2)\n",
    "        correct_predictions = torch.sum(preds == labels)\n",
    "        tot_acc_train += correct_predictions.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    tot_loss_val = 0\n",
    "    tot_acc_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_masks'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            loss = criterion(outputs['logits'].transpose(1, 2), labels)\n",
    "            tot_loss_val += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs['logits'], 2)\n",
    "            correct_predictions = torch.sum(preds == labels)\n",
    "            tot_acc_val += correct_predictions.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch: {epoch + 1}/{epochs}')\n",
    "    print(f'Train Loss: {tot_loss_train / len(train_loader.dataset)}')\n",
    "    print(f'Train Accuracy: {tot_acc_train / len(train_loader.dataset) / 100}')\n",
    "    print(f'Validation Loss: {tot_loss_val / len(val_loader.dataset)}')\n",
    "    print(f'Validation Accuracy: {tot_acc_val / len(val_loader.dataset) / 100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
