{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    "    PreTrainedTokenizerBase\n",
    ")\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import EvalPrediction, AutoConfig\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score #confusion_matrix\n",
    "import json\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForDynamicTokenClassification:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        # Get sequence lengths and find max length in batch\n",
    "        batch_size = len(features)\n",
    "        if self.pad_to_multiple_of is not None:\n",
    "            max_length = max(len(x[\"input_ids\"]) for x in features)\n",
    "            if max_length % self.pad_to_multiple_of != 0:\n",
    "                max_length = ((max_length // self.pad_to_multiple_of) + 1) * self.pad_to_multiple_of\n",
    "        else:\n",
    "            max_length = max(len(x[\"input_ids\"]) for x in features)\n",
    "\n",
    "        # Initialize padded tensors\n",
    "        input_ids = torch.full((batch_size, max_length), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((batch_size, max_length), dtype=torch.long)\n",
    "        labels = torch.full((batch_size, max_length), self.label_pad_token_id, dtype=torch.long)\n",
    "\n",
    "        # Fill tensors with actual values\n",
    "        for i, feature in enumerate(features):\n",
    "            seq_length = len(feature[\"input_ids\"])\n",
    "            input_ids[i, :seq_length] = feature[\"input_ids\"]\n",
    "            attention_mask[i, :seq_length] = feature[\"attention_mask\"]\n",
    "            labels[i, :seq_length] = feature[\"labels\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs: Dict[str, List], labels: List[List[int]], pad_token_id: int):\n",
    "        self.encodings = tokenized_inputs\n",
    "        self.labels = labels\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Remove padding from the encodings and labels\n",
    "        item = {}\n",
    "        for key, val in self.encodings.items():\n",
    "            if key == \"input_ids\":\n",
    "                # Find the last non-padding token\n",
    "                last_token = len(val[idx]) - 1\n",
    "                while last_token > 0 and val[idx][last_token] == self.pad_token_id:\n",
    "                    last_token -= 1\n",
    "                item[key] = torch.tensor(val[idx][:last_token + 1])\n",
    "            elif key == \"attention_mask\":\n",
    "                item[key] = torch.tensor(val[idx][:len(item[\"input_ids\"])])\n",
    "        \n",
    "        # Trim labels to match input length\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx][:len(item[\"input_ids\"])])\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "class WeightedTokenClassification(AutoModelForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.focal_loss_gamma = 4.0  # Increased focal loss parameter\n",
    "        self.class_weights = None\n",
    "        self.threshold = 0.4  # Threshold for O class prediction\n",
    "        \n",
    "    def compute_class_weights(self, labels):\n",
    "        # Remove padding tokens\n",
    "        valid_labels = labels[labels != -100]\n",
    "        # Count class frequencies\n",
    "        unique, counts = np.unique(valid_labels, return_counts=True)\n",
    "        total = np.sum(counts)\n",
    "        \n",
    "        # Extremely aggressive weighting scheme\n",
    "        weights = np.zeros(len(self.config.id2label))\n",
    "        weights[unique] = total / (len(unique) * counts)\n",
    "        \n",
    "        # Nearly eliminate 'O' class weight and heavily boost propaganda classes\n",
    "        weights[0] = weights[0] * 0.00001  # Extremely aggressive downweighting of 'O' class\n",
    "        weights[1:] = weights[1:] * 100.0   # Much stronger boost for propaganda classes\n",
    "        \n",
    "        # Apply cubic root to maintain some differences while reducing extreme values\n",
    "        weights = np.cbrt(weights)\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = weights / np.mean(weights[weights != 0])\n",
    "        \n",
    "        return torch.FloatTensor(weights).to(labels.device)\n",
    "\n",
    "    def focal_loss(self, logits, labels, class_weights):\n",
    "        \"\"\"\n",
    "        Compute focal loss with higher gamma for more focus on hard examples\n",
    "        \"\"\"\n",
    "        ce_loss = torch.nn.functional.cross_entropy(\n",
    "            logits, \n",
    "            labels, \n",
    "            weight=class_weights,\n",
    "            ignore_index=-100,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.focal_loss_gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        **kwargs\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        if labels is not None:\n",
    "            class_weights = self.compute_class_weights(labels.cpu().numpy())\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            if self.training:\n",
    "                # Custom logits adjustment to discourage 'O' class predictions\n",
    "                logits[:, :, 0] = logits[:, :, 0] - 2.0  # Penalize 'O' class logits\n",
    "                \n",
    "                # Create smoothed labels with propaganda class boosting\n",
    "                smoothed_labels = torch.zeros_like(logits)\n",
    "                valid_mask = labels != -100\n",
    "                \n",
    "                # One-hot encoding\n",
    "                smoothed_labels[valid_mask] = torch.nn.functional.one_hot(\n",
    "                    labels[valid_mask], \n",
    "                    num_classes=self.config.num_labels\n",
    "                ).float()\n",
    "                \n",
    "                # Stronger label smoothing for propaganda classes\n",
    "                propaganda_mask = (labels != -100) & (labels != 0)\n",
    "                smoothed_labels[propaganda_mask] = (\n",
    "                    smoothed_labels[propaganda_mask] * 0.95 +  # Higher confidence in main class\n",
    "                    0.05 / (self.config.num_labels - 1)       # Less distribution to other classes\n",
    "                )\n",
    "                \n",
    "                # Add auxiliary loss to encourage propaganda prediction\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                o_class_penalty = torch.mean(probs[:, :, 0] * (labels != -100).float())\n",
    "                \n",
    "                # Compute primary loss with smoothed labels\n",
    "                main_loss = -torch.sum(\n",
    "                    smoothed_labels * torch.log_softmax(logits, dim=-1),\n",
    "                    dim=-1\n",
    "                )\n",
    "                main_loss = (main_loss * (labels != -100).float()).mean()\n",
    "                \n",
    "                # Combine losses\n",
    "                loss = main_loss + 0.5 * o_class_penalty\n",
    "                \n",
    "            else:\n",
    "                # Use focal loss for evaluation\n",
    "                loss = self.focal_loss(\n",
    "                    logits.view(-1, self.config.num_labels),\n",
    "                    labels.view(-1),\n",
    "                    class_weights\n",
    "                )\n",
    "            \n",
    "            outputs.loss = loss\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# Modified training setup\n",
    "def setup_model(model_name: str, num_labels: int):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_labels,\n",
    "        hidden_dropout_prob=0.3,    # Increased dropout\n",
    "        attention_probs_dropout_prob=0.3\n",
    "    )\n",
    "    model = WeightedTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback to save detailed metrics after each evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: str, id_to_label: Dict[int, str]):\n",
    "        self.output_dir = output_dir\n",
    "        self.id_to_label = id_to_label\n",
    "        self.best_metrics = None\n",
    "        self.best_step = None\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        \"\"\"Called after each evaluation.\"\"\"\n",
    "        # Create epoch-specific directory\n",
    "        epoch_dir = os.path.join(self.output_dir, f\"epoch_{state.epoch}\")\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        \n",
    "        # Save detailed metrics for this epoch\n",
    "        detailed_metrics = {\n",
    "            \"epoch\": state.epoch,\n",
    "            \"step\": state.global_step,\n",
    "            \"overall_propaganda_metrics\": {\n",
    "                \"precision\": metrics.get(\"eval_propaganda_precision\", 0),\n",
    "                \"recall\": metrics.get(\"eval_propaganda_recall\", 0),\n",
    "                \"f1\": metrics.get(\"eval_propaganda_f1\", 0)\n",
    "            },\n",
    "            \"per_class_metrics\": {}\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        for i in range(len(self.id_to_label)):\n",
    "            class_name = self.id_to_label[i]\n",
    "            detailed_metrics[\"per_class_metrics\"][class_name] = {\n",
    "                \"precision\": metrics[\"eval_per_class_precision\"][i],\n",
    "                \"recall\": metrics[\"eval_per_class_recall\"][i],\n",
    "                \"f1\": metrics[\"eval_per_class_f1\"][i],\n",
    "                \"support\": metrics[\"eval_support\"][i]\n",
    "            }\n",
    "        \n",
    "        # Track best metrics\n",
    "        current_f1 = metrics.get(\"eval_propaganda_f1\", 0)\n",
    "        if self.best_metrics is None or current_f1 > self.best_metrics[\"overall_propaganda_metrics\"][\"f1\"]:\n",
    "            self.best_metrics = detailed_metrics\n",
    "            self.best_step = state.global_step\n",
    "            \n",
    "            # Save best metrics separately\n",
    "            best_metrics_path = os.path.join(self.output_dir, \"best_metrics.json\")\n",
    "            with open(best_metrics_path, 'w', encoding='utf8') as f:\n",
    "                json.dump({\n",
    "                    \"best_step\": self.best_step,\n",
    "                    \"metrics\": self.best_metrics\n",
    "                }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Save epoch metrics\n",
    "        metrics_path = os.path.join(epoch_dir, 'detailed_metrics.json')\n",
    "        with open(metrics_path, 'w', encoding='utf8') as f:\n",
    "            json.dump(detailed_metrics, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        '''        # Save confusion matrix if available\n",
    "        if \"eval_confusion_matrix\" in metrics:\n",
    "            confusion_path = os.path.join(epoch_dir, 'confusion_matrix.npy')\n",
    "            np.save(confusion_path, np.array(metrics[\"eval_confusion_matrix\"]))'''\n",
    "            \n",
    "        # Print summary of propaganda metrics\n",
    "        print(\"\\nPropaganda Detection Metrics:\")\n",
    "        print(f\"Precision: {metrics.get('eval_propaganda_precision', 0):.4f}\")\n",
    "        print(f\"Recall: {metrics.get('eval_propaganda_recall', 0):.4f}\")\n",
    "        print(f\"F1: {metrics.get('eval_propaganda_f1', 0):.4f}\")\n",
    "        \n",
    "        # Print per-class F1 scores for non-O classes\n",
    "        print(\"\\nPer-class F1 scores (excluding 'O'):\")\n",
    "        for i in range(1, len(self.id_to_label)):  # Skip 'O' class\n",
    "            class_name = self.id_to_label[i]\n",
    "            f1_score = metrics[\"eval_per_class_f1\"][i]\n",
    "            support = metrics[\"eval_support\"][i]\n",
    "            if support > 0:  # Only show classes that appear in the evaluation set\n",
    "                print(f\"{class_name}: {f1_score:.4f} (support: {support})\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Called at the end of training - print best results.\"\"\"\n",
    "        if self.best_metrics is not None:\n",
    "            print(\"\\nBest Model Performance:\")\n",
    "            print(f\"Step: {self.best_step}\")\n",
    "            best_f1 = self.best_metrics[\"overall_propaganda_metrics\"][\"f1\"]\n",
    "            print(f\"Best Propaganda F1: {best_f1:.4f}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenClassificationConfig:\n",
    "    model_name: str = 'microsoft/mdeberta-v3-base'\n",
    "    max_length: int = 512\n",
    "    stride: int = 256\n",
    "    num_labels: int = 24\n",
    "    output_dir: str = '/home/lgiordano/LUCA/checkthat_GITHUB/models/sliding_window'\n",
    "    \n",
    "    # Add training specific parameters\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 16\n",
    "    num_train_epochs: int = 20\n",
    "    learning_rate: float = 5e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    logging_steps: int = 50\n",
    "    eval_steps: int = 100\n",
    "    save_total_limit: int = 2\n",
    "    label_smoothing_factor: float = 0.1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        self.full_output_dir = os.path.join(self.output_dir, self.date_time)\n",
    "        # Add id_to_label mapping\n",
    "        self.id_to_label = {\n",
    "            0: \"O\", \n",
    "            1: \"Appeal_to_Authority\",\n",
    "            2: \"Appeal_to_Popularity\",\n",
    "            3: \"Appeal_to_Values\",\n",
    "            4: \"Appeal_to_Fear-Prejudice\",\n",
    "            5: \"Flag_Waving\",\n",
    "            6: \"Causal_Oversimplification\",\n",
    "            7: \"False_Dilemma-No_Choice\",\n",
    "            8: \"Consequential_Oversimplification\",\n",
    "            9: \"Straw_Man\",\n",
    "            10: \"Red_Herring\",\n",
    "            11: \"Whataboutism\",\n",
    "            12: \"Slogans\",\n",
    "            13: \"Appeal_to_Time\",\n",
    "            14: \"Conversation_Killer\",\n",
    "            15: \"Loaded_Language\",\n",
    "            16: \"Repetition\",\n",
    "            17: \"Exaggeration-Minimisation\",\n",
    "            18: \"Obfuscation-Vagueness-Confusion\",\n",
    "            19: \"Name_Calling-Labeling\",\n",
    "            20: \"Doubt\",\n",
    "            21: \"Guilt_by_Association\",\n",
    "            22: \"Appeal_to_Hypocrisy\",\n",
    "            23: \"Questioning_the_Reputation\"\n",
    "        }\n",
    "\n",
    "\n",
    "def encode_tags(tags: List[Dict], \n",
    "                token_offsets: List[Tuple[int, int]], \n",
    "                label_to_id: Dict[str, int], \n",
    "                max_length: int) -> List[int]:\n",
    "    \"\"\"Encode tags for token classification.\"\"\"\n",
    "    token_labels = [\"O\"] * len(token_offsets)\n",
    "    \n",
    "    for annotation in tags:\n",
    "        label = annotation[\"tag\"]\n",
    "        start = annotation[\"start\"]\n",
    "        end = annotation[\"end\"]\n",
    "        \n",
    "        # Find all tokens that overlap with the annotation span\n",
    "        for idx, (token_start, token_end) in enumerate(token_offsets):\n",
    "            if token_start < end and token_end > start:\n",
    "                token_labels[idx] = label\n",
    "    \n",
    "    # Convert string labels to ids\n",
    "    token_labels = [label_to_id[label] for label in token_labels]\n",
    "    \n",
    "    # Pad with -100 (ignored in loss calculation)\n",
    "    if len(token_labels) < max_length:\n",
    "        token_labels += [-100] * (max_length - len(token_labels))\n",
    "    \n",
    "    return token_labels\n",
    "\n",
    "def preprocess_data(texts: List[str], \n",
    "                    annotations: List[Dict], \n",
    "                    tokenizer, \n",
    "                    config: TokenClassificationConfig,\n",
    "                    label_to_id: Dict[str, int]) -> Tuple[Dict[str, List], List[List[int]]]:\n",
    "    \"\"\"Preprocess texts and annotations into model inputs.\"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for text, tags in zip(texts, annotations):\n",
    "        text_length = len(text)\n",
    "        start_idx = 0\n",
    "\n",
    "        while start_idx < text_length:\n",
    "            # Tokenize text chunk with overlap but without padding\n",
    "            encoded_chunk = tokenizer(\n",
    "                text[start_idx:start_idx + config.max_length],\n",
    "                truncation=True,\n",
    "                max_length=config.max_length,\n",
    "                padding=False,  # Don't pad here - we'll do it dynamically\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "\n",
    "            input_ids.append(encoded_chunk[\"input_ids\"])\n",
    "            attention_masks.append(encoded_chunk[\"attention_mask\"])\n",
    "            token_offsets = encoded_chunk.pop(\"offset_mapping\")\n",
    "            \n",
    "            chunk_labels = encode_tags(\n",
    "                tags, \n",
    "                token_offsets, \n",
    "                label_to_id, \n",
    "                len(encoded_chunk[\"input_ids\"])  # Use actual length instead of max_length\n",
    "            )\n",
    "            labels.append(chunk_labels)\n",
    "\n",
    "            start_idx += config.max_length - config.stride\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_masks}, labels\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute metrics for token classification, handling class imbalance \n",
    "    and reporting per-class metrics.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids.flatten()\n",
    "    preds = np.argmax(pred.predictions, axis=2).flatten()\n",
    "    \n",
    "    # Filter out padding tokens (-100)\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "\n",
    "    # Calculate metrics excluding the 'O' class for a more realistic evaluation\n",
    "    non_o_mask = labels != 0  # Assuming 0 is the ID for 'O' class\n",
    "    propaganda_labels = labels[non_o_mask]\n",
    "    propaganda_preds = preds[non_o_mask]\n",
    "\n",
    "    # Calculate metrics only for propaganda classes\n",
    "    propaganda_precision, propaganda_recall, propaganda_f1, _ = precision_recall_fscore_support(\n",
    "        propaganda_labels, \n",
    "        propaganda_preds, \n",
    "        average=\"micro\",\n",
    "        labels=list(range(1, 24))  # Exclude 'O' class\n",
    "    )\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision, per_class_recall, per_class_f1, support = precision_recall_fscore_support(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=list(range(24)\n",
    "        ),  # Include all classes\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"propaganda_precision\": propaganda_precision,\n",
    "        \"propaganda_recall\": propaganda_recall,\n",
    "        \"propaganda_f1\": propaganda_f1,\n",
    "        \"per_class_precision\": per_class_precision.tolist(),\n",
    "        \"per_class_recall\": per_class_recall.tolist(),\n",
    "        \"per_class_f1\": per_class_f1.tolist(),\n",
    "        \"support\": support.tolist()\n",
    "    }\n",
    "\n",
    "    '''    # Add confusion matrix\n",
    "    confusion = confusion_matrix(\n",
    "        labels, \n",
    "        preds, \n",
    "        labels=list(range(24))\n",
    "    )\n",
    "    results[\"confusion_matrix\"] = confusion.tolist()'''\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_detailed_metrics(metrics: Dict[str, float], \n",
    "                         output_dir: str,\n",
    "                         id_to_label: Dict[int, str]):\n",
    "    \"\"\"Save detailed metrics with per-class breakdown.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create detailed report\n",
    "    detailed_metrics = {\n",
    "        \"overall_propaganda_metrics\": {\n",
    "            \"precision\": metrics[\"propaganda_precision\"],\n",
    "            \"recall\": metrics[\"propaganda_recall\"],\n",
    "            \"f1\": metrics[\"propaganda_f1\"]\n",
    "        },\n",
    "        \"per_class_metrics\": {}\n",
    "    }\n",
    "\n",
    "    # Add per-class metrics\n",
    "    for i in range(len(id_to_label)):\n",
    "        class_name = id_to_label[i]\n",
    "        detailed_metrics[\"per_class_metrics\"][class_name] = {\n",
    "            \"precision\": metrics[\"per_class_precision\"][i],\n",
    "            \"recall\": metrics[\"per_class_recall\"][i],\n",
    "            \"f1\": metrics[\"per_class_f1\"][i],\n",
    "            \"support\": metrics[\"support\"][i]\n",
    "        }\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_path = os.path.join(output_dir, 'detailed_results.json')\n",
    "    with open(metrics_path, 'w', encoding='utf8') as f:\n",
    "        json.dump(detailed_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Save confusion matrix separately\n",
    "    #confusion_path = os.path.join(output_dir, 'confusion_matrix.npy')\n",
    "    #np.save(confusion_path, np.array(metrics[\"confusion_matrix\"]))\n",
    "\n",
    "class PropagandaThresholdTrainer(Trainer):\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # Compute predictions and loss\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Apply threshold to 'O' class predictions during evaluation\n",
    "            if not prediction_loss_only:\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                o_class_probs = probs[:, :, 0]\n",
    "                \n",
    "                # If 'O' class probability is below threshold, redistribute to other classes\n",
    "                mask = o_class_probs < 0.9  # High threshold for 'O' class\n",
    "                probs[mask] = probs[mask] * (1 - o_class_probs[mask].unsqueeze(-1))\n",
    "                probs[mask] = probs[mask] / probs[mask].sum(dim=-1, keepdim=True)\n",
    "                \n",
    "                # Convert back to logits\n",
    "                logits = torch.log(probs + 1e-10)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return outputs.loss, None, None\n",
    "\n",
    "        return outputs.loss, logits, inputs[\"labels\"]\n",
    "\n",
    "\n",
    "def setup_training(config, train_dataset, val_dataset, model, tokenizer):\n",
    "    num_training_steps = (\n",
    "        len(train_dataset) // config.train_batch_size * config.num_train_epochs\n",
    "    )\n",
    "    num_warmup_steps = int(num_training_steps * config.warmup_ratio)\n",
    "    \n",
    "    # Create custom data collator for dynamic padding\n",
    "    data_collator = DataCollatorForDynamicTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        max_length=config.max_length,\n",
    "        pad_to_multiple_of=8  # Optimize for GPU efficiency\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.full_output_dir,\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_only_model=True,\n",
    "        metric_for_best_model='propaganda_f1',\n",
    "        greater_is_better=True,\n",
    "        logging_strategy='epoch',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=config.train_batch_size,\n",
    "        per_device_eval_batch_size=config.eval_batch_size,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        weight_decay=0.02,\n",
    "        warmup_steps=num_warmup_steps,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        label_smoothing_factor=0,\n",
    "        dataloader_num_workers=4,\n",
    "        dataloader_pin_memory=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        remove_unused_columns=True,\n",
    "        max_grad_norm=1.0,\n",
    "    )\n",
    "\n",
    "    trainer = PropagandaThresholdTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,  # Add the custom data collator\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=3,\n",
    "                early_stopping_threshold=0.001\n",
    "            ),\n",
    "            MetricsCallback(\n",
    "                output_dir=config.full_output_dir,\n",
    "                id_to_label=config.id_to_label\n",
    "            )\n",
    "        ],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12136' max='15160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12136/15160 43:00 < 10:43, 4.70 it/s, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Propaganda Precision</th>\n",
       "      <th>Propaganda Recall</th>\n",
       "      <th>Propaganda F1</th>\n",
       "      <th>Per Class Precision</th>\n",
       "      <th>Per Class Recall</th>\n",
       "      <th>Per Class F1</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.584800</td>\n",
       "      <td>1.333735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.7229969723422623, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.9990462993423785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[0.8388958181610177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.168200</td>\n",
       "      <td>1.265712</td>\n",
       "      <td>0.157456</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>0.021244</td>\n",
       "      <td>[0.7339846720879892, 0.07938360961942563, 0.0, 0.0, 0.0, 0.0005841121495327102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2604166666666667, 0.0, 0.0, 0.0, 0.3048543689320388, 0.24159248269040554, 0.0, 0.0877228119367114, 0.010025062656641603]</td>\n",
       "      <td>[0.979723061279421, 0.12092471843509188, 0.0, 0.0, 0.0, 0.0015608740894901144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007214590788410481, 0.0, 0.0, 0.0, 0.00560154131582703, 0.019840383404747884, 0.0, 0.032718308807051615, 0.00016536442184464012]</td>\n",
       "      <td>[0.839234947556889, 0.09584664536741214, 0.0, 0.0, 0.0, 0.0008500991782374609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0014389317370783932, 0.0, 0.0, 0.0, 0.011000945941211504, 0.03666935650346238, 0.0, 0.04766050054406964, 0.0003253619651862697]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.053300</td>\n",
       "      <td>1.378802</td>\n",
       "      <td>0.262915</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>[0.7309002159016239, 0.19618304945618714, 0.0, 0.0, 0.012942519984773505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.02309782608695652, 0.0, 0.03018867924528302, 0.0, 0.29155313351498635, 0.15851025548758546, 0.0, 0.0, 0.08174791914387634]</td>\n",
       "      <td>[0.9702674681739789, 0.11333728512151749, 0.0, 0.0, 0.0026413921690490987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002452783909737552, 0.0, 0.0, 0.0004905921736119127, 0.0, 0.0006637902422834385, 0.0, 0.0038176109604681034, 0.05367260321263936, 0.0, 0.0, 0.011368804001819008]</td>\n",
       "      <td>[0.8337434441167841, 0.14367297865945297, 0.0, 0.0, 0.004387379830956836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004898359049718343, 0.0, 0.0, 0.0009607776647451114, 0.0, 0.0012990176179264433, 0.0, 0.007536538122908962, 0.08019175629958887, 0.0, 0.0, 0.01996152869016078]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.478083</td>\n",
       "      <td>0.242587</td>\n",
       "      <td>0.033415</td>\n",
       "      <td>0.058739</td>\n",
       "      <td>[0.7402003418812362, 0.10246863617968434, 0.0, 0.0, 0.029649595687331536, 0.0, 0.0, 0.0, 0.013842482100238664, 0.0, 0.0, 0.0, 0.010869565217391304, 0.0, 0.0, 0.09395532194480946, 0.0, 0.00938337801608579, 0.0, 0.2304147465437788, 0.18434025770917623, 0.0, 0.20601755119097367, 0.04867803311094638]</td>\n",
       "      <td>[0.9403289104219211, 0.15008891523414344, 0.0, 0.0, 0.001709136109384711, 0.0, 0.0, 0.0, 0.0106539309331374, 0.0, 0.0, 0.0, 0.0029433406916850625, 0.0, 0.0, 0.0041267459309707955, 0.0, 0.0011616329239960172, 0.0, 0.0035678607107178534, 0.11010702028714742, 0.0, 0.03682677224172705, 0.008144197775848527]</td>\n",
       "      <td>[0.8283483075599047, 0.12178932178932178, 0.0, 0.0, 0.003231967092698692, 0.0, 0.0, 0.0, 0.012040689225659124, 0.0, 0.0, 0.0, 0.004632310364794441, 0.0, 0.0, 0.007906230994637031, 0.0, 0.0020673360897814526, 0.0, 0.007026913077085236, 0.13786615134255492, 0.0, 0.06248415716096324, 0.013953817821221137]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.776800</td>\n",
       "      <td>1.599403</td>\n",
       "      <td>0.172833</td>\n",
       "      <td>0.021435</td>\n",
       "      <td>0.038139</td>\n",
       "      <td>[0.7359712267866161, 0.1018664563617245, 0.0, 0.0, 0.044167610419026046, 0.0, 0.0, 0.008980454305335447, 0.0, 0.0440251572327044, 0.0, 0.03225806451612903, 0.0, 0.0, 0.0, 0.1022944550669216, 0.0, 0.011057692307692308, 0.0, 0.19581589958158996, 0.1149544655695798, 0.0, 0.06446478312773578, 0.04866100587851078]</td>\n",
       "      <td>[0.9346249829696311, 0.09187907528156491, 0.0, 0.0, 0.0030298321939092603, 0.0, 0.0, 0.0015460167333575846, 0.0, 0.003336510962821735, 0.0, 0.1345962113659023, 0.0, 0.0, 0.0, 0.006175689714879372, 0.0, 0.0019083969465648854, 0.0, 0.008348794063079777, 0.05844485510630953, 0.0, 0.012101292298498543, 0.018479474141138534]</td>\n",
       "      <td>[0.8234869578710983, 0.09661534625693449, 0.0, 0.0, 0.005670665212649945, 0.0, 0.0, 0.0026379082938940185, 0.0, 0.006202924235711121, 0.0, 0.0520431765612953, 0.0, 0.0, 0.0, 0.011648160243849334, 0.0, 0.0032550240588734786, 0.0, 0.016014782876501384, 0.07749161943483353, 0.0, 0.020377358490566037, 0.026786516853932584]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.663300</td>\n",
       "      <td>1.816410</td>\n",
       "      <td>0.153315</td>\n",
       "      <td>0.035481</td>\n",
       "      <td>0.057626</td>\n",
       "      <td>[0.7480031994352598, 0.09545911657770557, 0.0, 0.00015080681646810435, 0.035876475930971846, 0.007142857142857143, 0.015482695810564663, 0.02201565557729941, 0.0, 0.00935785737334624, 0.0, 0.01977726574500768, 0.0189873417721519, 0.004206730769230769, 0.005952380952380952, 0.08623643632210165, 0.0, 0.005228302544440571, 0.0, 0.13740053050397877, 0.11718870544669033, 0.0, 0.08792940434919634, 0.047435897435897434]</td>\n",
       "      <td>[0.8732259340118896, 0.14629519857735626, 0.0, 0.0017730496453900709, 0.01841205717837166, 0.003121748178980229, 0.01754748142031379, 0.004092397235358312, 0.0, 0.006911344137273594, 0.0, 0.10269192422731804, 0.002207505518763797, 0.006944444444444444, 0.0018045112781954887, 0.013072838508599792, 0.0, 0.002489213408562894, 0.0, 0.00924075924075924, 0.09254107182746786, 0.0, 0.04168222902816165, 0.022944313530943818]</td>\n",
       "      <td>[0.8057785034764544, 0.11553225353431326, 0.0, 0.0002779708130646282, 0.024335147345723382, 0.0043446777697320775, 0.01645055157731759, 0.006901840490797546, 0.0, 0.007950651130911583, 0.0, 0.03316696184189341, 0.003955174686882004, 0.005239520958083833, 0.0027694438033694898, 0.022703921814308983, 0.0, 0.0033726812816188873, 0.0, 0.01731688563500819, 0.10341658250973006, 0.0, 0.056555009375158366, 0.030928696815180144]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>1.947873</td>\n",
       "      <td>0.169417</td>\n",
       "      <td>0.042153</td>\n",
       "      <td>0.067509</td>\n",
       "      <td>[0.7495894113714601, 0.09513335166345889, 0.0, 0.00018214936247723133, 0.020335801439149023, 0.01053704962610469, 0.019235172887565835, 0.04296248797691568, 0.019765110283586365, 0.04232062780269058, 0.0, 0.029917061611374408, 0.020796197266785502, 0.0030828516377649326, 0.0, 0.08351729212656366, 0.0, 0.01582805731422859, 0.0, 0.1863013698630137, 0.11950313443231948, 0.0, 0.10580235720761559, 0.05543286219081272]</td>\n",
       "      <td>[0.860701551508143, 0.12305868405453468, 0.0, 0.0017730496453900709, 0.015149160969546303, 0.016129032258064516, 0.017341040462427744, 0.012186249545289195, 0.025349008082292433, 0.035986653956148716, 0.0, 0.10069790628115653, 0.008584743684081433, 0.007936507936507936, 0.0, 0.01965254530763015, 0.0, 0.007882509127115831, 0.0, 0.009704581133152562, 0.10452247019880999, 0.0, 0.08717412415029506, 0.020753234941502337]</td>\n",
       "      <td>[0.8013120414061828, 0.10730900444536338, 0.0, 0.0003303600925008259, 0.017363429945238415, 0.01274671052631579, 0.0182390619910976, 0.018986893375841304, 0.022211492032834384, 0.03889747552807831, 0.0, 0.04612925325416762, 0.012152777777777778, 0.004440743824590619, 0.0, 0.03181796944353595, 0.0, 0.010523983604741332, 0.0, 0.018448182311448725, 0.11151192139785296, 0.0, 0.09558913871482982, 0.030200030079711237]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.491700</td>\n",
       "      <td>2.073999</td>\n",
       "      <td>0.154073</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>0.066820</td>\n",
       "      <td>[0.7536019879157206, 0.08756460930373974, 0.0, 0.0013312852022529442, 0.03072825975622247, 0.009631728045325779, 0.01752577319587629, 0.0298376480912681, 0.031028585389689715, 0.0423728813559322, 0.0, 0.007678244972577696, 0.010632344711807499, 0.001119402985074627, 0.0, 0.06260616355253579, 0.0, 0.019071985114548204, 0.0052462838822500725, 0.17816365366317793, 0.12050485825904036, 0.00034106412005457026, 0.11923480083857442, 0.050716032134125046]</td>\n",
       "      <td>[0.8465322845949098, 0.1024303497332543, 0.0, 0.02304964539007092, 0.023306401491609695, 0.008844953173777315, 0.014037985136251032, 0.012368133866860677, 0.04665686994856723, 0.02502383222116301, 0.0, 0.020937188434695914, 0.004660289428501349, 0.002976190476190476, 0.0, 0.02233637308091885, 0.0, 0.013607699966810488, 0.017029328287606435, 0.026723276723276724, 0.09771947281847167, 0.0003029385034837928, 0.06797639501008441, 0.030013642564802184]</td>\n",
       "      <td>[0.7973686002045461, 0.09441591082941755, 0.0, 0.0025171846258108237, 0.02650762094102054, 0.009221589368049905, 0.015589179275561669, 0.017487463032017488, 0.037270726338958185, 0.03146538807311956, 0.0, 0.011235955056179775, 0.006480218281036835, 0.0016268980477223427, 0.0, 0.0329256620227587, 0.0, 0.01588300808677546, 0.00802139037433155, 0.04647555224621494, 0.10792262405382674, 0.0003208727739451308, 0.0865883248489462, 0.03771036775399959]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>2.097882</td>\n",
       "      <td>0.150777</td>\n",
       "      <td>0.040504</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>[0.7508701682374547, 0.0934259997721317, 0.0, 0.0016689573924995092, 0.020858895705521473, 0.0037858301784748512, 0.014556797504549, 0.06143344709897611, 0.016638465877044557, 0.04223149113660062, 0.0, 0.025207057976233346, 0.005409060175794456, 0.001534526854219949, 0.0, 0.05324291176682779, 0.0, 0.020682033470161034, 0.0, 0.18453227182257606, 0.11607119657009224, 0.0006504770164787511, 0.11034883720930233, 0.039741100323624594]</td>\n",
       "      <td>[0.8437393040975898, 0.0972139893301719, 0.0, 0.030141843971631204, 0.02113113735239279, 0.0036420395421436005, 0.011560693641618497, 0.02782830120043652, 0.021675238795003673, 0.038608198284080075, 0.0, 0.06979062811565304, 0.0019622271277900416, 0.002976190476190476, 0.0, 0.02129747200738774, 0.0, 0.010869565217391304, 0.0, 0.02315541601255887, 0.09071340088946653, 0.0004544077552256892, 0.07088966908194518, 0.02538343875315226]</td>\n",
       "      <td>[0.7946004137158379, 0.09528236114338832, 0.0, 0.003162790697674419, 0.020994133991972832, 0.003712543092018032, 0.012886894488551376, 0.03830506352882268, 0.018825781748564138, 0.04033864541832669, 0.0, 0.037037037037037035, 0.002879769618430526, 0.0020249746878164025, 0.0, 0.03042483458042174, 0.0, 0.01424997280539541, 0.0, 0.04114756696782375, 0.10183749772022616, 0.0005350454788657037, 0.08632373675353619, 0.030979590807033476]</td>\n",
       "      <td>[601866, 8435, 2417, 564, 12872, 1922, 4844, 10996, 2722, 4196, 0, 1003, 4077, 1008, 3325, 34652, 2783, 12052, 1057, 28028, 49243, 6602, 13387, 24189]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0000 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0000 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0000 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0000 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0000 (support: 28028)\n",
      "Doubt: 0.0000 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0000 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0000 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1: 0.0000\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0000 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0000 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0000 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0000 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0000 (support: 28028)\n",
      "Doubt: 0.0000 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0000 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0000 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1575\n",
      "Recall: 0.0114\n",
      "F1: 0.0212\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0958 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0000 (support: 12872)\n",
      "Flag_Waving: 0.0009 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0014 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0000 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0110 (support: 28028)\n",
      "Doubt: 0.0367 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0477 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0003 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.3330\n",
      "Recall: 0.0093\n",
      "F1: 0.0182\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0921 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0000 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0002 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0000 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0062 (support: 28028)\n",
      "Doubt: 0.0543 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0000 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0000 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.2629\n",
      "Recall: 0.0175\n",
      "F1: 0.0329\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1437 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0044 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0005 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0010 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0013 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0075 (support: 28028)\n",
      "Doubt: 0.0802 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0000 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0200 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1562\n",
      "Recall: 0.0098\n",
      "F1: 0.0185\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0816 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0079 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0047 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0082 (support: 34652)\n",
      "Repetition: 0.0007 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0006 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0101 (support: 28028)\n",
      "Doubt: 0.0448 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0012 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0068 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.2426\n",
      "Recall: 0.0334\n",
      "F1: 0.0587\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1218 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0032 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0120 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0000 (support: 1003)\n",
      "Slogans: 0.0046 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0079 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0021 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0070 (support: 28028)\n",
      "Doubt: 0.1379 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0625 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0140 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1956\n",
      "Recall: 0.0283\n",
      "F1: 0.0494\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1179 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0281 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0042 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0000 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0150 (support: 2722)\n",
      "Straw_Man: 0.0068 (support: 4196)\n",
      "Whataboutism: 0.0021 (support: 1003)\n",
      "Slogans: 0.0008 (support: 4077)\n",
      "Appeal_to_Time: 0.0020 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0155 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0000 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0241 (support: 28028)\n",
      "Doubt: 0.0960 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0800 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0278 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1728\n",
      "Recall: 0.0214\n",
      "F1: 0.0381\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0966 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0000 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0057 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0000 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0026 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0062 (support: 4196)\n",
      "Whataboutism: 0.0520 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0116 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0033 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0160 (support: 28028)\n",
      "Doubt: 0.0775 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0204 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0268 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1896\n",
      "Recall: 0.0379\n",
      "F1: 0.0631\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1289 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0024 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0185 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0021 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0047 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0042 (support: 2722)\n",
      "Straw_Man: 0.0191 (support: 4196)\n",
      "Whataboutism: 0.0052 (support: 1003)\n",
      "Slogans: 0.0000 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0380 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0011 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0191 (support: 28028)\n",
      "Doubt: 0.1100 (support: 49243)\n",
      "Guilt_by_Association: 0.0005 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0550 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0435 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1533\n",
      "Recall: 0.0355\n",
      "F1: 0.0576\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1155 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0003 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0243 (support: 12872)\n",
      "Flag_Waving: 0.0043 (support: 1922)\n",
      "Causal_Oversimplification: 0.0165 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0069 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0000 (support: 2722)\n",
      "Straw_Man: 0.0080 (support: 4196)\n",
      "Whataboutism: 0.0332 (support: 1003)\n",
      "Slogans: 0.0040 (support: 4077)\n",
      "Appeal_to_Time: 0.0052 (support: 1008)\n",
      "Conversation_Killer: 0.0028 (support: 3325)\n",
      "Loaded_Language: 0.0227 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0034 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0173 (support: 28028)\n",
      "Doubt: 0.1034 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0566 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0309 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1827\n",
      "Recall: 0.0380\n",
      "F1: 0.0629\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0797 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0051 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0097 (support: 12872)\n",
      "Flag_Waving: 0.0000 (support: 1922)\n",
      "Causal_Oversimplification: 0.0142 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0129 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0155 (support: 2722)\n",
      "Straw_Man: 0.0000 (support: 4196)\n",
      "Whataboutism: 0.0614 (support: 1003)\n",
      "Slogans: 0.0108 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0407 (support: 34652)\n",
      "Repetition: 0.0034 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0031 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0329 (support: 28028)\n",
      "Doubt: 0.1116 (support: 49243)\n",
      "Guilt_by_Association: 0.0005 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0663 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0429 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1694\n",
      "Recall: 0.0422\n",
      "F1: 0.0675\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1073 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0003 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0174 (support: 12872)\n",
      "Flag_Waving: 0.0127 (support: 1922)\n",
      "Causal_Oversimplification: 0.0182 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0190 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0222 (support: 2722)\n",
      "Straw_Man: 0.0389 (support: 4196)\n",
      "Whataboutism: 0.0461 (support: 1003)\n",
      "Slogans: 0.0122 (support: 4077)\n",
      "Appeal_to_Time: 0.0044 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0318 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0105 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0184 (support: 28028)\n",
      "Doubt: 0.1115 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0956 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0302 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1607\n",
      "Recall: 0.0428\n",
      "F1: 0.0676\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.1098 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0028 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0188 (support: 12872)\n",
      "Flag_Waving: 0.0062 (support: 1922)\n",
      "Causal_Oversimplification: 0.0228 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0244 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0141 (support: 2722)\n",
      "Straw_Man: 0.0182 (support: 4196)\n",
      "Whataboutism: 0.0506 (support: 1003)\n",
      "Slogans: 0.0012 (support: 4077)\n",
      "Appeal_to_Time: 0.0000 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0434 (support: 34652)\n",
      "Repetition: 0.0056 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0064 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0181 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0440 (support: 28028)\n",
      "Doubt: 0.1129 (support: 49243)\n",
      "Guilt_by_Association: 0.0000 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0701 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0279 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1541\n",
      "Recall: 0.0427\n",
      "F1: 0.0668\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0944 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0025 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0265 (support: 12872)\n",
      "Flag_Waving: 0.0092 (support: 1922)\n",
      "Causal_Oversimplification: 0.0156 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0175 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0373 (support: 2722)\n",
      "Straw_Man: 0.0315 (support: 4196)\n",
      "Whataboutism: 0.0112 (support: 1003)\n",
      "Slogans: 0.0065 (support: 4077)\n",
      "Appeal_to_Time: 0.0016 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0329 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0159 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0080 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0465 (support: 28028)\n",
      "Doubt: 0.1079 (support: 49243)\n",
      "Guilt_by_Association: 0.0003 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0866 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0377 (support: 24189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Propaganda Detection Metrics:\n",
      "Precision: 0.1508\n",
      "Recall: 0.0405\n",
      "F1: 0.0639\n",
      "\n",
      "Per-class F1 scores (excluding 'O'):\n",
      "Appeal_to_Authority: 0.0953 (support: 8435)\n",
      "Appeal_to_Popularity: 0.0000 (support: 2417)\n",
      "Appeal_to_Values: 0.0032 (support: 564)\n",
      "Appeal_to_Fear-Prejudice: 0.0210 (support: 12872)\n",
      "Flag_Waving: 0.0037 (support: 1922)\n",
      "Causal_Oversimplification: 0.0129 (support: 4844)\n",
      "False_Dilemma-No_Choice: 0.0383 (support: 10996)\n",
      "Consequential_Oversimplification: 0.0188 (support: 2722)\n",
      "Straw_Man: 0.0403 (support: 4196)\n",
      "Whataboutism: 0.0370 (support: 1003)\n",
      "Slogans: 0.0029 (support: 4077)\n",
      "Appeal_to_Time: 0.0020 (support: 1008)\n",
      "Conversation_Killer: 0.0000 (support: 3325)\n",
      "Loaded_Language: 0.0304 (support: 34652)\n",
      "Repetition: 0.0000 (support: 2783)\n",
      "Exaggeration-Minimisation: 0.0142 (support: 12052)\n",
      "Obfuscation-Vagueness-Confusion: 0.0000 (support: 1057)\n",
      "Name_Calling-Labeling: 0.0411 (support: 28028)\n",
      "Doubt: 0.1018 (support: 49243)\n",
      "Guilt_by_Association: 0.0005 (support: 6602)\n",
      "Appeal_to_Hypocrisy: 0.0863 (support: 13387)\n",
      "Questioning_the_Reputation: 0.0310 (support: 24189)\n",
      "\n",
      "Best Model Performance:\n",
      "Step: 10619\n",
      "Best Propaganda F1: 0.0676\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    config = TokenClassificationConfig()\n",
    "    \n",
    "    # Load data\n",
    "    with open('/home/lgiordano/LUCA/checkthat_GITHUB/data/formatted/train.json', 'r', encoding='utf8') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    texts = [item['text'] for item in dataset]\n",
    "    annotations = [item['annotations'] for item in dataset]\n",
    "\n",
    "    # Split data\n",
    "    texts_train, texts_val, annotations_train, annotations_val = train_test_split(\n",
    "        texts, annotations, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    model = setup_model(config.model_name, config.num_labels)\n",
    "\n",
    "    # Create label_to_id mapping from id_to_label\n",
    "    label_to_id = {v: k for k, v in config.id_to_label.items()}\n",
    "\n",
    "    # Preprocess data\n",
    "    train_inputs, train_labels = preprocess_data(\n",
    "        texts_train, annotations_train, tokenizer, config, label_to_id\n",
    "    )\n",
    "    val_inputs, val_labels = preprocess_data(\n",
    "        texts_val, annotations_val, tokenizer, config, label_to_id\n",
    "    )\n",
    "\n",
    "    # Create datasets with pad token ID\n",
    "    train_dataset = TokenClassificationDataset(train_inputs, train_labels, tokenizer.pad_token_id)\n",
    "    val_dataset = TokenClassificationDataset(val_inputs, val_labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Setup and run training\n",
    "    trainer = setup_training(config, train_dataset, val_dataset, model, tokenizer)\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
