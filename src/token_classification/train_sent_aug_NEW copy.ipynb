{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from tokenizers import Encoding\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from seqeval.metrics import classification_report, precision_score\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_annotations_bio(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "            print('char_ix = ', char_ix)\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"B\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"I\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremental ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BI\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bio(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "    \n",
    "class WeightedLoss(CrossEntropyLoss):\n",
    "    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean'):\n",
    "        super(WeightedLoss, self).__init__(weight, size_average, ignore_index, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensure weight tensor is on the same device as input\n",
    "        weight = torch.tensor([0.5 if x == 0 else 2.0 for x in target.view(-1)], device=input.device, dtype=input.dtype)\n",
    "        # Ensure target is on the same device as input\n",
    "        target = target.to(input.device)\n",
    "        loss = super(WeightedLoss, self).forward(input.view(-1, input.size(-1)), target.view(-1))\n",
    "        return (loss * weight).mean()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = WeightedLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def tokenize_token_classification(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = [tokenized_inputs.token_to_word(i, j) for j in range(len(tokenized_inputs['input_ids'][i]))]  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "def dict_of_lists(lst_of_dicts):\n",
    "    result = defaultdict(list)\n",
    "    for d in lst_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            result[key].append(value)\n",
    "    return dict(result)\n",
    "\n",
    "def list_of_dicts(dict_of_lists):\n",
    "    # First, we need to check if all lists are of the same length to ensure correct transformation\n",
    "    if not all(len(lst) == len(next(iter(dict_of_lists.values()))) for lst in dict_of_lists.values()):\n",
    "        raise ValueError(\"All lists in the dictionary must have the same length\")\n",
    "\n",
    "    # Get the length of the items in any of the lists\n",
    "    length = len(next(iter(dict_of_lists.values())))\n",
    "    \n",
    "    # Create a list of dictionaries, one for each index in the lists\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        # Create a dictionary for the current index 'i' across all lists\n",
    "        new_dict = {key: dict_of_lists[key][i] for key in dict_of_lists}\n",
    "        result.append(new_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sub_shift_spans(text, ents = [], mappings = []):\n",
    "    for mapping in mappings:\n",
    "        adjustment = 0\n",
    "        pattern = re.compile(mapping['pattern'])\n",
    "        for match in re.finditer(pattern, text):\n",
    "            match_index = match.start() + adjustment\n",
    "            match_contents = match.group()\n",
    "            if all(mapping['check'](char) for char in match_contents):\n",
    "                subbed_text = mapping['target'].replace('placeholder', match_contents)\n",
    "            else:\n",
    "                subbed_text = mapping['target']\n",
    "            len_diff = len(subbed_text) - len(match_contents)\n",
    "            text = text[:match_index] + subbed_text + text[match_index + len(match_contents):]\n",
    "            if ents:\n",
    "                if isinstance(ents, list):\n",
    "                    for ent in ents:\n",
    "                        if ent['start'] <= match_index and ent['end'] > match_index:\n",
    "                            ent['end'] += len_diff\n",
    "                        if ent['start'] > match_index:\n",
    "                            ent['start'] += len_diff\n",
    "                            ent['end'] += len_diff\n",
    "                elif isinstance(ents, dict):\n",
    "                    if ents['value']['start'] <= match_index and ents['value']['end'] > match_index:\n",
    "                        ents['value']['end'] += len_diff\n",
    "                    if ents['value']['start'] > match_index:\n",
    "                        ents['value']['start'] += len_diff\n",
    "                        ents['value']['end'] += len_diff\n",
    "\n",
    "            adjustment += len_diff\n",
    "\n",
    "    return text, ents\n",
    "\n",
    "def get_entities_from_sample(sample, field = 'annotations', langs = ['en'], sort = False):\n",
    "    entities = []\n",
    "    for lang in langs:\n",
    "        entities += [ent for ent in sample[field][0]['result'] if ent['type'] == 'labels' and ent[f'from_name'] == f'label_{lang}']\n",
    "    if sort:\n",
    "        entities = sorted(entities, key = lambda ent: ent['value']['start'])\n",
    "    return entities\n",
    "'''\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    # if not any([l for l in samples['annotations']]):\n",
    "        \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                if annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        if first:\n",
    "                            labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                            first = False\n",
    "                        elif not first:\n",
    "                            labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0]})\n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        # if the training sample has no tags that we need, we just produce a 0s list\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # if the training sample has tags we need, we first exclude the label lists whose tags don't match\n",
    "        # and then we merge the label lists that have tags that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    return samples_new\n",
    "'''\n",
    "\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                # Check if 'labels' is non-empty\n",
    "                if annotation['value']['labels'] and annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        # Ensure the 'labels' list is not empty\n",
    "                        if isinstance(ents['value']['labels'], list) and ents['value']['labels']:\n",
    "                            if first:\n",
    "                                labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                                first = False\n",
    "                            else:\n",
    "                                labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                        else:\n",
    "                            # If 'labels' is empty, append 'O' or any fallback label\n",
    "                            labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0] if 'labels' in annotation['value'] and annotation['value']['labels'] else 'no_tag'})\n",
    "        \n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        \n",
    "        # If the training sample has no tags we need, produce a list of 0s\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # Otherwise, merge label lists that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "            \n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "        \n",
    "    return samples_new\n",
    "\n",
    "def make_balanced_df(df):\n",
    "    # get rows with annotations\n",
    "    df_pos = df[df['annotations'].apply(lambda x: len(x[0]['result']) > 0)]\n",
    "    # get the same number of rows without any annotations\n",
    "    df_neg = df[df['annotations'].apply(lambda x: x[0]['result'] == [])].sample(len(df_pos))\n",
    "    balanced_df = pd.concat([df_pos, df_neg])\n",
    "    return balanced_df\n",
    "\n",
    "def make_binary_balanced_df(df, target_tag='', labels_model=[]):\n",
    "    df_list = df.to_dict(orient='records')\n",
    "    df_list_binary = span_to_words_annotation(dict_of_lists(df_list), target_tag=target_tag, mappings=regex_tokenizer_mappings, labels_model=labels_model)\n",
    "    df_binary = pd.DataFrame(df_list_binary)\n",
    "    df_binary_pos = df_binary[df_binary['tag'] == target_tag]\n",
    "    df_binary_neg = df_binary[df_binary['tag'] != target_tag].sample(len(df_binary_pos), replace=True)  # Over-sampling\n",
    "    df_binary_subsampled = pd.concat([df_binary_pos, df_binary_neg])\n",
    "    return df_binary_subsampled\n",
    "\n",
    "regex_tokenizer_mappings = [\n",
    "    {'pattern': r'(?<!\\s)([^\\w\\s])|([^\\w\\s])(?!\\s)',\n",
    "    'target': ' placeholder ',\n",
    "    'check': lambda x: unicodedata.category(x).startswith('P'),\n",
    "    },\n",
    "    {'pattern': r'\\s+',\n",
    "     'target': ' ',\n",
    "     'check': lambda x: False if re.match('\\s+', x) is None else True,\n",
    "     },\n",
    "    ]\n",
    "\n",
    "def compute_metrics_wrapper(label_list, pt, model_name_simple, date_time, threshold=0):\n",
    "    def compute_metrics(eval_preds):\n",
    "        nonlocal label_list\n",
    "        nonlocal pt\n",
    "        nonlocal threshold\n",
    "        logits, labels = eval_preds\n",
    "        predictions = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        predictions = np.where(predictions >= threshold, predictions, 0)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "        # Extract the true predictions and labels from the sequences\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        # Compute sequence-level evaluation metrics\n",
    "        results = classification_report(true_predictions, true_labels, output_dict=True)\n",
    "\n",
    "        # Flatten the lists to calculate micro F1-score and supports\n",
    "        flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "        flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Calculate micro F1-score using sklearn\n",
    "        micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "        # Prepare the results dictionary\n",
    "        flat_results = {'micro_f1': float(micro_f1)}\n",
    "        \n",
    "        # Add detailed metrics for each label to the results dictionary\n",
    "        for label, metrics in results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    flat_results[f'{label}_{metric}'] = float(value)\n",
    "\n",
    "        # Compute support for each label using Counter\n",
    "        label_support = Counter(flat_true_labels)\n",
    "        for label, count in label_support.items():\n",
    "            flat_results[f'{label}_support'] = count\n",
    "        \n",
    "        models_dir = '/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results'\n",
    "        model_save_name = f'{model_name_simple}_{tt[0]}_target={tt[1]}_aug_SUBSAMPLED_{date_time}'\n",
    "        model_save_dir = os.path.join(models_dir, date_time+'_aug_no_cw_ts0_+ARAIEVAL(news)_&_SEMEVAL24', model_save_name)\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            os.makedirs(model_save_dir)\n",
    "\n",
    "        with open(os.path.join(model_save_dir, 'results.json'), 'w', encoding='utf8') as f:\n",
    "            json.dump(flat_results, f, ensure_ascii = False)\n",
    "\n",
    "        return flat_results\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "date_time = '2025-01-13-16-36-34'\n",
    "\n",
    "data_gold = '/home/lgiordano/LUCA/checkthat_GITHUB/data/formatted/train_sentences.json'\n",
    "with open(data_gold, 'r', encoding='utf8') as f:\n",
    "    dataset_gold = json.load(f)\n",
    "\n",
    "data_path_dict = {\n",
    "'sl': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/sl/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-slv_Latn_tok_regex_en-sl/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-slv_Latn_tok_regex_en-sl_mdeberta-v3-base_mdeberta_xlwa_en-sl_ME3_2024-05-04-12-12-14_ls.json',\n",
    "'ru': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/ru/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-rus_Cyrl_tok_regex_en-ru/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-rus_Cyrl_tok_regex_en-ru_mdeberta-v3-base_mdeberta_xlwa_en-ru_ME3_2024-05-04-12-09-20_ls.json',\n",
    "'pt': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/pt/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-por_Latn_tok_regex_en-pt/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-por_Latn_tok_regex_en-pt_mdeberta-v3-base_mdeberta_xlwa_en-pt_ME3_2024-05-04-12-07-45_ls.json',\n",
    "'it': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/it/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-ita_Latn_tok_regex_en-it/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-ita_Latn_tok_regex_en-it_mdeberta-v3-base_mdeberta_xlwa_en-it_ME3_2024-05-04-12-05-00_ls.json',\n",
    "'es': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/es/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-spa_Latn_tok_regex_en-es/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-spa_Latn_tok_regex_en-es_mdeberta-v3-base_mdeberta_xlwa_en-es_ME3_2024-05-04-12-01-43_ls.json',\n",
    "'bg': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/bg/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-bul_Cyrl_tok_regex_en-bg/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-bul_Cyrl_tok_regex_en-bg_mdeberta-v3-base_mdeberta_xlwa_en-bg_ME3_2024-05-04-11-58-52_ls.json',\n",
    "#'ar': '/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/araieval24_all_bin_formatted.json'\n",
    "}\n",
    "\n",
    "dataset_aug = []\n",
    "for key in data_path_dict:\n",
    "    with open(data_path_dict[key], 'r', encoding='utf8') as f:\n",
    "        dataset_aug_buffer = json.load(f)\n",
    "\n",
    "        for sample in dataset_aug_buffer:\n",
    "            sample['annotations'][0]['result'] = get_entities_from_sample(sample, langs=[key], sort = True)\n",
    "            if 'text_en' in sample['data']:\n",
    "                del sample['data']['text_en']\n",
    "            if f'text_{key}' in sample['data']:\n",
    "                sample['data']['text'] = sample['data'][f'text_{key}']\n",
    "                del sample['data'][f'text_{key}']\n",
    "            sample['data']['lang'] = key\n",
    "            if 'labels' in sample['data']:\n",
    "                sample['data']['label'] = sample['data'].pop('labels')\n",
    "        dataset_aug += dataset_aug_buffer\n",
    "\n",
    "dataset_aug += [sample for sample in json.load(open('/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/araieval24_all_bin_formatted.json')) if sample['data'].get('type') != 'tweet'] #filter out tweets from ar\n",
    "semeval_24 = json.load(open('/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/semeval24_all_bin_formatted.json', encoding='utf-8'))\n",
    "dataset_aug += semeval_24\n",
    "\n",
    "df_gold = pd.DataFrame(dataset_gold)\n",
    "balanced_df_gold = make_balanced_df(df_gold)\n",
    "\n",
    "df_aug = pd.DataFrame(dataset_aug)\n",
    "balanced_df_aug = make_balanced_df(df_aug)\n",
    "\n",
    "target_tags = [\"Appeal_to_Authority\", \"Appeal_to_Popularity\",\"Appeal_to_Values\",\"Appeal_to_Fear-Prejudice\",\"Flag_Waving\",\"Causal_Oversimplification\",\n",
    "               \"False_Dilemma-No_Choice\",\"Consequential_Oversimplification\",\"Straw_Man\",\"Red_Herring\",\"Whataboutism\",\"Slogans\",\"Appeal_to_Time\",\n",
    "               \"Conversation_Killer\",\"Loaded_Language\",\"Repetition\",\"Exaggeration-Minimisation\",\"Obfuscation-Vagueness-Confusion\",\"Name_Calling-Labeling\",\n",
    "               \"Doubt\",\"Guilt_by_Association\",\"Appeal_to_Hypocrisy\",\"Questioning_the_Reputation\"]\n",
    "target_tags = [(i, el.strip()) for i, el in enumerate(target_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 14\n",
    "for i, tt in enumerate(target_tags):\n",
    "    if i < shift:\n",
    "        continue\n",
    "    print(f'Training model no. {i} of {len(target_tags)} for {tt} persuasion technique...')\n",
    "    labels_model = LabelSet(labels=[tt[1]])\n",
    "    \n",
    "    token_columns = ['id', 'ner_tags', 'tokens', \n",
    "                     #'lang'\n",
    "                     ]\n",
    "\n",
    "    df_binary_subsampled_gold = make_binary_balanced_df(balanced_df_gold, target_tag=tt[1], labels_model=labels_model)\n",
    "    binary_dataset_gold = Dataset.from_pandas(df_binary_subsampled_gold[token_columns])\n",
    "\n",
    "    df_binary_subsampled_aug = make_binary_balanced_df(balanced_df_aug, target_tag=tt[1], labels_model=labels_model)\n",
    "    binary_dataset_aug = Dataset.from_pandas(df_binary_subsampled_aug[token_columns])\n",
    "    \n",
    "    split_ratio = 0.2\n",
    "    split_seed = 42\n",
    "    datadict = binary_dataset_gold.train_test_split(split_ratio, seed=split_seed)\n",
    "\n",
    "    #model_name = 'bert-base-multilingual-cased'\n",
    "    #model_name = 'xlm-roberta-base'\n",
    "    model_name = 'microsoft/mdeberta-v3-base'\n",
    "    #model_name = 'FacebookAI/xlm-roberta-large'\n",
    "    model_name_simple = model_name.split('/')[-1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 16\n",
    "    datadict['train'] = concatenate_datasets([datadict['train'], binary_dataset_aug]) # this is where we merge english gold data with aug data\n",
    "    datadict = datadict.map(lambda x: tokenize_token_classification(x, tokenizer), batched=True, batch_size=None)\n",
    "\n",
    "    columns = [\n",
    "                'input_ids',\n",
    "                'token_type_ids',\n",
    "                'attention_mask',\n",
    "                'labels'\n",
    "                ]\n",
    "\n",
    "    datadict.set_format('torch', columns = columns)\n",
    "\n",
    "    train_data = datadict['train']\n",
    "    val_data = datadict['test']\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding='longest')\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                num_labels=len(labels_model.ids_to_label.values()),\n",
    "                                                                label2id=labels_model.labels_to_id,\n",
    "                                                                id2label=labels_model.ids_to_label,\n",
    "                                                                )\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=f'/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results/{date_time}_aug_no_cw_ts0_+ARAIEVAL(news)_&_SEMEVAL24/mdeberta-v3-base-NEW_aug_{i}_{tt[1]}',\n",
    "                                  save_total_limit=2,\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_only_model=True,\n",
    "                                  metric_for_best_model='eval_macro avg_f1-score',\n",
    "                                  logging_strategy='epoch',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=5e-5,\n",
    "                                  optim='adamw_torch',\n",
    "                                  num_train_epochs=10)\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    ###CustomTrainer per class weighting, threshold=0.9 per threshold\n",
    "    trainer = Trainer(model,\n",
    "                      training_args,\n",
    "                      train_dataset=train_data,\n",
    "                      eval_dataset=val_data,\n",
    "                      data_collator=data_collator,\n",
    "                      tokenizer=tokenizer,\n",
    "                      callbacks=[early_stopping],\n",
    "                      compute_metrics=compute_metrics_wrapper(\n",
    "                          label_list=[i for i in labels_model.ids_to_label.values()],\n",
    "                          pt=tt[1],\n",
    "                          model_name_simple=model_name_simple,\n",
    "                          date_time=date_time,\n",
    "                          #threshold=0.9\n",
    "                          ),\n",
    "                      )\n",
    "    \n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
