{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from tokenizers import Encoding\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from seqeval.metrics import classification_report, precision_score\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_annotations_bio(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "            print('char_ix = ', char_ix)\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"B\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"I\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremental ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BI\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bio(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "    \n",
    "class WeightedLoss(CrossEntropyLoss):\n",
    "    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean'):\n",
    "        super(WeightedLoss, self).__init__(weight, size_average, ignore_index, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensure weight tensor is on the same device as input\n",
    "        weight = torch.tensor([0.5 if x == 0 else 2.0 for x in target.view(-1)], device=input.device, dtype=input.dtype)\n",
    "        # Ensure target is on the same device as input\n",
    "        target = target.to(input.device)\n",
    "        loss = super(WeightedLoss, self).forward(input.view(-1, input.size(-1)), target.view(-1))\n",
    "        return (loss * weight).mean()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = WeightedLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def tokenize_token_classification(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = [tokenized_inputs.token_to_word(i, j) for j in range(len(tokenized_inputs['input_ids'][i]))]  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "def dict_of_lists(lst_of_dicts):\n",
    "    result = defaultdict(list)\n",
    "    for d in lst_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            result[key].append(value)\n",
    "    return dict(result)\n",
    "\n",
    "def list_of_dicts(dict_of_lists):\n",
    "    # First, we need to check if all lists are of the same length to ensure correct transformation\n",
    "    if not all(len(lst) == len(next(iter(dict_of_lists.values()))) for lst in dict_of_lists.values()):\n",
    "        raise ValueError(\"All lists in the dictionary must have the same length\")\n",
    "\n",
    "    # Get the length of the items in any of the lists\n",
    "    length = len(next(iter(dict_of_lists.values())))\n",
    "    \n",
    "    # Create a list of dictionaries, one for each index in the lists\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        # Create a dictionary for the current index 'i' across all lists\n",
    "        new_dict = {key: dict_of_lists[key][i] for key in dict_of_lists}\n",
    "        result.append(new_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sub_shift_spans(text, ents = [], mappings = []):\n",
    "    for mapping in mappings:\n",
    "        adjustment = 0\n",
    "        pattern = re.compile(mapping['pattern'])\n",
    "        for match in re.finditer(pattern, text):\n",
    "            match_index = match.start() + adjustment\n",
    "            match_contents = match.group()\n",
    "            if all(mapping['check'](char) for char in match_contents):\n",
    "                subbed_text = mapping['target'].replace('placeholder', match_contents)\n",
    "            else:\n",
    "                subbed_text = mapping['target']\n",
    "            len_diff = len(subbed_text) - len(match_contents)\n",
    "            text = text[:match_index] + subbed_text + text[match_index + len(match_contents):]\n",
    "            if ents:\n",
    "                if isinstance(ents, list):\n",
    "                    for ent in ents:\n",
    "                        if ent['start'] <= match_index and ent['end'] > match_index:\n",
    "                            ent['end'] += len_diff\n",
    "                        if ent['start'] > match_index:\n",
    "                            ent['start'] += len_diff\n",
    "                            ent['end'] += len_diff\n",
    "                elif isinstance(ents, dict):\n",
    "                    if ents['value']['start'] <= match_index and ents['value']['end'] > match_index:\n",
    "                        ents['value']['end'] += len_diff\n",
    "                    if ents['value']['start'] > match_index:\n",
    "                        ents['value']['start'] += len_diff\n",
    "                        ents['value']['end'] += len_diff\n",
    "\n",
    "            adjustment += len_diff\n",
    "\n",
    "    return text, ents\n",
    "\n",
    "def get_entities_from_sample(sample, field = 'annotations', langs = ['en'], sort = False):\n",
    "    entities = []\n",
    "    for lang in langs:\n",
    "        entities += [ent for ent in sample[field][0]['result'] if ent['type'] == 'labels' and ent[f'from_name'] == f'label_{lang}']\n",
    "    if sort:\n",
    "        entities = sorted(entities, key = lambda ent: ent['value']['start'])\n",
    "    return entities\n",
    "'''\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    # if not any([l for l in samples['annotations']]):\n",
    "        \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                if annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        if first:\n",
    "                            labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                            first = False\n",
    "                        elif not first:\n",
    "                            labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0]})\n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        # if the training sample has no tags that we need, we just produce a 0s list\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # if the training sample has tags we need, we first exclude the label lists whose tags don't match\n",
    "        # and then we merge the label lists that have tags that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    return samples_new\n",
    "'''\n",
    "\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                # Check if 'labels' is non-empty\n",
    "                if annotation['value']['labels'] and annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        # Ensure the 'labels' list is not empty\n",
    "                        if isinstance(ents['value']['labels'], list) and ents['value']['labels']:\n",
    "                            if first:\n",
    "                                labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                                first = False\n",
    "                            else:\n",
    "                                labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                        else:\n",
    "                            # If 'labels' is empty, append 'O' or any fallback label\n",
    "                            labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0] if 'labels' in annotation['value'] and annotation['value']['labels'] else 'no_tag'})\n",
    "        \n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        \n",
    "        # If the training sample has no tags we need, produce a list of 0s\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # Otherwise, merge label lists that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "            \n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "        \n",
    "    return samples_new\n",
    "\n",
    "def make_balanced_df(df):\n",
    "    # get rows with annotations\n",
    "    df_pos = df[df['annotations'].apply(lambda x: len(x[0]['result']) > 0)]\n",
    "    # get the same number of rows without any annotations\n",
    "    df_neg = df[df['annotations'].apply(lambda x: x[0]['result'] == [])].sample(len(df_pos))\n",
    "    balanced_df = pd.concat([df_pos, df_neg])\n",
    "    return balanced_df\n",
    "\n",
    "def make_binary_balanced_df(df, target_tag='', labels_model=[]):\n",
    "    df_list = df.to_dict(orient='records')\n",
    "    df_list_binary = span_to_words_annotation(dict_of_lists(df_list), target_tag=target_tag, mappings=regex_tokenizer_mappings, labels_model=labels_model)\n",
    "    df_binary = pd.DataFrame(df_list_binary)\n",
    "    df_binary_pos = df_binary[df_binary['tag'] == target_tag]\n",
    "    df_binary_neg = df_binary[df_binary['tag'] != target_tag].sample(len(df_binary_pos), replace=True)  # Over-sampling\n",
    "    df_binary_subsampled = pd.concat([df_binary_pos, df_binary_neg])\n",
    "    return df_binary_subsampled\n",
    "\n",
    "regex_tokenizer_mappings = [\n",
    "    {'pattern': r'(?<!\\s)([^\\w\\s])|([^\\w\\s])(?!\\s)',\n",
    "    'target': ' placeholder ',\n",
    "    'check': lambda x: unicodedata.category(x).startswith('P'),\n",
    "    },\n",
    "    {'pattern': r'\\s+',\n",
    "     'target': ' ',\n",
    "     'check': lambda x: False if re.match('\\s+', x) is None else True,\n",
    "     },\n",
    "    ]\n",
    "\n",
    "def compute_metrics_wrapper(label_list, pt, model_name_simple, date_time, threshold=0):\n",
    "    def compute_metrics(eval_preds):\n",
    "        nonlocal label_list\n",
    "        nonlocal pt\n",
    "        nonlocal threshold\n",
    "        logits, labels = eval_preds\n",
    "        predictions = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        predictions = np.where(predictions >= threshold, predictions, 0)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "        # Extract the true predictions and labels from the sequences\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        # Compute sequence-level evaluation metrics\n",
    "        results = classification_report(true_predictions, true_labels, output_dict=True)\n",
    "\n",
    "        # Flatten the lists to calculate micro F1-score and supports\n",
    "        flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "        flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Calculate micro F1-score using sklearn\n",
    "        micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "        # Prepare the results dictionary\n",
    "        flat_results = {'micro_f1': float(micro_f1)}\n",
    "        \n",
    "        # Add detailed metrics for each label to the results dictionary\n",
    "        for label, metrics in results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    flat_results[f'{label}_{metric}'] = float(value)\n",
    "\n",
    "        # Compute support for each label using Counter\n",
    "        label_support = Counter(flat_true_labels)\n",
    "        for label, count in label_support.items():\n",
    "            flat_results[f'{label}_support'] = count\n",
    "        \n",
    "        models_dir = '/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results'\n",
    "        model_save_name = f'{model_name_simple}_{tt[0]}_target={tt[1]}_aug_SUBSAMPLED_{date_time}'\n",
    "        model_save_dir = os.path.join('/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results/2024-10-24-18-12-14_aug_cw_ts0.9', model_save_name) #os.path.join(models_dir, date_time+'_aug_cw_ts0.9', model_save_name)\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            os.makedirs(model_save_dir)\n",
    "\n",
    "        with open(os.path.join(model_save_dir, 'results.json'), 'w', encoding='utf8') as f:\n",
    "            json.dump(flat_results, f, ensure_ascii = False)\n",
    "\n",
    "        return flat_results\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "data_gold = '/home/lgiordano/LUCA/checkthat_GITHUB/data/formatted/train_sentences.json'\n",
    "with open(data_gold, 'r', encoding='utf8') as f:\n",
    "    dataset_gold = json.load(f)\n",
    "\n",
    "data_path_dict = {\n",
    "'sl': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/sl/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-slv_Latn_tok_regex_en-sl/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-slv_Latn_tok_regex_en-sl_mdeberta-v3-base_mdeberta_xlwa_en-sl_ME3_2024-05-04-12-12-14_ls.json',\n",
    "'ru': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/ru/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-rus_Cyrl_tok_regex_en-ru/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-rus_Cyrl_tok_regex_en-ru_mdeberta-v3-base_mdeberta_xlwa_en-ru_ME3_2024-05-04-12-09-20_ls.json',\n",
    "'pt': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/pt/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-por_Latn_tok_regex_en-pt/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-por_Latn_tok_regex_en-pt_mdeberta-v3-base_mdeberta_xlwa_en-pt_ME3_2024-05-04-12-07-45_ls.json',\n",
    "'it': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/it/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-ita_Latn_tok_regex_en-it/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-ita_Latn_tok_regex_en-it_mdeberta-v3-base_mdeberta_xlwa_en-it_ME3_2024-05-04-12-05-00_ls.json',\n",
    "'es': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/es/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-spa_Latn_tok_regex_en-es/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-spa_Latn_tok_regex_en-es_mdeberta-v3-base_mdeberta_xlwa_en-es_ME3_2024-05-04-12-01-43_ls.json',\n",
    "'bg': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/bg/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-bul_Cyrl_tok_regex_en-bg/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-bul_Cyrl_tok_regex_en-bg_mdeberta-v3-base_mdeberta_xlwa_en-bg_ME3_2024-05-04-11-58-52_ls.json',\n",
    "#'ar': '/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/araieval24_all_bin_formatted.json'\n",
    "}\n",
    "\n",
    "dataset_aug = []\n",
    "for key in data_path_dict:\n",
    "    with open(data_path_dict[key], 'r', encoding='utf8') as f:\n",
    "        dataset_aug_buffer = json.load(f)\n",
    "\n",
    "        for sample in dataset_aug_buffer:\n",
    "            sample['annotations'][0]['result'] = get_entities_from_sample(sample, langs=[key], sort = True)\n",
    "            if 'text_en' in sample['data']:\n",
    "                del sample['data']['text_en']\n",
    "            if f'text_{key}' in sample['data']:\n",
    "                sample['data']['text'] = sample['data'][f'text_{key}']\n",
    "                del sample['data'][f'text_{key}']\n",
    "            sample['data']['lang'] = key\n",
    "            if 'labels' in sample['data']:\n",
    "                sample['data']['label'] = sample['data'].pop('labels')\n",
    "        dataset_aug += dataset_aug_buffer\n",
    "\n",
    "dataset_aug += [sample for sample in json.load(open('/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/araieval24_all_bin_formatted.json')) if sample['data'].get('type') != 'tweet'] #filter out tweets from ar\n",
    "semeval_24 = json.load(open('/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/semeval24_all_bin_formatted.json', encoding='utf-8'))\n",
    "dataset_aug += semeval_24\n",
    "\n",
    "df_gold = pd.DataFrame(dataset_gold)\n",
    "balanced_df_gold = make_balanced_df(df_gold)\n",
    "\n",
    "df_aug = pd.DataFrame(dataset_aug)\n",
    "balanced_df_aug = make_balanced_df(df_aug)\n",
    "\n",
    "target_tags = [\"Appeal_to_Authority\", \"Appeal_to_Popularity\",\"Appeal_to_Values\",\"Appeal_to_Fear-Prejudice\",\"Flag_Waving\",\"Causal_Oversimplification\",\n",
    "               \"False_Dilemma-No_Choice\",\"Consequential_Oversimplification\",\"Straw_Man\",\"Red_Herring\",\"Whataboutism\",\"Slogans\",\"Appeal_to_Time\",\n",
    "               \"Conversation_Killer\",\"Loaded_Language\",\"Repetition\",\"Exaggeration-Minimisation\",\"Obfuscation-Vagueness-Confusion\",\"Name_Calling-Labeling\",\n",
    "               \"Doubt\",\"Guilt_by_Association\",\"Appeal_to_Hypocrisy\",\"Questioning_the_Reputation\"]\n",
    "target_tags = [(i, el.strip()) for i, el in enumerate(target_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 14 of 23 for (14, 'Loaded_Language') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/45107 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 45107/45107 [00:16<00:00, 2725.26 examples/s]\n",
      "Map: 100%|██████████| 3077/3077 [00:00<00:00, 4318.12 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56390' max='56390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56390/56390 4:42:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Loaded Language Precision</th>\n",
       "      <th>Loaded Language Recall</th>\n",
       "      <th>Loaded Language F1-score</th>\n",
       "      <th>Loaded Language Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-loaded Language Support</th>\n",
       "      <th>I-loaded Language Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.556500</td>\n",
       "      <td>0.530935</td>\n",
       "      <td>0.893071</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.538278</td>\n",
       "      <td>0.905323</td>\n",
       "      <td>0.063534</td>\n",
       "      <td>0.187661</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>0.063534</td>\n",
       "      <td>0.187661</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>0.063534</td>\n",
       "      <td>0.187661</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>0.063534</td>\n",
       "      <td>0.187661</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>389.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.732323</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.226284</td>\n",
       "      <td>0.321782</td>\n",
       "      <td>0.265713</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>0.226284</td>\n",
       "      <td>0.321782</td>\n",
       "      <td>0.265713</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>0.226284</td>\n",
       "      <td>0.321782</td>\n",
       "      <td>0.265713</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>0.226284</td>\n",
       "      <td>0.321782</td>\n",
       "      <td>0.265713</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.675487</td>\n",
       "      <td>0.921720</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>0.348519</td>\n",
       "      <td>0.301924</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>0.348519</td>\n",
       "      <td>0.301924</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>0.348519</td>\n",
       "      <td>0.301924</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>0.266319</td>\n",
       "      <td>0.348519</td>\n",
       "      <td>0.301924</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.720338</td>\n",
       "      <td>0.922907</td>\n",
       "      <td>0.308094</td>\n",
       "      <td>0.392897</td>\n",
       "      <td>0.345366</td>\n",
       "      <td>901.000000</td>\n",
       "      <td>0.308094</td>\n",
       "      <td>0.392897</td>\n",
       "      <td>0.345366</td>\n",
       "      <td>901.000000</td>\n",
       "      <td>0.308094</td>\n",
       "      <td>0.392897</td>\n",
       "      <td>0.345366</td>\n",
       "      <td>901.000000</td>\n",
       "      <td>0.308094</td>\n",
       "      <td>0.392897</td>\n",
       "      <td>0.345366</td>\n",
       "      <td>901.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.857235</td>\n",
       "      <td>0.922404</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.423267</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.423267</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.423267</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.423267</td>\n",
       "      <td>0.349515</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.964367</td>\n",
       "      <td>0.925824</td>\n",
       "      <td>0.360313</td>\n",
       "      <td>0.427245</td>\n",
       "      <td>0.390935</td>\n",
       "      <td>969.000000</td>\n",
       "      <td>0.360313</td>\n",
       "      <td>0.427245</td>\n",
       "      <td>0.390935</td>\n",
       "      <td>969.000000</td>\n",
       "      <td>0.360313</td>\n",
       "      <td>0.427245</td>\n",
       "      <td>0.390935</td>\n",
       "      <td>969.000000</td>\n",
       "      <td>0.360313</td>\n",
       "      <td>0.427245</td>\n",
       "      <td>0.390935</td>\n",
       "      <td>969.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>1.060461</td>\n",
       "      <td>0.925200</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.399429</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.399429</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.399429</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>0.365535</td>\n",
       "      <td>0.440252</td>\n",
       "      <td>0.399429</td>\n",
       "      <td>954.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>1.131311</td>\n",
       "      <td>0.925985</td>\n",
       "      <td>0.381201</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>0.413013</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>0.381201</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>0.413013</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>0.381201</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>0.413013</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>0.381201</td>\n",
       "      <td>0.450617</td>\n",
       "      <td>0.413013</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>1.115828</td>\n",
       "      <td>0.926186</td>\n",
       "      <td>0.384682</td>\n",
       "      <td>0.440239</td>\n",
       "      <td>0.410590</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>0.384682</td>\n",
       "      <td>0.440239</td>\n",
       "      <td>0.410590</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>0.384682</td>\n",
       "      <td>0.440239</td>\n",
       "      <td>0.410590</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>0.384682</td>\n",
       "      <td>0.440239</td>\n",
       "      <td>0.410590</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>44083</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 15 of 23 for (15, 'Repetition') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/9729 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 9729/9729 [00:03<00:00, 3190.40 examples/s]\n",
      "Map: 100%|██████████| 445/445 [00:00<00:00, 4731.93 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8519' max='12170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8519/12170 37:10 < 15:56, 3.82 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Repetition Precision</th>\n",
       "      <th>Repetition Recall</th>\n",
       "      <th>Repetition F1-score</th>\n",
       "      <th>Repetition Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-repetition Support</th>\n",
       "      <th>I-repetition Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.591874</td>\n",
       "      <td>0.899985</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.143541</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.143541</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.143541</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.143541</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.598640</td>\n",
       "      <td>0.926491</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.739841</td>\n",
       "      <td>0.937895</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.612546</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152900</td>\n",
       "      <td>0.799643</td>\n",
       "      <td>0.937587</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>0.613793</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.732727</td>\n",
       "      <td>0.937433</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.654930</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.654930</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.654930</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.654930</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.719360</td>\n",
       "      <td>0.938820</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.907631</td>\n",
       "      <td>0.938203</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>5744</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 16 of 23 for (16, 'Exaggeration-Minimisation') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/9673 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 9673/9673 [00:02<00:00, 4477.27 examples/s]\n",
      "Map: 100%|██████████| 685/685 [00:00<00:00, 5670.04 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9680' max='12100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9680/12100 25:30 < 06:22, 6.32 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Exaggeration-minimisation Precision</th>\n",
       "      <th>Exaggeration-minimisation Recall</th>\n",
       "      <th>Exaggeration-minimisation F1-score</th>\n",
       "      <th>Exaggeration-minimisation Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-exaggeration-minimisation Support</th>\n",
       "      <th>I-exaggeration-minimisation Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.865352</td>\n",
       "      <td>0.834037</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>1.076434</td>\n",
       "      <td>0.857437</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.314800</td>\n",
       "      <td>1.354627</td>\n",
       "      <td>0.852454</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.273292</td>\n",
       "      <td>0.239782</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.273292</td>\n",
       "      <td>0.239782</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.273292</td>\n",
       "      <td>0.239782</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.273292</td>\n",
       "      <td>0.239782</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>1.561338</td>\n",
       "      <td>0.858954</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.350282</td>\n",
       "      <td>0.323760</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.350282</td>\n",
       "      <td>0.323760</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.350282</td>\n",
       "      <td>0.323760</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.350282</td>\n",
       "      <td>0.323760</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>1.744217</td>\n",
       "      <td>0.872928</td>\n",
       "      <td>0.344660</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.383784</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.344660</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.383784</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.344660</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.383784</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.344660</td>\n",
       "      <td>0.432927</td>\n",
       "      <td>0.383784</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>1.652377</td>\n",
       "      <td>0.868595</td>\n",
       "      <td>0.388350</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.398010</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.388350</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.398010</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.388350</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.398010</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.388350</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.398010</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>1.744071</td>\n",
       "      <td>0.864912</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>0.374429</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>0.374429</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>0.374429</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.353448</td>\n",
       "      <td>0.374429</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>1.881642</td>\n",
       "      <td>0.866212</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.392947</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.392947</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.392947</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.408377</td>\n",
       "      <td>0.392947</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>7168</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 17 of 23 for (17, 'Obfuscation-Vagueness-Confusion') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1946 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1946/1946 [00:00<00:00, 3206.39 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 4372.58 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='732' max='2440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 732/2440 01:43 < 04:02, 7.03 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Obfuscation-vagueness-confusion Precision</th>\n",
       "      <th>Obfuscation-vagueness-confusion Recall</th>\n",
       "      <th>Obfuscation-vagueness-confusion F1-score</th>\n",
       "      <th>Obfuscation-vagueness-confusion Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-obfuscation-vagueness-confusion Support</th>\n",
       "      <th>I-obfuscation-vagueness-confusion Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.065600</td>\n",
       "      <td>1.276465</td>\n",
       "      <td>0.585131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1275</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>1.343929</td>\n",
       "      <td>0.596145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1275</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>1.503299</td>\n",
       "      <td>0.587884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1275</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 18 of 23 for (18, 'Name_Calling-Labeling') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/22579 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 22579/22579 [00:08<00:00, 2570.04 examples/s]\n",
      "Map: 100%|██████████| 2205/2205 [00:00<00:00, 4808.58 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28230' max='28230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28230/28230 2:38:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Name Calling-labeling Precision</th>\n",
       "      <th>Name Calling-labeling Recall</th>\n",
       "      <th>Name Calling-labeling F1-score</th>\n",
       "      <th>Name Calling-labeling Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-name Calling-labeling Support</th>\n",
       "      <th>I-name Calling-labeling Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.349176</td>\n",
       "      <td>0.923163</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>0.017157</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.361951</td>\n",
       "      <td>0.938796</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.319355</td>\n",
       "      <td>0.275766</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.319355</td>\n",
       "      <td>0.275766</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.319355</td>\n",
       "      <td>0.275766</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.319355</td>\n",
       "      <td>0.275766</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.399479</td>\n",
       "      <td>0.941149</td>\n",
       "      <td>0.280637</td>\n",
       "      <td>0.360630</td>\n",
       "      <td>0.315644</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>0.280637</td>\n",
       "      <td>0.360630</td>\n",
       "      <td>0.315644</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>0.280637</td>\n",
       "      <td>0.360630</td>\n",
       "      <td>0.315644</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>0.280637</td>\n",
       "      <td>0.360630</td>\n",
       "      <td>0.315644</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.482429</td>\n",
       "      <td>0.944245</td>\n",
       "      <td>0.325980</td>\n",
       "      <td>0.427653</td>\n",
       "      <td>0.369958</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>0.325980</td>\n",
       "      <td>0.427653</td>\n",
       "      <td>0.369958</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>0.325980</td>\n",
       "      <td>0.427653</td>\n",
       "      <td>0.369958</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>0.325980</td>\n",
       "      <td>0.427653</td>\n",
       "      <td>0.369958</td>\n",
       "      <td>622.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.477545</td>\n",
       "      <td>0.948393</td>\n",
       "      <td>0.405637</td>\n",
       "      <td>0.459085</td>\n",
       "      <td>0.430709</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>0.405637</td>\n",
       "      <td>0.459085</td>\n",
       "      <td>0.430709</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>0.405637</td>\n",
       "      <td>0.459085</td>\n",
       "      <td>0.430709</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>0.405637</td>\n",
       "      <td>0.459085</td>\n",
       "      <td>0.430709</td>\n",
       "      <td>721.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.523380</td>\n",
       "      <td>0.947681</td>\n",
       "      <td>0.428922</td>\n",
       "      <td>0.486787</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>0.428922</td>\n",
       "      <td>0.486787</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>0.428922</td>\n",
       "      <td>0.486787</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>0.428922</td>\n",
       "      <td>0.486787</td>\n",
       "      <td>0.456026</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.944121</td>\n",
       "      <td>0.351716</td>\n",
       "      <td>0.548757</td>\n",
       "      <td>0.428678</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>0.351716</td>\n",
       "      <td>0.548757</td>\n",
       "      <td>0.428678</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>0.351716</td>\n",
       "      <td>0.548757</td>\n",
       "      <td>0.428678</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>0.351716</td>\n",
       "      <td>0.548757</td>\n",
       "      <td>0.428678</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.609052</td>\n",
       "      <td>0.951582</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.505898</td>\n",
       "      <td>0.488917</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.505898</td>\n",
       "      <td>0.488917</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.505898</td>\n",
       "      <td>0.488917</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.505898</td>\n",
       "      <td>0.488917</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.631548</td>\n",
       "      <td>0.950282</td>\n",
       "      <td>0.464461</td>\n",
       "      <td>0.529330</td>\n",
       "      <td>0.494778</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.464461</td>\n",
       "      <td>0.529330</td>\n",
       "      <td>0.494778</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.464461</td>\n",
       "      <td>0.529330</td>\n",
       "      <td>0.494778</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.464461</td>\n",
       "      <td>0.529330</td>\n",
       "      <td>0.494778</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.707421</td>\n",
       "      <td>0.951953</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.503209</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.503209</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.503209</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.503209</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>29269</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 19 of 23 for (19, 'Doubt') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/14449 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 14449/14449 [00:04<00:00, 3515.17 examples/s]\n",
      "Map: 100%|██████████| 1731/1731 [00:00<00:00, 4814.21 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16263' max='18070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16263/18070 59:41 < 06:37, 4.54 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Doubt Precision</th>\n",
       "      <th>Doubt Recall</th>\n",
       "      <th>Doubt F1-score</th>\n",
       "      <th>Doubt Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-doubt Support</th>\n",
       "      <th>I-doubt Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>1.062984</td>\n",
       "      <td>0.646880</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>1.416375</td>\n",
       "      <td>0.669830</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.560600</td>\n",
       "      <td>1.674476</td>\n",
       "      <td>0.745548</td>\n",
       "      <td>0.168524</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>0.167822</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>0.168524</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>0.167822</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>0.168524</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>0.167822</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>0.168524</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>0.167822</td>\n",
       "      <td>724.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.391700</td>\n",
       "      <td>1.598650</td>\n",
       "      <td>0.783890</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.228443</td>\n",
       "      <td>0.253259</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.228443</td>\n",
       "      <td>0.253259</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.228443</td>\n",
       "      <td>0.253259</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>0.228443</td>\n",
       "      <td>0.253259</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>2.285712</td>\n",
       "      <td>0.760077</td>\n",
       "      <td>0.250696</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.253879</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>0.250696</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.253879</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>0.250696</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.253879</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>0.250696</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.253879</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>2.316992</td>\n",
       "      <td>0.754003</td>\n",
       "      <td>0.240947</td>\n",
       "      <td>0.252924</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>0.240947</td>\n",
       "      <td>0.252924</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>0.240947</td>\n",
       "      <td>0.252924</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>0.240947</td>\n",
       "      <td>0.252924</td>\n",
       "      <td>0.246790</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>2.330703</td>\n",
       "      <td>0.768567</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.268507</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.268507</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.268507</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.268507</td>\n",
       "      <td>0.282508</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>2.530013</td>\n",
       "      <td>0.772156</td>\n",
       "      <td>0.285515</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.274799</td>\n",
       "      <td>774.000000</td>\n",
       "      <td>0.285515</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.274799</td>\n",
       "      <td>774.000000</td>\n",
       "      <td>0.285515</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.274799</td>\n",
       "      <td>774.000000</td>\n",
       "      <td>0.285515</td>\n",
       "      <td>0.264858</td>\n",
       "      <td>0.274799</td>\n",
       "      <td>774.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>2.758104</td>\n",
       "      <td>0.775400</td>\n",
       "      <td>0.279944</td>\n",
       "      <td>0.260363</td>\n",
       "      <td>0.269799</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>0.279944</td>\n",
       "      <td>0.260363</td>\n",
       "      <td>0.269799</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>0.279944</td>\n",
       "      <td>0.260363</td>\n",
       "      <td>0.269799</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>0.279944</td>\n",
       "      <td>0.260363</td>\n",
       "      <td>0.269799</td>\n",
       "      <td>772.000000</td>\n",
       "      <td>16549</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 20 of 23 for (20, 'Guilt_by_Association') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1644 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1644/1644 [00:00<00:00, 4569.52 examples/s]\n",
      "Map: 100%|██████████| 258/258 [00:00<00:00, 5980.27 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1442' max='2060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1442/2060 03:42 < 01:35, 6.46 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Guilt By Association Precision</th>\n",
       "      <th>Guilt By Association Recall</th>\n",
       "      <th>Guilt By Association F1-score</th>\n",
       "      <th>Guilt By Association Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-guilt By Association Support</th>\n",
       "      <th>I-guilt By Association Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.247600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.701105</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.660400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.764917</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.797514</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.164103</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.164103</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.164103</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.164103</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.737845</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.790331</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.244186</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.793923</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.803039</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.216495</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>2265</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 21 of 23 for (21, 'Appeal_to_Hypocrisy') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1942 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1942/1942 [00:00<00:00, 4603.01 examples/s]\n",
      "Map: 100%|██████████| 340/340 [00:00<00:00, 3730.19 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2430' max='2430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2430/2430 06:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Hypocrisy Precision</th>\n",
       "      <th>Appeal To Hypocrisy Recall</th>\n",
       "      <th>Appeal To Hypocrisy F1-score</th>\n",
       "      <th>Appeal To Hypocrisy Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-appeal To Hypocrisy Support</th>\n",
       "      <th>I-appeal To Hypocrisy Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.348800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.498795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.847800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.732802</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.021858</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.694790</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.742073</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.103343</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.103343</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.103343</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.103343</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.739292</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.128079</td>\n",
       "      <td>0.153392</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.128079</td>\n",
       "      <td>0.153392</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.128079</td>\n",
       "      <td>0.153392</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.128079</td>\n",
       "      <td>0.153392</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.766920</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.196319</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.750603</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.231975</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.231975</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.231975</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.231975</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.771185</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.181319</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.181319</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.181319</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.181319</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.776006</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.263323</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.263323</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.263323</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.263323</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.775821</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "      <td>2378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 22 of 23 for (22, 'Questioning_the_Reputation') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/4560 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 4560/4560 [00:00<00:00, 4571.15 examples/s]\n",
      "Map: 100%|██████████| 874/874 [00:00<00:00, 6220.54 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5700' max='5700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5700/5700 15:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Questioning The Reputation Precision</th>\n",
       "      <th>Questioning The Reputation Recall</th>\n",
       "      <th>Questioning The Reputation F1-score</th>\n",
       "      <th>Questioning The Reputation Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-questioning The Reputation Support</th>\n",
       "      <th>I-questioning The Reputation Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.186100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.584233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.688719</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.743774</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.783603</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.149792</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.149792</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.149792</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.149792</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.773702</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>456.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>456.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>456.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.192802</td>\n",
       "      <td>456.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.775728</td>\n",
       "      <td>0.204969</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.183844</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>0.204969</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.183844</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>0.204969</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.183844</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>0.204969</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.183844</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.774077</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.180288</td>\n",
       "      <td>0.203252</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.771002</td>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.185930</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.773927</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.182039</td>\n",
       "      <td>0.204360</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.182039</td>\n",
       "      <td>0.204360</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.182039</td>\n",
       "      <td>0.204360</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0.232919</td>\n",
       "      <td>0.182039</td>\n",
       "      <td>0.204360</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.778128</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>0.267081</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>0.234332</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>7789</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "shift = 14\n",
    "for i, tt in enumerate(target_tags):\n",
    "    if i < shift:\n",
    "        continue\n",
    "    print(f'Training model no. {i} of {len(target_tags)} for {tt} persuasion technique...')\n",
    "    labels_model = LabelSet(labels=[tt[1]])\n",
    "    \n",
    "    token_columns = ['id', 'ner_tags', 'tokens', \n",
    "                     #'lang'\n",
    "                     ]\n",
    "\n",
    "    df_binary_subsampled_gold = make_binary_balanced_df(balanced_df_gold, target_tag=tt[1], labels_model=labels_model)\n",
    "    binary_dataset_gold = Dataset.from_pandas(df_binary_subsampled_gold[token_columns])\n",
    "\n",
    "    df_binary_subsampled_aug = make_binary_balanced_df(balanced_df_aug, target_tag=tt[1], labels_model=labels_model)\n",
    "    binary_dataset_aug = Dataset.from_pandas(df_binary_subsampled_aug[token_columns])\n",
    "    \n",
    "    split_ratio = 0.2\n",
    "    split_seed = 42\n",
    "    datadict = binary_dataset_gold.train_test_split(split_ratio, seed=split_seed)\n",
    "\n",
    "    #model_name = 'bert-base-multilingual-cased'\n",
    "    #model_name = 'xlm-roberta-base'\n",
    "    model_name = 'microsoft/mdeberta-v3-base'\n",
    "    #model_name = 'FacebookAI/xlm-roberta-large'\n",
    "    model_name_simple = model_name.split('/')[-1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 16\n",
    "    datadict['train'] = concatenate_datasets([datadict['train'], binary_dataset_aug]) # this is where we merge english gold data with aug data\n",
    "    datadict = datadict.map(lambda x: tokenize_token_classification(x, tokenizer), batched=True, batch_size=None)\n",
    "\n",
    "    columns = [\n",
    "                'input_ids',\n",
    "                'token_type_ids',\n",
    "                'attention_mask',\n",
    "                'labels'\n",
    "                ]\n",
    "\n",
    "    datadict.set_format('torch', columns = columns)\n",
    "\n",
    "    train_data = datadict['train']\n",
    "    val_data = datadict['test']\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding='longest')\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                num_labels=len(labels_model.ids_to_label.values()),\n",
    "                                                                label2id=labels_model.labels_to_id,\n",
    "                                                                id2label=labels_model.ids_to_label,\n",
    "                                                                )\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=f'/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results/2024-10-24-18-12-14_aug_cw_ts0.9/mdeberta-v3-base-NEW_aug_{i}_{tt[1]}', #f'/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results/{date_time}_aug_cw_ts0.9/mdeberta-v3-base-NEW_aug_{i}_{tt[1]}',\n",
    "                                  save_total_limit=2,\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_only_model=True,\n",
    "                                  metric_for_best_model='eval_macro avg_f1-score',\n",
    "                                  logging_strategy='epoch',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=7e-5,\n",
    "                                  optim='adamw_torch',\n",
    "                                  num_train_epochs=10)\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    ###CustomTrainer per class weighting, threshold=0.9 per threshold\n",
    "    trainer = CustomTrainer(model,\n",
    "                      training_args,\n",
    "                      train_dataset=train_data,\n",
    "                      eval_dataset=val_data,\n",
    "                      data_collator=data_collator,\n",
    "                      tokenizer=tokenizer,\n",
    "                      callbacks=[early_stopping],\n",
    "                      compute_metrics=compute_metrics_wrapper(\n",
    "                          label_list=[i for i in labels_model.ids_to_label.values()],\n",
    "                          pt=tt[1],\n",
    "                          model_name_simple=model_name_simple,\n",
    "                          date_time=date_time,\n",
    "                          threshold=0.9\n",
    "                          ),\n",
    "                      )\n",
    "    \n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
