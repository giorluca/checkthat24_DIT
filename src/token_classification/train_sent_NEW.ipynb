{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from tokenizers import Encoding\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from seqeval.metrics import classification_report\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_annotations_bio(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "            print('char_ix = ', char_ix)\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"B\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"I\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremental ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BI\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bio(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "\n",
    "\n",
    "def tokenize_token_classification(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = [tokenized_inputs.token_to_word(i, j) for j in range(len(tokenized_inputs['input_ids'][i]))]  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "def dict_of_lists(lst_of_dicts):\n",
    "    result = defaultdict(list)\n",
    "    for d in lst_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            result[key].append(value)\n",
    "    return dict(result)\n",
    "\n",
    "def list_of_dicts(dict_of_lists):\n",
    "    # First, we need to check if all lists are of the same length to ensure correct transformation\n",
    "    if not all(len(lst) == len(next(iter(dict_of_lists.values()))) for lst in dict_of_lists.values()):\n",
    "        raise ValueError(\"All lists in the dictionary must have the same length\")\n",
    "\n",
    "    # Get the length of the items in any of the lists\n",
    "    length = len(next(iter(dict_of_lists.values())))\n",
    "    \n",
    "    # Create a list of dictionaries, one for each index in the lists\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        # Create a dictionary for the current index 'i' across all lists\n",
    "        new_dict = {key: dict_of_lists[key][i] for key in dict_of_lists}\n",
    "        result.append(new_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sub_shift_spans(text, ents = [], mappings = []):\n",
    "    for mapping in mappings:\n",
    "        adjustment = 0\n",
    "        pattern = re.compile(mapping['pattern'])\n",
    "        for match in re.finditer(pattern, text):\n",
    "            match_index = match.start() + adjustment\n",
    "            match_contents = match.group()\n",
    "            if all(mapping['check'](char) for char in match_contents):\n",
    "                subbed_text = mapping['target'].replace('placeholder', match_contents)\n",
    "            else:\n",
    "                subbed_text = mapping['target']\n",
    "            len_diff = len(subbed_text) - len(match_contents)\n",
    "            text = text[:match_index] + subbed_text + text[match_index + len(match_contents):]\n",
    "            if ents:\n",
    "                if isinstance(ents, list):\n",
    "                    for ent in ents:\n",
    "                        if ent['start'] <= match_index and ent['end'] > match_index:\n",
    "                            ent['end'] += len_diff\n",
    "                        if ent['start'] > match_index:\n",
    "                            ent['start'] += len_diff\n",
    "                            ent['end'] += len_diff\n",
    "                elif isinstance(ents, dict):\n",
    "                    if ents['value']['start'] <= match_index and ents['value']['end'] > match_index:\n",
    "                        ents['value']['end'] += len_diff\n",
    "                    if ents['value']['start'] > match_index:\n",
    "                        ents['value']['start'] += len_diff\n",
    "                        ents['value']['end'] += len_diff\n",
    "\n",
    "            adjustment += len_diff\n",
    "\n",
    "    return text, ents\n",
    "\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    # if not any([l for l in samples['annotations']]):\n",
    "        \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                if annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        if first:\n",
    "                            labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                            first = False\n",
    "                        elif not first:\n",
    "                            labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0]})\n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        # if the training sample has no tags that we need, we just produce a 0s list\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # if the training sample has tags we need, we first exclude the label lists whose tags don't match\n",
    "        # and then we merge the label lists that have tags that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    return samples_new\n",
    "\n",
    "regex_tokenizer_mappings = [\n",
    "    {'pattern': r'(?<!\\s)([^\\w\\s])|([^\\w\\s])(?!\\s)',\n",
    "    'target': ' placeholder ',\n",
    "    'check': lambda x: unicodedata.category(x).startswith('P'),\n",
    "    },\n",
    "    {'pattern': r'\\s+',\n",
    "     'target': ' ',\n",
    "     'check': lambda x: False if re.match('\\s+', x) is None else True,\n",
    "     },\n",
    "    ]\n",
    "\n",
    "def compute_metrics_wrapper(label_list, pt, model_name_simple, date_time):\n",
    "    def compute_metrics(eval_preds):\n",
    "        nonlocal label_list\n",
    "        nonlocal pt\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "        # Extract the true predictions and labels from the sequences\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        # Compute sequence-level evaluation metrics\n",
    "        results = classification_report(true_predictions, true_labels, output_dict=True)\n",
    "\n",
    "        # Flatten the lists to calculate micro F1-score and supports\n",
    "        flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "        flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Calculate micro F1-score using sklearn\n",
    "        micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "        # Prepare the results dictionary\n",
    "        flat_results = {'micro_f1': float(micro_f1)}\n",
    "\n",
    "        # Add detailed metrics for each label to the results dictionary\n",
    "        for label, metrics in results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    flat_results[f'{label}_{metric}'] = float(value)\n",
    "\n",
    "        # Compute support for each label using Counter\n",
    "        label_support = Counter(flat_true_labels)\n",
    "        for label, count in label_support.items():\n",
    "            flat_results[f'{label}_support'] = count\n",
    "        \n",
    "        models_dir = '/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results'\n",
    "        model_save_name = f'{model_name_simple}_{tt[0]}_target={tt[1]}_SUBSAMPLED_{date_time}'\n",
    "        model_save_dir = os.path.join(models_dir, date_time+'_no_aug_no_cw_ts0', model_save_name)\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            os.makedirs(model_save_dir)\n",
    "\n",
    "        with open(os.path.join(model_save_dir, 'results.json'), 'w', encoding='utf8') as f:\n",
    "            json.dump(flat_results, f, ensure_ascii = False)\n",
    "\n",
    "        return flat_results\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "train_data_path = '/home/lgiordano/LUCA/checkthat_GITHUB/data/formatted/train_sentences.json'\n",
    "with open(train_data_path, 'r', encoding='utf8') as f:\n",
    "    dataset_raw = json.load(f)\n",
    "\n",
    "df_raw = pd.DataFrame(dataset_raw)  \n",
    "\n",
    "df_pos = df_raw[df_raw['annotations'].apply(lambda x: len(x[0]['result']) > 0)]\n",
    "df_neg = df_raw[df_raw['annotations'].apply(lambda x: x[0]['result'] == [])].sample(len(df_pos))\n",
    "df = pd.concat([df_pos,df_neg])\n",
    "\n",
    "target_tags = [\"Appeal_to_Authority\", \"Appeal_to_Popularity\",\"Appeal_to_Values\",\"Appeal_to_Fear-Prejudice\",\"Flag_Waving\",\"Causal_Oversimplification\",\n",
    "               \"False_Dilemma-No_Choice\",\"Consequential_Oversimplification\",\"Straw_Man\",\"Red_Herring\",\"Whataboutism\",\"Slogans\",\"Appeal_to_Time\",\n",
    "               \"Conversation_Killer\",\"Loaded_Language\",\"Repetition\",\"Exaggeration-Minimisation\",\"Obfuscation-Vagueness-Confusion\",\"Name_Calling-Labeling\",\n",
    "               \"Doubt\",\"Guilt_by_Association\",\"Appeal_to_Hypocrisy\",\"Questioning_the_Reputation\"]\n",
    "target_tags = [(i, el.strip()) for i, el in enumerate(target_tags)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 0 of 23 for Appeal_to_Authority persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1092 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1092/1092 [00:00<00:00, 3991.04 examples/s]\n",
      "Map: 100%|██████████| 274/274 [00:00<00:00, 4837.84 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='822' max='1370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 822/1370 02:19 < 01:33, 5.86 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Authority Precision</th>\n",
       "      <th>Appeal To Authority Recall</th>\n",
       "      <th>Appeal To Authority F1-score</th>\n",
       "      <th>Appeal To Authority Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Authority Support</th>\n",
       "      <th>I-appeal To Authority Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.708100</td>\n",
       "      <td>0.596136</td>\n",
       "      <td>0.702357</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>2877</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.584248</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.062016</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>2877</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.680259</td>\n",
       "      <td>0.712718</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.097046</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.097046</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.097046</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.097046</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>2877</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>1.020438</td>\n",
       "      <td>0.725518</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>2877</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>1.019248</td>\n",
       "      <td>0.724705</td>\n",
       "      <td>0.213675</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.213675</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.213675</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.213675</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>2877</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>1.216146</td>\n",
       "      <td>0.738318</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.137755</td>\n",
       "      <td>0.172524</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.137755</td>\n",
       "      <td>0.172524</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.137755</td>\n",
       "      <td>0.172524</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.137755</td>\n",
       "      <td>0.172524</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>2877</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 1 of 23 for Appeal_to_Popularity persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/540 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 540/540 [00:00<00:00, 5496.88 examples/s]\n",
      "Map: 100%|██████████| 136/136 [00:00<00:00, 2985.49 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='612' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [612/680 01:17 < 00:08, 7.84 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Popularity Precision</th>\n",
       "      <th>Appeal To Popularity Recall</th>\n",
       "      <th>Appeal To Popularity F1-score</th>\n",
       "      <th>Appeal To Popularity Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Popularity Support</th>\n",
       "      <th>I-appeal To Popularity Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.507714</td>\n",
       "      <td>0.769728</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>0.505493</td>\n",
       "      <td>0.754038</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.639077</td>\n",
       "      <td>0.785879</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.067308</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>1.254878</td>\n",
       "      <td>0.690817</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.129412</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.933606</td>\n",
       "      <td>0.770189</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.204724</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.204724</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.204724</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.204724</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>1.372795</td>\n",
       "      <td>0.720812</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.209150</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>1.293351</td>\n",
       "      <td>0.735118</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>1.268428</td>\n",
       "      <td>0.757268</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.358307</td>\n",
       "      <td>0.750808</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>1499</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 2 of 23 for Appeal_to_Values persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1107 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1107/1107 [00:00<00:00, 4888.13 examples/s]\n",
      "Map: 100%|██████████| 277/277 [00:00<00:00, 3405.06 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='556' max='1390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 556/1390 01:18 < 01:57, 7.09 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Values Precision</th>\n",
       "      <th>Appeal To Values Recall</th>\n",
       "      <th>Appeal To Values F1-score</th>\n",
       "      <th>Appeal To Values Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Values Support</th>\n",
       "      <th>I-appeal To Values Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>0.848962</td>\n",
       "      <td>0.624444</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>2464</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>0.459499</td>\n",
       "      <td>0.810349</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.250951</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.250951</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.250951</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.250951</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>2464</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.491673</td>\n",
       "      <td>0.778974</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.138728</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.138728</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.138728</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.138728</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>2464</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.134600</td>\n",
       "      <td>0.655965</td>\n",
       "      <td>0.789979</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>2464</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 3 of 23 for Appeal_to_Fear-Prejudice persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2497 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2497/2497 [00:00<00:00, 5413.71 examples/s]\n",
      "Map: 100%|██████████| 625/625 [00:00<00:00, 6885.86 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3130/3130 06:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Fear-prejudice Precision</th>\n",
       "      <th>Appeal To Fear-prejudice Recall</th>\n",
       "      <th>Appeal To Fear-prejudice F1-score</th>\n",
       "      <th>Appeal To Fear-prejudice Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-appeal To Fear-prejudice Support</th>\n",
       "      <th>I-appeal To Fear-prejudice Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.734638</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.444300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.769870</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>356.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.772479</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>0.153005</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.120950</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.777936</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.135036</td>\n",
       "      <td>0.161926</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.135036</td>\n",
       "      <td>0.161926</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.135036</td>\n",
       "      <td>0.161926</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>0.202186</td>\n",
       "      <td>0.135036</td>\n",
       "      <td>0.161926</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.775682</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.164751</td>\n",
       "      <td>0.193694</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.164751</td>\n",
       "      <td>0.193694</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.164751</td>\n",
       "      <td>0.193694</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0.164751</td>\n",
       "      <td>0.193694</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.783037</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.204380</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.204380</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.204380</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.204380</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.786714</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.216704</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.216704</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.216704</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.216704</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.788612</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>0.213270</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>0.213270</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>0.213270</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>0.213270</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.785053</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.218679</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.218679</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.218679</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.218679</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.784104</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.177165</td>\n",
       "      <td>0.205950</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.177165</td>\n",
       "      <td>0.205950</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.177165</td>\n",
       "      <td>0.205950</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.177165</td>\n",
       "      <td>0.205950</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "      <td>5506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 4 of 23 for Flag_Waving persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1139 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1139/1139 [00:00<00:00, 3816.17 examples/s]\n",
      "Map: 100%|██████████| 285/285 [00:00<00:00, 5425.61 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1001' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1001/1430 02:41 < 01:09, 6.20 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Flag Waving Precision</th>\n",
       "      <th>Flag Waving Recall</th>\n",
       "      <th>Flag Waving F1-score</th>\n",
       "      <th>Flag Waving Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-flag Waving Support</th>\n",
       "      <th>I-flag Waving Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.636500</td>\n",
       "      <td>0.442158</td>\n",
       "      <td>0.795509</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.034843</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.034843</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.034843</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.034843</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.447700</td>\n",
       "      <td>0.456750</td>\n",
       "      <td>0.801062</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.074906</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.074906</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.074906</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>0.109890</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.074906</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.562242</td>\n",
       "      <td>0.804201</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>0.141079</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.712092</td>\n",
       "      <td>0.807098</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.829991</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.201754</td>\n",
       "      <td>0.224390</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.201754</td>\n",
       "      <td>0.224390</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.201754</td>\n",
       "      <td>0.224390</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>0.201754</td>\n",
       "      <td>0.224390</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.778117</td>\n",
       "      <td>0.827137</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.916027</td>\n",
       "      <td>0.830517</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.197309</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.197309</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.197309</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.197309</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>3002</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 5 of 23 for Causal_Oversimplification persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/843 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 843/843 [00:00<00:00, 5287.39 examples/s]\n",
      "Map: 100%|██████████| 211/211 [00:00<00:00, 5411.08 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='742' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 742/1060 01:30 < 00:38, 8.15 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Causal Oversimplification Precision</th>\n",
       "      <th>Causal Oversimplification Recall</th>\n",
       "      <th>Causal Oversimplification F1-score</th>\n",
       "      <th>Causal Oversimplification Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-causal Oversimplification Support</th>\n",
       "      <th>I-causal Oversimplification Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.746900</td>\n",
       "      <td>0.695282</td>\n",
       "      <td>0.695170</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.568365</td>\n",
       "      <td>0.714671</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.757592</td>\n",
       "      <td>0.744974</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>1.025115</td>\n",
       "      <td>0.730873</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>1.159447</td>\n",
       "      <td>0.750375</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.280822</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.280822</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.280822</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.280822</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>1.448799</td>\n",
       "      <td>0.738974</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.349776</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>1.332985</td>\n",
       "      <td>0.731473</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.238298</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.238298</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.238298</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.238298</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 6 of 23 for False_Dilemma-No_Choice persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/736 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 736/736 [00:00<00:00, 2871.82 examples/s]\n",
      "Map: 100%|██████████| 184/184 [00:00<00:00, 3409.46 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='644' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [644/920 01:33 < 00:40, 6.88 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>False Dilemma-no Choice Precision</th>\n",
       "      <th>False Dilemma-no Choice Recall</th>\n",
       "      <th>False Dilemma-no Choice F1-score</th>\n",
       "      <th>False Dilemma-no Choice Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-false Dilemma-no Choice Support</th>\n",
       "      <th>I-false Dilemma-no Choice Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.841671</td>\n",
       "      <td>0.587473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.448300</td>\n",
       "      <td>0.543067</td>\n",
       "      <td>0.763653</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0.202643</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0.202643</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0.202643</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0.202643</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.844074</td>\n",
       "      <td>0.772293</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.798662</td>\n",
       "      <td>0.775687</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.931255</td>\n",
       "      <td>0.792348</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.968050</td>\n",
       "      <td>0.760568</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.981401</td>\n",
       "      <td>0.798210</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>0.265403</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>0.265403</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>0.265403</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.220472</td>\n",
       "      <td>0.265403</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 7 of 23 for Consequential_Oversimplification persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/561 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 561/561 [00:00<00:00, 4688.24 examples/s]\n",
      "Map: 100%|██████████| 141/141 [00:00<00:00, 3843.26 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='568' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [568/710 01:20 < 00:20, 7.04 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Consequential Oversimplification Precision</th>\n",
       "      <th>Consequential Oversimplification Recall</th>\n",
       "      <th>Consequential Oversimplification F1-score</th>\n",
       "      <th>Consequential Oversimplification Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-consequential Oversimplification Support</th>\n",
       "      <th>I-consequential Oversimplification Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>0.777546</td>\n",
       "      <td>0.506173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.759674</td>\n",
       "      <td>0.765046</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.507041</td>\n",
       "      <td>0.847608</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.203883</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.645870</td>\n",
       "      <td>0.837963</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.678294</td>\n",
       "      <td>0.867284</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.818362</td>\n",
       "      <td>0.848765</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.750618</td>\n",
       "      <td>0.866127</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.789515</td>\n",
       "      <td>0.878086</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.341085</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.341085</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.341085</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.341085</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1241</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 8 of 23 for Straw_Man persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/512 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 512/512 [00:00<00:00, 3424.53 examples/s]\n",
      "Map: 100%|██████████| 128/128 [00:00<00:00, 5124.72 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='448' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [448/640 01:25 < 00:36, 5.22 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Straw Man Precision</th>\n",
       "      <th>Straw Man Recall</th>\n",
       "      <th>Straw Man F1-score</th>\n",
       "      <th>Straw Man Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-straw Man Support</th>\n",
       "      <th>I-straw Man Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>0.669282</td>\n",
       "      <td>0.677138</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.630401</td>\n",
       "      <td>0.715532</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.750704</td>\n",
       "      <td>0.738802</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>1.001218</td>\n",
       "      <td>0.763816</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>1.152282</td>\n",
       "      <td>0.753345</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.234234</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.234234</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.234234</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.234234</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>1.319996</td>\n",
       "      <td>0.744619</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>1.319879</td>\n",
       "      <td>0.753927</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>946</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 9 of 23 for Red_Herring persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/302 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 302/302 [00:00<00:00, 4532.66 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 3867.78 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='304' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [304/380 00:45 < 00:11, 6.59 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Red Herring Precision</th>\n",
       "      <th>Red Herring Recall</th>\n",
       "      <th>Red Herring F1-score</th>\n",
       "      <th>Red Herring Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-red Herring Support</th>\n",
       "      <th>I-red Herring Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.828500</td>\n",
       "      <td>0.712003</td>\n",
       "      <td>0.639780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.796287</td>\n",
       "      <td>0.690192</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.700280</td>\n",
       "      <td>0.705775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.920769</td>\n",
       "      <td>0.700275</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.176100</td>\n",
       "      <td>1.199473</td>\n",
       "      <td>0.645280</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>1.124695</td>\n",
       "      <td>0.728689</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>1.280623</td>\n",
       "      <td>0.723190</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>1.342625</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>695</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 10 of 23 for Whataboutism persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/217 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 217/217 [00:00<00:00, 4118.76 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 2944.95 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168/280 00:28 < 00:19, 5.87 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Whataboutism Precision</th>\n",
       "      <th>Whataboutism Recall</th>\n",
       "      <th>Whataboutism F1-score</th>\n",
       "      <th>Whataboutism Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-whataboutism Support</th>\n",
       "      <th>I-whataboutism Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.969100</td>\n",
       "      <td>0.710754</td>\n",
       "      <td>0.630252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.711200</td>\n",
       "      <td>0.604726</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.461500</td>\n",
       "      <td>0.751381</td>\n",
       "      <td>0.661998</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.650959</td>\n",
       "      <td>0.760971</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.791090</td>\n",
       "      <td>0.758170</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.811960</td>\n",
       "      <td>0.741363</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 11 of 23 for Slogans persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1081 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1081/1081 [00:00<00:00, 4852.02 examples/s]\n",
      "Map: 100%|██████████| 271/271 [00:00<00:00, 5915.40 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 680/1360 01:34 < 01:34, 7.21 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Slogans Precision</th>\n",
       "      <th>Slogans Recall</th>\n",
       "      <th>Slogans F1-score</th>\n",
       "      <th>Slogans Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-slogans Support</th>\n",
       "      <th>I-slogans Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393200</td>\n",
       "      <td>0.266209</td>\n",
       "      <td>0.914263</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>3123</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.222958</td>\n",
       "      <td>0.920614</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.301724</td>\n",
       "      <td>0.328638</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.301724</td>\n",
       "      <td>0.328638</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.301724</td>\n",
       "      <td>0.328638</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>0.360825</td>\n",
       "      <td>0.301724</td>\n",
       "      <td>0.328638</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>3123</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.392618</td>\n",
       "      <td>0.917438</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3123</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.295403</td>\n",
       "      <td>0.925377</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.334884</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.334884</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.334884</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.334884</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>3123</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.366124</td>\n",
       "      <td>0.926171</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>3123</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 12 of 23 for Appeal_to_Time persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/259 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 259/259 [00:00<00:00, 3358.52 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 2842.88 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/330 00:35 < 00:15, 6.50 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Time Precision</th>\n",
       "      <th>Appeal To Time Recall</th>\n",
       "      <th>Appeal To Time F1-score</th>\n",
       "      <th>Appeal To Time Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Time Support</th>\n",
       "      <th>I-appeal To Time Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>0.558920</td>\n",
       "      <td>0.759048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.482700</td>\n",
       "      <td>0.595881</td>\n",
       "      <td>0.721905</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.580167</td>\n",
       "      <td>0.779048</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.155400</td>\n",
       "      <td>0.534903</td>\n",
       "      <td>0.835238</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.788998</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.679575</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>1.003050</td>\n",
       "      <td>0.787619</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>750</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 13 of 23 for Conversation_Killer persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1553 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1553/1553 [00:00<00:00, 5330.30 examples/s]\n",
      "Map: 100%|██████████| 389/389 [00:00<00:00, 5620.40 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='1950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/1950 03:23 < 00:50, 7.67 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Conversation Killer Precision</th>\n",
       "      <th>Conversation Killer Recall</th>\n",
       "      <th>Conversation Killer F1-score</th>\n",
       "      <th>Conversation Killer Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-conversation Killer Support</th>\n",
       "      <th>I-conversation Killer Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526400</td>\n",
       "      <td>0.531473</td>\n",
       "      <td>0.767592</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.230216</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>0.579982</td>\n",
       "      <td>0.797504</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.258964</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.258964</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.258964</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.258964</td>\n",
       "      <td>0.326633</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.770673</td>\n",
       "      <td>0.838821</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.851489</td>\n",
       "      <td>0.838390</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.325203</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.888478</td>\n",
       "      <td>0.843985</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.394649</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.394649</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.394649</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.388158</td>\n",
       "      <td>0.394649</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>1.106362</td>\n",
       "      <td>0.836023</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>1.014257</td>\n",
       "      <td>0.831289</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>1.138676</td>\n",
       "      <td>0.840542</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.389058</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.389058</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.389058</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.389058</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 14 of 23 for Loaded_Language persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/12305 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 12305/12305 [00:03<00:00, 3324.65 examples/s]\n",
      "Map: 100%|██████████| 3077/3077 [00:00<00:00, 4761.65 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9234' max='15390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9234/15390 30:21 < 20:14, 5.07 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Loaded Language Precision</th>\n",
       "      <th>Loaded Language Recall</th>\n",
       "      <th>Loaded Language F1-score</th>\n",
       "      <th>Loaded Language Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-loaded Language Support</th>\n",
       "      <th>I-loaded Language Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.279836</td>\n",
       "      <td>0.902974</td>\n",
       "      <td>0.160139</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>0.182449</td>\n",
       "      <td>868.000000</td>\n",
       "      <td>0.160139</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>0.182449</td>\n",
       "      <td>868.000000</td>\n",
       "      <td>0.160139</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>0.182449</td>\n",
       "      <td>868.000000</td>\n",
       "      <td>0.160139</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>0.182449</td>\n",
       "      <td>868.000000</td>\n",
       "      <td>43137</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.298171</td>\n",
       "      <td>0.901743</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>0.248899</td>\n",
       "      <td>0.270012</td>\n",
       "      <td>1362.000000</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>0.248899</td>\n",
       "      <td>0.270012</td>\n",
       "      <td>1362.000000</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>0.248899</td>\n",
       "      <td>0.270012</td>\n",
       "      <td>1362.000000</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>0.248899</td>\n",
       "      <td>0.270012</td>\n",
       "      <td>1362.000000</td>\n",
       "      <td>43137</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.407383</td>\n",
       "      <td>0.907075</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.271993</td>\n",
       "      <td>0.267786</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.271993</td>\n",
       "      <td>0.267786</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.271993</td>\n",
       "      <td>0.267786</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>0.263708</td>\n",
       "      <td>0.271993</td>\n",
       "      <td>0.267786</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>43137</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.380102</td>\n",
       "      <td>0.905947</td>\n",
       "      <td>0.306353</td>\n",
       "      <td>0.288998</td>\n",
       "      <td>0.297423</td>\n",
       "      <td>1218.000000</td>\n",
       "      <td>0.306353</td>\n",
       "      <td>0.288998</td>\n",
       "      <td>0.297423</td>\n",
       "      <td>1218.000000</td>\n",
       "      <td>0.306353</td>\n",
       "      <td>0.288998</td>\n",
       "      <td>0.297423</td>\n",
       "      <td>1218.000000</td>\n",
       "      <td>0.306353</td>\n",
       "      <td>0.288998</td>\n",
       "      <td>0.297423</td>\n",
       "      <td>1218.000000</td>\n",
       "      <td>43137</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.906870</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.282569</td>\n",
       "      <td>0.275123</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.282569</td>\n",
       "      <td>0.275123</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.282569</td>\n",
       "      <td>0.275123</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.282569</td>\n",
       "      <td>0.275123</td>\n",
       "      <td>1090.000000</td>\n",
       "      <td>43137</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.573146</td>\n",
       "      <td>0.906399</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>1079.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>1079.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>1079.000000</td>\n",
       "      <td>0.268059</td>\n",
       "      <td>0.285449</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>1079.000000</td>\n",
       "      <td>43137</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='385' max='385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [385/385 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 15 of 23 for Repetition persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1777 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1777/1777 [00:00<00:00, 4692.28 examples/s]\n",
      "Map: 100%|██████████| 445/445 [00:00<00:00, 1232.29 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1561' max='2230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1561/2230 03:41 < 01:35, 7.03 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Repetition Precision</th>\n",
       "      <th>Repetition Recall</th>\n",
       "      <th>Repetition F1-score</th>\n",
       "      <th>Repetition Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-repetition Support</th>\n",
       "      <th>I-repetition Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.382523</td>\n",
       "      <td>0.892672</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.032468</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.053476</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.365278</td>\n",
       "      <td>0.899186</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.218310</td>\n",
       "      <td>0.209459</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.218310</td>\n",
       "      <td>0.209459</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.218310</td>\n",
       "      <td>0.209459</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.218310</td>\n",
       "      <td>0.209459</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.418313</td>\n",
       "      <td>0.900518</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.240803</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.240803</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.240803</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.248276</td>\n",
       "      <td>0.240803</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.509967</td>\n",
       "      <td>0.904959</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.344371</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.344371</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.344371</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.344371</td>\n",
       "      <td>0.340984</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.629804</td>\n",
       "      <td>0.904219</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.360759</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.635546</td>\n",
       "      <td>0.900222</td>\n",
       "      <td>0.396104</td>\n",
       "      <td>0.311224</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.396104</td>\n",
       "      <td>0.311224</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.396104</td>\n",
       "      <td>0.311224</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.396104</td>\n",
       "      <td>0.311224</td>\n",
       "      <td>0.348571</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>0.905107</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.356250</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.356250</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.356250</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.356250</td>\n",
       "      <td>0.363057</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>6010</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 16 of 23 for Exaggeration-Minimisation persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2737 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2737/2737 [00:00<00:00, 5328.79 examples/s]\n",
      "Map: 100%|██████████| 685/685 [00:00<00:00, 5771.06 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2058' max='3430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2058/3430 04:22 < 02:55, 7.83 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Exaggeration-minimisation Precision</th>\n",
       "      <th>Exaggeration-minimisation Recall</th>\n",
       "      <th>Exaggeration-minimisation F1-score</th>\n",
       "      <th>Exaggeration-minimisation Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-exaggeration-minimisation Support</th>\n",
       "      <th>I-exaggeration-minimisation Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.810053</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.064838</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>7108</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.568403</td>\n",
       "      <td>0.811471</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.067708</td>\n",
       "      <td>0.065327</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.067708</td>\n",
       "      <td>0.065327</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.067708</td>\n",
       "      <td>0.065327</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.063107</td>\n",
       "      <td>0.067708</td>\n",
       "      <td>0.065327</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>7108</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.778985</td>\n",
       "      <td>0.831861</td>\n",
       "      <td>0.150485</td>\n",
       "      <td>0.135965</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.150485</td>\n",
       "      <td>0.135965</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.150485</td>\n",
       "      <td>0.135965</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>0.150485</td>\n",
       "      <td>0.135965</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>7108</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.799205</td>\n",
       "      <td>0.832188</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.161435</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.161435</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.161435</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.161435</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>7108</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.983236</td>\n",
       "      <td>0.824447</td>\n",
       "      <td>0.140777</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.132420</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.140777</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.132420</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.140777</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.132420</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>0.140777</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.132420</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>7108</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>1.221787</td>\n",
       "      <td>0.830771</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.145631</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.150754</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>7108</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 17 of 23 for Obfuscation-Vagueness-Confusion persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/560 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 560/560 [00:00<00:00, 3169.17 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 3171.60 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/700 01:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Obfuscation-vagueness-confusion Precision</th>\n",
       "      <th>Obfuscation-vagueness-confusion Recall</th>\n",
       "      <th>Obfuscation-vagueness-confusion F1-score</th>\n",
       "      <th>Obfuscation-vagueness-confusion Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-obfuscation-vagueness-confusion Support</th>\n",
       "      <th>I-obfuscation-vagueness-confusion Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.613324</td>\n",
       "      <td>0.675562</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.546400</td>\n",
       "      <td>0.669329</td>\n",
       "      <td>0.673689</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.677524</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.769938</td>\n",
       "      <td>0.677434</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>1.083193</td>\n",
       "      <td>0.665262</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>1.132720</td>\n",
       "      <td>0.702247</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.035398</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>1.176569</td>\n",
       "      <td>0.714419</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>1.278691</td>\n",
       "      <td>0.693820</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.044944</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.356662</td>\n",
       "      <td>0.715356</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>1.366375</td>\n",
       "      <td>0.717697</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1232</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 18 of 23 for Name_Calling-Labeling persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/8819 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 8819/8819 [00:02<00:00, 3919.59 examples/s]\n",
      "Map: 100%|██████████| 2205/2205 [00:00<00:00, 2904.65 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6618' max='11030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6618/11030 15:56 < 10:37, 6.92 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Name Calling-labeling Precision</th>\n",
       "      <th>Name Calling-labeling Recall</th>\n",
       "      <th>Name Calling-labeling F1-score</th>\n",
       "      <th>Name Calling-labeling Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-name Calling-labeling Support</th>\n",
       "      <th>I-name Calling-labeling Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.936503</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>0.322275</td>\n",
       "      <td>872.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>0.322275</td>\n",
       "      <td>872.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>0.322275</td>\n",
       "      <td>872.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>0.322275</td>\n",
       "      <td>872.000000</td>\n",
       "      <td>29441</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.933762</td>\n",
       "      <td>0.409314</td>\n",
       "      <td>0.343621</td>\n",
       "      <td>0.373602</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>0.409314</td>\n",
       "      <td>0.343621</td>\n",
       "      <td>0.373602</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>0.409314</td>\n",
       "      <td>0.343621</td>\n",
       "      <td>0.373602</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>0.409314</td>\n",
       "      <td>0.343621</td>\n",
       "      <td>0.373602</td>\n",
       "      <td>972.000000</td>\n",
       "      <td>29441</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.938936</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.393795</td>\n",
       "      <td>860.000000</td>\n",
       "      <td>29441</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.937458</td>\n",
       "      <td>0.422794</td>\n",
       "      <td>0.386338</td>\n",
       "      <td>0.403745</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>0.422794</td>\n",
       "      <td>0.386338</td>\n",
       "      <td>0.403745</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>0.422794</td>\n",
       "      <td>0.386338</td>\n",
       "      <td>0.403745</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>0.422794</td>\n",
       "      <td>0.386338</td>\n",
       "      <td>0.403745</td>\n",
       "      <td>893.000000</td>\n",
       "      <td>29441</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.938228</td>\n",
       "      <td>0.379902</td>\n",
       "      <td>0.411141</td>\n",
       "      <td>0.394904</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>0.379902</td>\n",
       "      <td>0.411141</td>\n",
       "      <td>0.394904</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>0.379902</td>\n",
       "      <td>0.411141</td>\n",
       "      <td>0.394904</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>0.379902</td>\n",
       "      <td>0.411141</td>\n",
       "      <td>0.394904</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>29441</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.938505</td>\n",
       "      <td>0.370098</td>\n",
       "      <td>0.387179</td>\n",
       "      <td>0.378446</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>0.370098</td>\n",
       "      <td>0.387179</td>\n",
       "      <td>0.378446</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>0.370098</td>\n",
       "      <td>0.387179</td>\n",
       "      <td>0.378446</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>0.370098</td>\n",
       "      <td>0.387179</td>\n",
       "      <td>0.378446</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>29441</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='276' max='276' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [276/276 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 19 of 23 for Doubt persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/6923 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 6923/6923 [00:01<00:00, 3711.85 examples/s]\n",
      "Map: 100%|██████████| 1731/1731 [00:00<00:00, 4814.55 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5196' max='8660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5196/8660 12:54 < 08:36, 6.70 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Doubt Precision</th>\n",
       "      <th>Doubt Recall</th>\n",
       "      <th>Doubt F1-score</th>\n",
       "      <th>Doubt Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-doubt Support</th>\n",
       "      <th>I-doubt Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.647600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.718598</td>\n",
       "      <td>0.210306</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>0.159873</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>0.210306</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>0.159873</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>0.210306</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>0.159873</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>0.210306</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>0.159873</td>\n",
       "      <td>1171.000000</td>\n",
       "      <td>15931</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.732386</td>\n",
       "      <td>0.305014</td>\n",
       "      <td>0.224385</td>\n",
       "      <td>0.258560</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>0.305014</td>\n",
       "      <td>0.224385</td>\n",
       "      <td>0.258560</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>0.305014</td>\n",
       "      <td>0.224385</td>\n",
       "      <td>0.258560</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>0.305014</td>\n",
       "      <td>0.224385</td>\n",
       "      <td>0.258560</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>15931</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.738169</td>\n",
       "      <td>0.320334</td>\n",
       "      <td>0.240084</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>0.320334</td>\n",
       "      <td>0.240084</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>0.320334</td>\n",
       "      <td>0.240084</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>0.320334</td>\n",
       "      <td>0.240084</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>958.000000</td>\n",
       "      <td>15931</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.734149</td>\n",
       "      <td>0.353760</td>\n",
       "      <td>0.260246</td>\n",
       "      <td>0.299882</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>0.353760</td>\n",
       "      <td>0.260246</td>\n",
       "      <td>0.299882</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>0.353760</td>\n",
       "      <td>0.260246</td>\n",
       "      <td>0.299882</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>0.353760</td>\n",
       "      <td>0.260246</td>\n",
       "      <td>0.299882</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>15931</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.736441</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.284574</td>\n",
       "      <td>0.291156</td>\n",
       "      <td>752.000000</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.284574</td>\n",
       "      <td>0.291156</td>\n",
       "      <td>752.000000</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.284574</td>\n",
       "      <td>0.291156</td>\n",
       "      <td>752.000000</td>\n",
       "      <td>0.298050</td>\n",
       "      <td>0.284574</td>\n",
       "      <td>0.291156</td>\n",
       "      <td>752.000000</td>\n",
       "      <td>15931</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.742189</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>0.259398</td>\n",
       "      <td>0.273087</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>0.259398</td>\n",
       "      <td>0.273087</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>0.259398</td>\n",
       "      <td>0.273087</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>0.288301</td>\n",
       "      <td>0.259398</td>\n",
       "      <td>0.273087</td>\n",
       "      <td>798.000000</td>\n",
       "      <td>15931</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='217' max='217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [217/217 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 20 of 23 for Guilt_by_Association persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1030 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1030/1030 [00:00<00:00, 4624.14 examples/s]\n",
      "Map: 100%|██████████| 258/258 [00:00<00:00, 5824.45 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1032' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1032/1290 02:21 < 00:35, 7.26 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Guilt By Association Precision</th>\n",
       "      <th>Guilt By Association Recall</th>\n",
       "      <th>Guilt By Association F1-score</th>\n",
       "      <th>Guilt By Association Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-guilt By Association Support</th>\n",
       "      <th>I-guilt By Association Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.709779</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.769716</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.085526</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.085526</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.085526</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.085526</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.750215</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.735016</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.134200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.773731</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.753083</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.763980</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.160804</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.160804</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.160804</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>0.160804</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.756811</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>2132</td>\n",
       "      <td>85</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 21 of 23 for Appeal_to_Hypocrisy persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1358 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1358/1358 [00:00<00:00, 4264.01 examples/s]\n",
      "Map: 100%|██████████| 340/340 [00:00<00:00, 4188.80 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1700 02:28 < 01:39, 6.87 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Hypocrisy Precision</th>\n",
       "      <th>Appeal To Hypocrisy Recall</th>\n",
       "      <th>Appeal To Hypocrisy F1-score</th>\n",
       "      <th>Appeal To Hypocrisy Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Hypocrisy Support</th>\n",
       "      <th>I-appeal To Hypocrisy Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.655826</td>\n",
       "      <td>0.696277</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>2760</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.639905</td>\n",
       "      <td>0.739394</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.200542</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.200542</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.200542</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.200542</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>2760</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.896254</td>\n",
       "      <td>0.759481</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>2760</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.917281</td>\n",
       "      <td>0.762944</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.269939</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.269939</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.269939</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>0.269939</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>2760</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>1.224465</td>\n",
       "      <td>0.753939</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>2760</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>1.270145</td>\n",
       "      <td>0.756710</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.263930</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.263930</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.263930</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.263930</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>2760</td>\n",
       "      <td>136</td>\n",
       "      <td>2879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 22 of 23 for Questioning_the_Reputation persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/3494 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 3494/3494 [00:00<00:00, 5005.81 examples/s]\n",
      "Map: 100%|██████████| 874/874 [00:00<00:00, 6374.33 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='4370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/4370 02:54 < 06:48, 7.50 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Questioning The Reputation Precision</th>\n",
       "      <th>Questioning The Reputation Recall</th>\n",
       "      <th>Questioning The Reputation F1-score</th>\n",
       "      <th>Questioning The Reputation Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-questioning The Reputation Support</th>\n",
       "      <th>I-questioning The Reputation Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.525320</td>\n",
       "      <td>0.725221</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>0.235561</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>0.235561</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>0.235561</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.185383</td>\n",
       "      <td>0.235561</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>7573</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.481720</td>\n",
       "      <td>0.785377</td>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.230179</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.230179</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.230179</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.230179</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>7573</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.731673</td>\n",
       "      <td>0.767536</td>\n",
       "      <td>0.251553</td>\n",
       "      <td>0.199017</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>0.251553</td>\n",
       "      <td>0.199017</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>0.251553</td>\n",
       "      <td>0.199017</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>0.251553</td>\n",
       "      <td>0.199017</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>7573</td>\n",
       "      <td>322</td>\n",
       "      <td>5221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift = 0\n",
    "for i, tt in enumerate(target_tags):\n",
    "    if i < shift:\n",
    "        continue\n",
    "    print(f'Training model no. {i} of {len(target_tags)} for {tt[1]} persuasion technique...')\n",
    "    labels_model = LabelSet(labels=[tt[1]])\n",
    "    \n",
    "    df_list = df.to_dict(orient='records')\n",
    "    df_list_binary = span_to_words_annotation(dict_of_lists(df_list), target_tag=tt[1], mappings=regex_tokenizer_mappings, labels_model=labels_model)\n",
    "    df_binary = pd.DataFrame(df_list_binary)\n",
    "    df_binary_pos = df_binary[df_binary['tag'] == tt[1]]\n",
    "    df_binary_neg = df_binary[df_binary['tag'] != tt[1]].sample(len(df_binary_pos))\n",
    "    df_binary_subsampled = pd.concat([df_binary_pos, df_binary_neg])#.sample(1000)\n",
    "\n",
    "    binary_dataset = Dataset.from_pandas(df_binary_subsampled[['id', 'ner_tags', 'tokens']])\n",
    "\n",
    "    split_ratio = 0.2\n",
    "    split_seed = 42\n",
    "    datadict = binary_dataset.train_test_split(split_ratio, seed=split_seed)\n",
    "\n",
    "    #model_name = 'bert-base-multilingual-cased'\n",
    "    #model_name = 'xlm-roberta-base'\n",
    "    model_name = 'microsoft/mdeberta-v3-base'\n",
    "    model_name_simple = model_name.split('/')[-1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 16\n",
    "    datadict = datadict.map(lambda x: tokenize_token_classification(x, tokenizer), batched=True, batch_size=None)\n",
    "\n",
    "    columns = [\n",
    "                'input_ids',\n",
    "                'token_type_ids',\n",
    "                'attention_mask',\n",
    "                'labels'\n",
    "                ]\n",
    "\n",
    "    datadict.set_format('torch', columns = columns)\n",
    "\n",
    "    train_data = datadict['train']\n",
    "    val_data = datadict['test']\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                num_labels=len(labels_model.ids_to_label.values()),\n",
    "                                                                label2id=labels_model.labels_to_id,\n",
    "                                                                id2label=labels_model.ids_to_label,\n",
    "                                                                )\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=f'/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results/{date_time}_no_aug_no_cw_ts0/weights/mdeberta-v3-base-NEW_aug_{i}_{tt[1]}',\n",
    "                                  save_total_limit=2,\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_only_model=True,\n",
    "                                  metric_for_best_model='eval_macro avg_f1-score',\n",
    "                                  logging_strategy='epoch',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=5e-5,\n",
    "                                  optim='adamw_torch',\n",
    "                                  num_train_epochs=10)\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    trainer = Trainer(model,\n",
    "                      training_args,\n",
    "                      train_dataset=train_data,\n",
    "                      eval_dataset=val_data,\n",
    "                      data_collator=data_collator,\n",
    "                      tokenizer=tokenizer,\n",
    "                      compute_metrics=compute_metrics_wrapper(\n",
    "                          label_list=[i for i in labels_model.ids_to_label.values()],\n",
    "                          pt=tt[1],\n",
    "                          model_name_simple=model_name_simple,\n",
    "                          date_time=date_time),\n",
    "                      callbacks=[early_stopping])\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MACRO F1 (across PTs), best models = 0.266\n",
    "### last models = 0.232"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
