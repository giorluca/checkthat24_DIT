{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from tokenizers import Encoding\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from seqeval.metrics import classification_report\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_annotations_bio(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "            print('char_ix = ', char_ix)\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"B\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"I\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremental ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BI\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bio(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "\n",
    "\n",
    "def tokenize_token_classification(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = [tokenized_inputs.token_to_word(i, j) for j in range(len(tokenized_inputs['input_ids'][i]))]  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "def dict_of_lists(lst_of_dicts):\n",
    "    result = defaultdict(list)\n",
    "    for d in lst_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            result[key].append(value)\n",
    "    return dict(result)\n",
    "\n",
    "def list_of_dicts(dict_of_lists):\n",
    "    # First, we need to check if all lists are of the same length to ensure correct transformation\n",
    "    if not all(len(lst) == len(next(iter(dict_of_lists.values()))) for lst in dict_of_lists.values()):\n",
    "        raise ValueError(\"All lists in the dictionary must have the same length\")\n",
    "\n",
    "    # Get the length of the items in any of the lists\n",
    "    length = len(next(iter(dict_of_lists.values())))\n",
    "    \n",
    "    # Create a list of dictionaries, one for each index in the lists\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        # Create a dictionary for the current index 'i' across all lists\n",
    "        new_dict = {key: dict_of_lists[key][i] for key in dict_of_lists}\n",
    "        result.append(new_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sub_shift_spans(text, ents = [], mappings = []):\n",
    "    for mapping in mappings:\n",
    "        adjustment = 0\n",
    "        pattern = re.compile(mapping['pattern'])\n",
    "        for match in re.finditer(pattern, text):\n",
    "            match_index = match.start() + adjustment\n",
    "            match_contents = match.group()\n",
    "            if all(mapping['check'](char) for char in match_contents):\n",
    "                subbed_text = mapping['target'].replace('placeholder', match_contents)\n",
    "            else:\n",
    "                subbed_text = mapping['target']\n",
    "            len_diff = len(subbed_text) - len(match_contents)\n",
    "            text = text[:match_index] + subbed_text + text[match_index + len(match_contents):]\n",
    "            if ents:\n",
    "                if isinstance(ents, list):\n",
    "                    for ent in ents:\n",
    "                        if ent['start'] <= match_index and ent['end'] > match_index:\n",
    "                            ent['end'] += len_diff\n",
    "                        if ent['start'] > match_index:\n",
    "                            ent['start'] += len_diff\n",
    "                            ent['end'] += len_diff\n",
    "                elif isinstance(ents, dict):\n",
    "                    if ents['value']['start'] <= match_index and ents['value']['end'] > match_index:\n",
    "                        ents['value']['end'] += len_diff\n",
    "                    if ents['value']['start'] > match_index:\n",
    "                        ents['value']['start'] += len_diff\n",
    "                        ents['value']['end'] += len_diff\n",
    "\n",
    "            adjustment += len_diff\n",
    "\n",
    "    return text, ents\n",
    "\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    # if not any([l for l in samples['annotations']]):\n",
    "        \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                if annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        if first:\n",
    "                            labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                            first = False\n",
    "                        elif not first:\n",
    "                            labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0]})\n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        # if the training sample has no tags that we need, we just produce a 0s list\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # if the training sample has tags we need, we first exclude the label lists whose tags don't match\n",
    "        # and then we merge the label lists that have tags that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    return samples_new\n",
    "\n",
    "regex_tokenizer_mappings = [\n",
    "    {'pattern': r'(?<!\\s)([^\\w\\s])|([^\\w\\s])(?!\\s)',\n",
    "    'target': ' placeholder ',\n",
    "    'check': lambda x: unicodedata.category(x).startswith('P'),\n",
    "    },\n",
    "    {'pattern': r'\\s+',\n",
    "     'target': ' ',\n",
    "     'check': lambda x: False if re.match('\\s+', x) is None else True,\n",
    "     },\n",
    "    ]\n",
    "\n",
    "def compute_metrics_wrapper(label_list):\n",
    "    def compute_metrics(eval_preds):\n",
    "        nonlocal label_list\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "        # Extract the true predictions and labels from the sequences\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        # Compute sequence-level evaluation metrics\n",
    "        results = classification_report(true_predictions, true_labels, output_dict=True)\n",
    "\n",
    "        # Flatten the lists to calculate micro F1-score and supports\n",
    "        flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "        flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Calculate micro F1-score using sklearn\n",
    "        micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "        # Prepare the results dictionary\n",
    "        flat_results = {'micro_f1': float(micro_f1)}\n",
    "\n",
    "        # Add detailed metrics for each label to the results dictionary\n",
    "        for label, metrics in results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    flat_results[f'{label}_{metric}'] = float(value)\n",
    "\n",
    "        # Compute support for each label using Counter\n",
    "        label_support = Counter(flat_true_labels)\n",
    "        for label, count in label_support.items():\n",
    "            flat_results[f'{label}_support'] = count\n",
    "\n",
    "        return flat_results\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/home/luca/Desktop/checkthat24_DIT/data/formatted/train_sentences.json'\n",
    "with open(train_data_path, 'r', encoding='utf8') as f:\n",
    "    dataset_raw = json.load(f)\n",
    "\n",
    "df_raw = pd.DataFrame(dataset_raw)  \n",
    "\n",
    "df_pos = df_raw[df_raw['annotations'].apply(lambda x: len(x[0]['result']) > 0)]\n",
    "df_neg = df_raw[df_raw['annotations'].apply(lambda x: x[0]['result'] == [])].sample(len(df_pos))\n",
    "df = pd.concat([df_pos,df_neg])\n",
    "\n",
    "target_tags = [\"Appeal_to_Authority\", \"Appeal_to_Popularity\",\"Appeal_to_Values\",\"Appeal_to_Fear-Prejudice\",\"Flag_Waving\",\"Causal_Oversimplification\",\n",
    "               \"False_Dilemma-No_Choice\",\"Consequential_Oversimplification\",\"Straw_Man\",\"Red_Herring\",\"Whataboutism\",\"Slogans\",\"Appeal_to_Time\",\n",
    "               \"Conversation_Killer\",\"Loaded_Language\",\"Repetition\",\"Exaggeration-Minimisation\",\"Obfuscation-Vagueness-Confusion\",\"Name_Calling-Labeling\",\n",
    "               \"Doubt\",\"Guilt_by_Association\",\"Appeal_to_Hypocrisy\",\"Questioning_the_Reputation\"]\n",
    "target_tags = [(i, el.strip()) for i, el in enumerate(target_tags)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 0\n",
    "for i, tt in enumerate(target_tags):\n",
    "    if i < shift:\n",
    "        continue\n",
    "    print(f'Training model no. {i} of {len(target_tags)} for {tt[1]} persuasion technique...')\n",
    "    labels_model = LabelSet(labels=[tt[1]])\n",
    "    \n",
    "    df_list = df.to_dict(orient='records')\n",
    "    df_list_binary = span_to_words_annotation(dict_of_lists(df_list), target_tag=tt[1], mappings=regex_tokenizer_mappings, labels_model=labels_model)\n",
    "    df_binary = pd.DataFrame(df_list_binary)\n",
    "    df_binary_pos = df_binary[df_binary['tag'] == tt[1]]\n",
    "    df_binary_neg = df_binary[df_binary['tag'] != tt[1]].sample(len(df_binary_pos))\n",
    "    df_binary_subsampled = pd.concat([df_binary_pos, df_binary_neg])#.sample(1000)\n",
    "\n",
    "    binary_dataset = Dataset.from_pandas(df_binary_subsampled[['id', 'ner_tags', 'tokens']])\n",
    "\n",
    "    split_ratio = 0.2\n",
    "    split_seed = 42\n",
    "    datadict = binary_dataset.train_test_split(split_ratio, seed=split_seed)\n",
    "\n",
    "    #model_name = 'bert-base-multilingual-cased'\n",
    "    #model_name = 'xlm-roberta-base'\n",
    "    model_name = 'microsoft/mdeberta-v3-base'\n",
    "    model_name_simple = model_name.split('/')[-1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 16\n",
    "    datadict = datadict.map(lambda x: tokenize_token_classification(x, tokenizer), batched=True, batch_size=None)\n",
    "\n",
    "    columns = [\n",
    "                'input_ids',\n",
    "                'token_type_ids',\n",
    "                'attention_mask',\n",
    "                'labels'\n",
    "                ]\n",
    "\n",
    "    datadict.set_format('torch', columns = columns)\n",
    "\n",
    "    train_data = datadict['train']\n",
    "    val_data = datadict['test']\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                num_labels=len(labels_model.ids_to_label.values()),\n",
    "                                                                label2id=labels_model.labels_to_id,\n",
    "                                                                id2label=labels_model.ids_to_label,\n",
    "                                                                )\n",
    "    \n",
    "    training_args = TrainingArguments(#output_dir='/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/mdeberta-v3-base-NEW',\n",
    "        output_dir='/home/luca/Desktop',\n",
    "                                  save_total_limit=2,\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_only_model=True,\n",
    "                                  metric_for_best_model='eval_macro avg_f1-score',\n",
    "                                  logging_strategy='epoch',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=5e-5,\n",
    "                                  optim='adamw_torch',\n",
    "                                  num_train_epochs=10)\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    trainer = Trainer(model,\n",
    "                      training_args,\n",
    "                      train_dataset=train_data,\n",
    "                      eval_dataset=val_data,\n",
    "                      data_collator=data_collator,\n",
    "                      tokenizer=tokenizer,\n",
    "                      compute_metrics=compute_metrics_wrapper(label_list=[i for i in labels_model.ids_to_label.values()]),\n",
    "                      callbacks=[early_stopping])\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
