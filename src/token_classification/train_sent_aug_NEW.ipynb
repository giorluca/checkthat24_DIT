{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "import os\n",
    "from tokenizers import Encoding\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from seqeval.metrics import classification_report, precision_score\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_annotations_bio(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "            print('char_ix = ', char_ix)\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"B\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"I\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['tag']}\"\n",
    "    return aligned_labels\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremental ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BI\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bio(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "    \n",
    "class WeightedLoss(CrossEntropyLoss):\n",
    "    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean'):\n",
    "        super(WeightedLoss, self).__init__(weight, size_average, ignore_index, reduce, reduction)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensure weight tensor is on the same device as input\n",
    "        weight = torch.tensor([0.5 if x == 0 else 2.0 for x in target.view(-1)], device=input.device, dtype=input.dtype)\n",
    "        # Ensure target is on the same device as input\n",
    "        target = target.to(input.device)\n",
    "        loss = super(WeightedLoss, self).forward(input.view(-1, input.size(-1)), target.view(-1))\n",
    "        return (loss * weight).mean()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = WeightedLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def tokenize_token_classification(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='longest', return_tensors='pt')\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = [tokenized_inputs.token_to_word(i, j) for j in range(len(tokenized_inputs['input_ids'][i]))]  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "def dict_of_lists(lst_of_dicts):\n",
    "    result = defaultdict(list)\n",
    "    for d in lst_of_dicts:\n",
    "        for key, value in d.items():\n",
    "            result[key].append(value)\n",
    "    return dict(result)\n",
    "\n",
    "def list_of_dicts(dict_of_lists):\n",
    "    # First, we need to check if all lists are of the same length to ensure correct transformation\n",
    "    if not all(len(lst) == len(next(iter(dict_of_lists.values()))) for lst in dict_of_lists.values()):\n",
    "        raise ValueError(\"All lists in the dictionary must have the same length\")\n",
    "\n",
    "    # Get the length of the items in any of the lists\n",
    "    length = len(next(iter(dict_of_lists.values())))\n",
    "    \n",
    "    # Create a list of dictionaries, one for each index in the lists\n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        # Create a dictionary for the current index 'i' across all lists\n",
    "        new_dict = {key: dict_of_lists[key][i] for key in dict_of_lists}\n",
    "        result.append(new_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sub_shift_spans(text, ents = [], mappings = []):\n",
    "    for mapping in mappings:\n",
    "        adjustment = 0\n",
    "        pattern = re.compile(mapping['pattern'])\n",
    "        for match in re.finditer(pattern, text):\n",
    "            match_index = match.start() + adjustment\n",
    "            match_contents = match.group()\n",
    "            if all(mapping['check'](char) for char in match_contents):\n",
    "                subbed_text = mapping['target'].replace('placeholder', match_contents)\n",
    "            else:\n",
    "                subbed_text = mapping['target']\n",
    "            len_diff = len(subbed_text) - len(match_contents)\n",
    "            text = text[:match_index] + subbed_text + text[match_index + len(match_contents):]\n",
    "            if ents:\n",
    "                if isinstance(ents, list):\n",
    "                    for ent in ents:\n",
    "                        if ent['start'] <= match_index and ent['end'] > match_index:\n",
    "                            ent['end'] += len_diff\n",
    "                        if ent['start'] > match_index:\n",
    "                            ent['start'] += len_diff\n",
    "                            ent['end'] += len_diff\n",
    "                elif isinstance(ents, dict):\n",
    "                    if ents['value']['start'] <= match_index and ents['value']['end'] > match_index:\n",
    "                        ents['value']['end'] += len_diff\n",
    "                    if ents['value']['start'] > match_index:\n",
    "                        ents['value']['start'] += len_diff\n",
    "                        ents['value']['end'] += len_diff\n",
    "\n",
    "            adjustment += len_diff\n",
    "\n",
    "    return text, ents\n",
    "\n",
    "def get_entities_from_sample(sample, field = 'annotations', langs = ['en'], sort = False):\n",
    "    entities = []\n",
    "    for lang in langs:\n",
    "        entities += [ent for ent in sample[field][0]['result'] if ent['type'] == 'labels' and ent[f'from_name'] == f'label_{lang}']\n",
    "    if sort:\n",
    "        entities = sorted(entities, key = lambda ent: ent['value']['start'])\n",
    "    return entities\n",
    "'''\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    # if not any([l for l in samples['annotations']]):\n",
    "        \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                if annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        if first:\n",
    "                            labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                            first = False\n",
    "                        elif not first:\n",
    "                            labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0]})\n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        # if the training sample has no tags that we need, we just produce a 0s list\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # if the training sample has tags we need, we first exclude the label lists whose tags don't match\n",
    "        # and then we merge the label lists that have tags that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    return samples_new\n",
    "'''\n",
    "\n",
    "def span_to_words_annotation(samples, target_tag = '', mappings = {}, labels_model = []):\n",
    "    samples_new = []\n",
    "    \n",
    "    for i in range(len(samples['data'])):\n",
    "        text, annotation_list = samples['data'][i]['text'], samples['annotations'][i][0]['result']\n",
    "        labels_text = []\n",
    "        tokens = []\n",
    "        if not annotation_list:\n",
    "            annotation_list = [[]]\n",
    "        for j, annotation in enumerate(annotation_list):\n",
    "            if isinstance(annotation, dict):\n",
    "                # Check if 'labels' is non-empty\n",
    "                if annotation['value']['labels'] and annotation['value']['labels'][0] != target_tag:\n",
    "                    continue\n",
    "            text_subshifted, ents = sub_shift_spans(text, annotation, mappings=mappings)\n",
    "            text_subshifted_matches = re.finditer(r'[^\\s]+', text_subshifted)\n",
    "            labels_words = []\n",
    "            first = True\n",
    "            for regex_match in text_subshifted_matches:\n",
    "                if j == 0:\n",
    "                    tokens.append(regex_match.group())\n",
    "                if isinstance(annotation, dict):\n",
    "                    if regex_match.start() < ents['value']['start']:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    elif regex_match.start() >= ents['value']['start'] and regex_match.end() <= ents['value']['end']:\n",
    "                        # Ensure the 'labels' list is not empty\n",
    "                        if isinstance(ents['value']['labels'], list) and ents['value']['labels']:\n",
    "                            if first:\n",
    "                                labels_words.append(labels_model.labels_to_id['B-' + ents['value']['labels'][0]])\n",
    "                                first = False\n",
    "                            else:\n",
    "                                labels_words.append(labels_model.labels_to_id['I-' + ents['value']['labels'][0]])\n",
    "                        else:\n",
    "                            # If 'labels' is empty, append 'O' or any fallback label\n",
    "                            labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    else:\n",
    "                        labels_words.append(labels_model.labels_to_id['O'])\n",
    "                    labels_text.append({'labels': labels_words, 'tag': annotation['value']['labels'][0] if 'labels' in annotation['value'] and annotation['value']['labels'] else 'no_tag'})\n",
    "        \n",
    "        allowed_labels = [labels_model.labels_to_id['O'],\n",
    "                          labels_model.labels_to_id['B-' + target_tag],\n",
    "                          labels_model.labels_to_id['I-' + target_tag],\n",
    "                          ]\n",
    "        \n",
    "        # If the training sample has no tags we need, produce a list of 0s\n",
    "        if target_tag not in [labels['tag'] for labels in labels_text]:\n",
    "            labels = [0] * len(tokens)\n",
    "            tag = 'no_tag'\n",
    "        # Otherwise, merge label lists that match the target tag\n",
    "        else:\n",
    "            labels = [max(values) for values in zip(*[labels['labels'] for labels in labels_text if labels['tag'] == target_tag])]\n",
    "            labels = [(label if label in allowed_labels else 0) for label in labels]\n",
    "            tag = target_tag\n",
    "            \n",
    "        samples_new.append({\n",
    "            'id': i,\n",
    "            'ner_tags': labels,\n",
    "            'tokens': tokens,\n",
    "            'tag': tag,\n",
    "        })\n",
    "        \n",
    "    return samples_new\n",
    "\n",
    "def make_balanced_df(df):\n",
    "    # get rows with annotations\n",
    "    df_pos = df[df['annotations'].apply(lambda x: len(x[0]['result']) > 0)]\n",
    "    # get the same number of rows without any annotations\n",
    "    df_neg = df[df['annotations'].apply(lambda x: x[0]['result'] == [])].sample(len(df_pos))\n",
    "    balanced_df = pd.concat([df_pos, df_neg])\n",
    "    return balanced_df\n",
    "\n",
    "def make_binary_balanced_df(df, target_tag='', labels_model=[]):\n",
    "    df_list = df.to_dict(orient='records')\n",
    "    df_list_binary = span_to_words_annotation(dict_of_lists(df_list), target_tag=target_tag, mappings=regex_tokenizer_mappings, labels_model=labels_model)\n",
    "    df_binary = pd.DataFrame(df_list_binary)\n",
    "    df_binary_pos = df_binary[df_binary['tag'] == target_tag]\n",
    "    df_binary_neg = df_binary[df_binary['tag'] != target_tag].sample(len(df_binary_pos), replace=True)  # Over-sampling\n",
    "    df_binary_subsampled = pd.concat([df_binary_pos, df_binary_neg])\n",
    "    return df_binary_subsampled\n",
    "\n",
    "regex_tokenizer_mappings = [\n",
    "    {'pattern': r'(?<!\\s)([^\\w\\s])|([^\\w\\s])(?!\\s)',\n",
    "    'target': ' placeholder ',\n",
    "    'check': lambda x: unicodedata.category(x).startswith('P'),\n",
    "    },\n",
    "    {'pattern': r'\\s+',\n",
    "     'target': ' ',\n",
    "     'check': lambda x: False if re.match('\\s+', x) is None else True,\n",
    "     },\n",
    "    ]\n",
    "\n",
    "def compute_metrics_wrapper(label_list, pt, model_name_simple, date_time, threshold=0):\n",
    "    def compute_metrics(eval_preds):\n",
    "        nonlocal label_list\n",
    "        nonlocal pt\n",
    "        nonlocal threshold\n",
    "        logits, labels = eval_preds\n",
    "        predictions = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        predictions = np.where(predictions >= threshold, predictions, 0)\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "        # Extract the true predictions and labels from the sequences\n",
    "        true_predictions = [\n",
    "            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        # Compute sequence-level evaluation metrics\n",
    "        results = classification_report(true_predictions, true_labels, output_dict=True)\n",
    "\n",
    "        # Flatten the lists to calculate micro F1-score and supports\n",
    "        flat_true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "        flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "        # Calculate micro F1-score using sklearn\n",
    "        micro_f1 = f1_score(flat_true_labels, flat_true_predictions, average='micro')\n",
    "\n",
    "        # Prepare the results dictionary\n",
    "        flat_results = {'micro_f1': float(micro_f1)}\n",
    "        \n",
    "        # Add detailed metrics for each label to the results dictionary\n",
    "        for label, metrics in results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric, value in metrics.items():\n",
    "                    flat_results[f'{label}_{metric}'] = float(value)\n",
    "\n",
    "        # Compute support for each label using Counter\n",
    "        label_support = Counter(flat_true_labels)\n",
    "        for label, count in label_support.items():\n",
    "            flat_results[f'{label}_support'] = count\n",
    "        \n",
    "        models_dir = '/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results'\n",
    "        model_save_name = f'{model_name_simple}_{tt[0]}_target={tt[1]}_aug_SUBSAMPLED_{date_time}'\n",
    "        model_save_dir = os.path.join(models_dir, date_time+'_aug_cw_ts0.9', model_save_name)\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            os.makedirs(model_save_dir)\n",
    "\n",
    "        with open(os.path.join(model_save_dir, 'results.json'), 'w', encoding='utf8') as f:\n",
    "            json.dump(flat_results, f, ensure_ascii = False)\n",
    "\n",
    "        return flat_results\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "data_gold = '/home/lgiordano/LUCA/checkthat_GITHUB/data/formatted/train_sentences.json'\n",
    "with open(data_gold, 'r', encoding='utf8') as f:\n",
    "    dataset_gold = json.load(f)\n",
    "\n",
    "data_path_dict = {\n",
    "'sl': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/sl/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-slv_Latn_tok_regex_en-sl/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-slv_Latn_tok_regex_en-sl_mdeberta-v3-base_mdeberta_xlwa_en-sl_ME3_2024-05-04-12-12-14_ls.json',\n",
    "'ru': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/ru/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-rus_Cyrl_tok_regex_en-ru/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-rus_Cyrl_tok_regex_en-ru_mdeberta-v3-base_mdeberta_xlwa_en-ru_ME3_2024-05-04-12-09-20_ls.json',\n",
    "'pt': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/pt/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-por_Latn_tok_regex_en-pt/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-por_Latn_tok_regex_en-pt_mdeberta-v3-base_mdeberta_xlwa_en-pt_ME3_2024-05-04-12-07-45_ls.json',\n",
    "'it': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/it/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-ita_Latn_tok_regex_en-it/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-ita_Latn_tok_regex_en-it_mdeberta-v3-base_mdeberta_xlwa_en-it_ME3_2024-05-04-12-05-00_ls.json',\n",
    "'es': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/es/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-spa_Latn_tok_regex_en-es/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-spa_Latn_tok_regex_en-es_mdeberta-v3-base_mdeberta_xlwa_en-es_ME3_2024-05-04-12-01-43_ls.json',\n",
    "'bg': '/home/lgiordano/LUCA/checkthat_GITHUB/data/train_sent_mt/bg/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-bul_Cyrl_tok_regex_en-bg/train_gold_sentences_translated_nllb-200-3.3B_eng_Latn-bul_Cyrl_tok_regex_en-bg_mdeberta-v3-base_mdeberta_xlwa_en-bg_ME3_2024-05-04-11-58-52_ls.json',\n",
    "#'ar': '/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/araieval24_all_bin_formatted.json'\n",
    "}\n",
    "\n",
    "dataset_aug = []\n",
    "for key in data_path_dict:\n",
    "    with open(data_path_dict[key], 'r', encoding='utf8') as f:\n",
    "        dataset_aug_buffer = json.load(f)\n",
    "\n",
    "        for sample in dataset_aug_buffer:\n",
    "            sample['annotations'][0]['result'] = get_entities_from_sample(sample, langs=[key], sort = True)\n",
    "            if 'text_en' in sample['data']:\n",
    "                del sample['data']['text_en']\n",
    "            if f'text_{key}' in sample['data']:\n",
    "                sample['data']['text'] = sample['data'][f'text_{key}']\n",
    "                del sample['data'][f'text_{key}']\n",
    "            sample['data']['lang'] = key\n",
    "            if 'labels' in sample['data']:\n",
    "                sample['data']['label'] = sample['data'].pop('labels')\n",
    "        dataset_aug += dataset_aug_buffer\n",
    "\n",
    "#dataset_aug += [sample for sample in json.load(open('/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/araieval24_all_bin_formatted.json')) if sample['data'].get('type') != 'tweet'] #filter out tweets from ar\n",
    "#semeval_24 = json.load(open('/home/lgiordano/LUCA/checkthat_GITHUB/data/aug_NEW/semeval24_all_bin_formatted.json', encoding='utf-8'))\n",
    "#dataset_aug += semeval_24\n",
    "\n",
    "df_gold = pd.DataFrame(dataset_gold)\n",
    "balanced_df_gold = make_balanced_df(df_gold)\n",
    "\n",
    "df_aug = pd.DataFrame(dataset_aug)\n",
    "balanced_df_aug = make_balanced_df(df_aug)\n",
    "\n",
    "target_tags = [\"Appeal_to_Authority\", \"Appeal_to_Popularity\",\"Appeal_to_Values\",\"Appeal_to_Fear-Prejudice\",\"Flag_Waving\",\"Causal_Oversimplification\",\n",
    "               \"False_Dilemma-No_Choice\",\"Consequential_Oversimplification\",\"Straw_Man\",\"Red_Herring\",\"Whataboutism\",\"Slogans\",\"Appeal_to_Time\",\n",
    "               \"Conversation_Killer\",\"Loaded_Language\",\"Repetition\",\"Exaggeration-Minimisation\",\"Obfuscation-Vagueness-Confusion\",\"Name_Calling-Labeling\",\n",
    "               \"Doubt\",\"Guilt_by_Association\",\"Appeal_to_Hypocrisy\",\"Questioning_the_Reputation\"]\n",
    "target_tags = [(i, el.strip()) for i, el in enumerate(target_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 0 of 23 for (0, 'Appeal_to_Authority') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2412 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2412/2412 [00:00<00:00, 3971.51 examples/s]\n",
      "Map: 100%|██████████| 274/274 [00:00<00:00, 4891.88 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2718' max='3020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2718/3020 08:29 < 00:56, 5.33 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Authority Precision</th>\n",
       "      <th>Appeal To Authority Recall</th>\n",
       "      <th>Appeal To Authority F1-score</th>\n",
       "      <th>Appeal To Authority Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Authority Support</th>\n",
       "      <th>I-appeal To Authority Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.328600</td>\n",
       "      <td>1.321609</td>\n",
       "      <td>0.696254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.761600</td>\n",
       "      <td>1.123420</td>\n",
       "      <td>0.669860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>1.587930</td>\n",
       "      <td>0.746914</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.113360</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.113360</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.113360</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.113360</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.278600</td>\n",
       "      <td>1.380706</td>\n",
       "      <td>0.754151</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>2.154739</td>\n",
       "      <td>0.770328</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>2.331102</td>\n",
       "      <td>0.773521</td>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.264957</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>2.476562</td>\n",
       "      <td>0.769689</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.238411</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.238411</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.238411</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.238411</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>2.781228</td>\n",
       "      <td>0.764155</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.249057</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.249057</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.249057</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.249057</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>2.749578</td>\n",
       "      <td>0.784802</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.266160</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.266160</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.266160</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>0.299145</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.266160</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>2653</td>\n",
       "      <td>117</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 1 of 23 for (1, 'Appeal_to_Popularity') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/888 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 888/888 [00:00<00:00, 5844.25 examples/s]\n",
      "Map: 100%|██████████| 136/136 [00:00<00:00, 3994.85 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='888' max='1110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 888/1110 02:00 < 00:30, 7.37 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Popularity Precision</th>\n",
       "      <th>Appeal To Popularity Recall</th>\n",
       "      <th>Appeal To Popularity F1-score</th>\n",
       "      <th>Appeal To Popularity Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Popularity Support</th>\n",
       "      <th>I-appeal To Popularity Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.242000</td>\n",
       "      <td>1.516741</td>\n",
       "      <td>0.758786</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.679900</td>\n",
       "      <td>1.297172</td>\n",
       "      <td>0.731629</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391500</td>\n",
       "      <td>1.350639</td>\n",
       "      <td>0.785410</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>2.042474</td>\n",
       "      <td>0.761448</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>2.543234</td>\n",
       "      <td>0.705005</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>2.216602</td>\n",
       "      <td>0.756124</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>3.011124</td>\n",
       "      <td>0.699148</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.179310</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.179310</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.179310</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.132653</td>\n",
       "      <td>0.179310</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>2.572029</td>\n",
       "      <td>0.754526</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>1210</td>\n",
       "      <td>47</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 2 of 23 for (2, 'Appeal_to_Values') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1107 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1107/1107 [00:00<00:00, 5057.20 examples/s]\n",
      "Map: 100%|██████████| 277/277 [00:00<00:00, 3643.17 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1390' max='1390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1390/1390 03:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Values Precision</th>\n",
       "      <th>Appeal To Values Recall</th>\n",
       "      <th>Appeal To Values F1-score</th>\n",
       "      <th>Appeal To Values Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Values Support</th>\n",
       "      <th>I-appeal To Values Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.153200</td>\n",
       "      <td>1.378629</td>\n",
       "      <td>0.665452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.806700</td>\n",
       "      <td>0.889606</td>\n",
       "      <td>0.712594</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>1.357672</td>\n",
       "      <td>0.690731</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.290100</td>\n",
       "      <td>1.503483</td>\n",
       "      <td>0.786609</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>1.419327</td>\n",
       "      <td>0.787520</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.129555</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.129555</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.129555</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.161616</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.129555</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>2.004062</td>\n",
       "      <td>0.805739</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>0.188976</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>2.105884</td>\n",
       "      <td>0.809383</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.159763</td>\n",
       "      <td>0.201493</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>2.236309</td>\n",
       "      <td>0.801412</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.162651</td>\n",
       "      <td>0.203774</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>2.447025</td>\n",
       "      <td>0.801867</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.155280</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>2.518376</td>\n",
       "      <td>0.803462</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>2584</td>\n",
       "      <td>99</td>\n",
       "      <td>1708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 3 of 23 for (3, 'Appeal_to_Fear-Prejudice') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/5893 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 5893/5893 [00:01<00:00, 3125.83 examples/s]\n",
      "Map: 100%|██████████| 625/625 [00:00<00:00, 5955.56 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6633' max='7370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6633/7370 18:47 < 02:05, 5.88 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Fear-prejudice Precision</th>\n",
       "      <th>Appeal To Fear-prejudice Recall</th>\n",
       "      <th>Appeal To Fear-prejudice F1-score</th>\n",
       "      <th>Appeal To Fear-prejudice Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Fear-prejudice Support</th>\n",
       "      <th>I-appeal To Fear-prejudice Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.061600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.780582</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.802069</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.120219</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.308000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.839245</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.200477</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.200477</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.200477</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.200477</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.845839</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.823215</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.280519</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.280519</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.280519</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.280519</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.835039</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.295567</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.295567</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.295567</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.269058</td>\n",
       "      <td>0.295567</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.840950</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.300716</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.300716</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.300716</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.266949</td>\n",
       "      <td>0.300716</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.834584</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.289786</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.289786</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.289786</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.289786</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.842997</td>\n",
       "      <td>0.338798</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.338798</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.338798</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>0.338798</td>\n",
       "      <td>0.262712</td>\n",
       "      <td>0.295943</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>5872</td>\n",
       "      <td>183</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 4 of 23 for (4, 'Flag_Waving') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/3947 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 3947/3947 [00:01<00:00, 3474.31 examples/s]\n",
      "Map: 100%|██████████| 285/285 [00:00<00:00, 5262.94 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4940' max='4940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4940/4940 14:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Flag Waving Precision</th>\n",
       "      <th>Flag Waving Recall</th>\n",
       "      <th>Flag Waving F1-score</th>\n",
       "      <th>Flag Waving Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-flag Waving Support</th>\n",
       "      <th>I-flag Waving Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.888600</td>\n",
       "      <td>1.011743</td>\n",
       "      <td>0.778069</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>1.010114</td>\n",
       "      <td>0.851871</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.301075</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.961017</td>\n",
       "      <td>0.837739</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.235849</td>\n",
       "      <td>0.253807</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.235849</td>\n",
       "      <td>0.253807</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.235849</td>\n",
       "      <td>0.253807</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.235849</td>\n",
       "      <td>0.253807</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>1.108881</td>\n",
       "      <td>0.838786</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>1.253254</td>\n",
       "      <td>0.856844</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>1.360889</td>\n",
       "      <td>0.857891</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.315315</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>1.499899</td>\n",
       "      <td>0.855797</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.409756</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.409756</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.409756</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.409756</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>1.610232</td>\n",
       "      <td>0.855535</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>1.586969</td>\n",
       "      <td>0.859461</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>1.684152</td>\n",
       "      <td>0.855012</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.407960</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.407960</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.407960</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.407960</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>91</td>\n",
       "      <td>1049</td>\n",
       "      <td>2681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 5 of 23 for (5, 'Causal_Oversimplification') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2823 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2823/2823 [00:00<00:00, 3120.97 examples/s]\n",
      "Map: 100%|██████████| 211/211 [00:00<00:00, 3470.24 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3530' max='3530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3530/3530 08:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Causal Oversimplification Precision</th>\n",
       "      <th>Causal Oversimplification Recall</th>\n",
       "      <th>Causal Oversimplification F1-score</th>\n",
       "      <th>Causal Oversimplification Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-causal Oversimplification Support</th>\n",
       "      <th>I-causal Oversimplification Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.215700</td>\n",
       "      <td>0.893156</td>\n",
       "      <td>0.784495</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.134529</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.134529</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.134529</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.134529</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>1.408405</td>\n",
       "      <td>0.749205</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>1.532689</td>\n",
       "      <td>0.792016</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>1.598698</td>\n",
       "      <td>0.774082</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.385417</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>1.397265</td>\n",
       "      <td>0.845820</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>1.655524</td>\n",
       "      <td>0.833382</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.450262</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.450262</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.450262</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.450262</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>1.773511</td>\n",
       "      <td>0.823836</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.486188</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>1.978438</td>\n",
       "      <td>0.818340</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>1.989614</td>\n",
       "      <td>0.840903</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>2.035455</td>\n",
       "      <td>0.839456</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>88</td>\n",
       "      <td>1886</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 6 of 23 for (6, 'False_Dilemma-No_Choice') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2476 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2476/2476 [00:00<00:00, 4237.03 examples/s]\n",
      "Map: 100%|██████████| 184/184 [00:00<00:00, 4867.78 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2480' max='3100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2480/3100 07:28 < 01:52, 5.53 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>False Dilemma-no Choice Precision</th>\n",
       "      <th>False Dilemma-no Choice Recall</th>\n",
       "      <th>False Dilemma-no Choice F1-score</th>\n",
       "      <th>False Dilemma-no Choice Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-false Dilemma-no Choice Support</th>\n",
       "      <th>I-false Dilemma-no Choice Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.070400</td>\n",
       "      <td>0.801214</td>\n",
       "      <td>0.757229</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.446000</td>\n",
       "      <td>1.261066</td>\n",
       "      <td>0.829679</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.268156</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>1.378751</td>\n",
       "      <td>0.827455</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>1.596518</td>\n",
       "      <td>0.846520</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>1.551690</td>\n",
       "      <td>0.849063</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>0.359788</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>0.359788</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>0.359788</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.323810</td>\n",
       "      <td>0.359788</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>1.820954</td>\n",
       "      <td>0.834128</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>1.986371</td>\n",
       "      <td>0.840801</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>1.922694</td>\n",
       "      <td>0.849380</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.405660</td>\n",
       "      <td>0.452632</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>1344</td>\n",
       "      <td>1719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 7 of 23 for (7, 'Consequential_Oversimplification') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/561 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 561/561 [00:00<00:00, 5561.46 examples/s]\n",
      "Map: 100%|██████████| 141/141 [00:00<00:00, 3623.24 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='710' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [710/710 01:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Consequential Oversimplification Precision</th>\n",
       "      <th>Consequential Oversimplification Recall</th>\n",
       "      <th>Consequential Oversimplification F1-score</th>\n",
       "      <th>Consequential Oversimplification Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-consequential Oversimplification Support</th>\n",
       "      <th>I-consequential Oversimplification Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.192900</td>\n",
       "      <td>0.703762</td>\n",
       "      <td>0.425559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.798038</td>\n",
       "      <td>0.824871</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.913189</td>\n",
       "      <td>0.864458</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>1.381363</td>\n",
       "      <td>0.818417</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>1.188033</td>\n",
       "      <td>0.845955</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.259542</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>1.212882</td>\n",
       "      <td>0.874355</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.207317</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>1.752921</td>\n",
       "      <td>0.850688</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>1.405001</td>\n",
       "      <td>0.870482</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>1.487077</td>\n",
       "      <td>0.867900</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.550221</td>\n",
       "      <td>0.863597</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>973</td>\n",
       "      <td>51</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 8 of 23 for (8, 'Straw_Man') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/728 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 728/728 [00:00<00:00, 3720.62 examples/s]\n",
      "Map: 100%|██████████| 128/128 [00:00<00:00, 5326.95 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='819' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [819/910 02:54 < 00:19, 4.68 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Straw Man Precision</th>\n",
       "      <th>Straw Man Recall</th>\n",
       "      <th>Straw Man F1-score</th>\n",
       "      <th>Straw Man Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-straw Man Support</th>\n",
       "      <th>I-straw Man Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.349100</td>\n",
       "      <td>1.037411</td>\n",
       "      <td>0.573438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>1.121676</td>\n",
       "      <td>0.729882</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>1.541505</td>\n",
       "      <td>0.755205</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>2.095742</td>\n",
       "      <td>0.786156</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.148649</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>2.213611</td>\n",
       "      <td>0.767023</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>3.067693</td>\n",
       "      <td>0.756894</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>2.935874</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>2.930820</td>\n",
       "      <td>0.782217</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>3.209661</td>\n",
       "      <td>0.767023</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>728</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 9 of 23 for (9, 'Red_Herring') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/842 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 842/842 [00:00<00:00, 5045.60 examples/s]\n",
      "Map: 100%|██████████| 76/76 [00:00<00:00, 3545.44 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='530' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 530/1060 01:16 < 01:16, 6.94 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Red Herring Precision</th>\n",
       "      <th>Red Herring Recall</th>\n",
       "      <th>Red Herring F1-score</th>\n",
       "      <th>Red Herring Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-red Herring Support</th>\n",
       "      <th>I-red Herring Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.201200</td>\n",
       "      <td>1.214582</td>\n",
       "      <td>0.700244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>835</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>1.316640</td>\n",
       "      <td>0.826158</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>835</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.829771</td>\n",
       "      <td>0.874086</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>835</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>1.750185</td>\n",
       "      <td>0.798538</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>835</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>1.272262</td>\n",
       "      <td>0.846466</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>835</td>\n",
       "      <td>23</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 10 of 23 for (10, 'Whataboutism') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/373 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 373/373 [00:00<00:00, 4453.86 examples/s]\n",
      "Map: 100%|██████████| 55/55 [00:00<00:00, 2900.30 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/470 00:46 < 00:31, 6.04 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Whataboutism Precision</th>\n",
       "      <th>Whataboutism Recall</th>\n",
       "      <th>Whataboutism F1-score</th>\n",
       "      <th>Whataboutism Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-whataboutism Support</th>\n",
       "      <th>I-whataboutism Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.451000</td>\n",
       "      <td>1.121069</td>\n",
       "      <td>0.396677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>1.195066</td>\n",
       "      <td>0.617861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>1.021496</td>\n",
       "      <td>0.753894</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>1.083045</td>\n",
       "      <td>0.798546</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>1.267629</td>\n",
       "      <td>0.796469</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>1.779776</td>\n",
       "      <td>0.800623</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>593</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 11 of 23 for (11, 'Slogans') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2629 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2629/2629 [00:00<00:00, 4780.93 examples/s]\n",
      "Map: 100%|██████████| 271/271 [00:00<00:00, 4623.41 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1974' max='3290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1974/3290 05:21 < 03:34, 6.14 it/s, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Slogans Precision</th>\n",
       "      <th>Slogans Recall</th>\n",
       "      <th>Slogans F1-score</th>\n",
       "      <th>Slogans Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-slogans Support</th>\n",
       "      <th>I-slogans Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689700</td>\n",
       "      <td>0.482358</td>\n",
       "      <td>0.883454</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2922</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.530620</td>\n",
       "      <td>0.906372</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.305419</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.305419</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.305419</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.319588</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.305419</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>2922</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>0.694472</td>\n",
       "      <td>0.906093</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.319328</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.319328</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.319328</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.319328</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>2922</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.680419</td>\n",
       "      <td>0.904975</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>2922</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.904416</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.401961</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2922</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.783813</td>\n",
       "      <td>0.908888</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.429268</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.429268</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.429268</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.453608</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.429268</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>2922</td>\n",
       "      <td>97</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 12 of 23 for (12, 'Appeal_to_Time') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/259 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 259/259 [00:00<00:00, 4615.04 examples/s]\n",
      "Map: 100%|██████████| 65/65 [00:00<00:00, 4206.86 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [297/330 00:49 < 00:05, 5.96 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Appeal To Time Precision</th>\n",
       "      <th>Appeal To Time Recall</th>\n",
       "      <th>Appeal To Time F1-score</th>\n",
       "      <th>Appeal To Time Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-appeal To Time Support</th>\n",
       "      <th>I-appeal To Time Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.340600</td>\n",
       "      <td>1.189686</td>\n",
       "      <td>0.631450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>1.202635</td>\n",
       "      <td>0.652334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>1.840097</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>1.945092</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>2.075658</td>\n",
       "      <td>0.788698</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>2.384689</td>\n",
       "      <td>0.757985</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>2.832501</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>3.082626</td>\n",
       "      <td>0.734644</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>2.974648</td>\n",
       "      <td>0.761671</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>514</td>\n",
       "      <td>27</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 13 of 23 for (13, 'Conversation_Killer') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/2633 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 2633/2633 [00:00<00:00, 5426.90 examples/s]\n",
      "Map: 100%|██████████| 389/389 [00:00<00:00, 4825.62 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2970' max='3300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2970/3300 07:13 < 00:48, 6.84 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Conversation Killer Precision</th>\n",
       "      <th>Conversation Killer Recall</th>\n",
       "      <th>Conversation Killer F1-score</th>\n",
       "      <th>Conversation Killer Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>B-conversation Killer Support</th>\n",
       "      <th>I-conversation Killer Support</th>\n",
       "      <th>O Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>1.376837</td>\n",
       "      <td>0.791142</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.092827</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>1.128862</td>\n",
       "      <td>0.847397</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.207101</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.207101</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.207101</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.207101</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>1.465284</td>\n",
       "      <td>0.837112</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.237918</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.237918</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.237918</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.237918</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>1.474508</td>\n",
       "      <td>0.850756</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>1.700255</td>\n",
       "      <td>0.848447</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>0.364261</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>0.364261</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>0.364261</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.368056</td>\n",
       "      <td>0.364261</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>1.613440</td>\n",
       "      <td>0.859992</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.328671</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.328671</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.328671</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.338129</td>\n",
       "      <td>0.328671</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>1.942927</td>\n",
       "      <td>0.855584</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.383481</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.383481</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.383481</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.383481</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>2.087618</td>\n",
       "      <td>0.856003</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.362018</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.362018</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.362018</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.362018</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>2.192946</td>\n",
       "      <td>0.850966</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.322404</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.322404</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.322404</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.322404</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>1038</td>\n",
       "      <td>3579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 14 of 23 for (14, 'Loaded_Language') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/37063 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 37063/37063 [00:11<00:00, 3259.31 examples/s]\n",
      "Map: 100%|██████████| 3077/3077 [00:01<00:00, 3021.26 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46330' max='46330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46330/46330 3:36:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Loaded Language Precision</th>\n",
       "      <th>Loaded Language Recall</th>\n",
       "      <th>Loaded Language F1-score</th>\n",
       "      <th>Loaded Language Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-loaded Language Support</th>\n",
       "      <th>I-loaded Language Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.591902</td>\n",
       "      <td>0.885960</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.645528</td>\n",
       "      <td>0.913141</td>\n",
       "      <td>0.155788</td>\n",
       "      <td>0.288245</td>\n",
       "      <td>0.202260</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>0.155788</td>\n",
       "      <td>0.288245</td>\n",
       "      <td>0.202260</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>0.155788</td>\n",
       "      <td>0.288245</td>\n",
       "      <td>0.202260</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>0.155788</td>\n",
       "      <td>0.288245</td>\n",
       "      <td>0.202260</td>\n",
       "      <td>621.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.589573</td>\n",
       "      <td>0.922236</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.375157</td>\n",
       "      <td>0.307297</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.375157</td>\n",
       "      <td>0.307297</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.375157</td>\n",
       "      <td>0.307297</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>0.260226</td>\n",
       "      <td>0.375157</td>\n",
       "      <td>0.307297</td>\n",
       "      <td>797.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.733265</td>\n",
       "      <td>0.924300</td>\n",
       "      <td>0.278503</td>\n",
       "      <td>0.424403</td>\n",
       "      <td>0.336311</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>0.278503</td>\n",
       "      <td>0.424403</td>\n",
       "      <td>0.336311</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>0.278503</td>\n",
       "      <td>0.424403</td>\n",
       "      <td>0.336311</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>0.278503</td>\n",
       "      <td>0.424403</td>\n",
       "      <td>0.336311</td>\n",
       "      <td>754.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.820128</td>\n",
       "      <td>0.924525</td>\n",
       "      <td>0.328111</td>\n",
       "      <td>0.460880</td>\n",
       "      <td>0.383325</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>0.328111</td>\n",
       "      <td>0.460880</td>\n",
       "      <td>0.383325</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>0.328111</td>\n",
       "      <td>0.460880</td>\n",
       "      <td>0.383325</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>0.328111</td>\n",
       "      <td>0.460880</td>\n",
       "      <td>0.383325</td>\n",
       "      <td>818.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.827661</td>\n",
       "      <td>0.926834</td>\n",
       "      <td>0.355091</td>\n",
       "      <td>0.442037</td>\n",
       "      <td>0.393822</td>\n",
       "      <td>923.000000</td>\n",
       "      <td>0.355091</td>\n",
       "      <td>0.442037</td>\n",
       "      <td>0.393822</td>\n",
       "      <td>923.000000</td>\n",
       "      <td>0.355091</td>\n",
       "      <td>0.442037</td>\n",
       "      <td>0.393822</td>\n",
       "      <td>923.000000</td>\n",
       "      <td>0.355091</td>\n",
       "      <td>0.442037</td>\n",
       "      <td>0.393822</td>\n",
       "      <td>923.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.069600</td>\n",
       "      <td>0.956238</td>\n",
       "      <td>0.926957</td>\n",
       "      <td>0.389904</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.415392</td>\n",
       "      <td>1008.000000</td>\n",
       "      <td>0.389904</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.415392</td>\n",
       "      <td>1008.000000</td>\n",
       "      <td>0.389904</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.415392</td>\n",
       "      <td>1008.000000</td>\n",
       "      <td>0.389904</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.415392</td>\n",
       "      <td>1008.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>1.045658</td>\n",
       "      <td>0.926262</td>\n",
       "      <td>0.383812</td>\n",
       "      <td>0.452308</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>975.000000</td>\n",
       "      <td>0.383812</td>\n",
       "      <td>0.452308</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>975.000000</td>\n",
       "      <td>0.383812</td>\n",
       "      <td>0.452308</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>975.000000</td>\n",
       "      <td>0.383812</td>\n",
       "      <td>0.452308</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>975.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>1.036601</td>\n",
       "      <td>0.927039</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>1086.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>1.151109</td>\n",
       "      <td>0.926569</td>\n",
       "      <td>0.412533</td>\n",
       "      <td>0.447170</td>\n",
       "      <td>0.429153</td>\n",
       "      <td>1060.000000</td>\n",
       "      <td>0.412533</td>\n",
       "      <td>0.447170</td>\n",
       "      <td>0.429153</td>\n",
       "      <td>1060.000000</td>\n",
       "      <td>0.412533</td>\n",
       "      <td>0.447170</td>\n",
       "      <td>0.429153</td>\n",
       "      <td>1060.000000</td>\n",
       "      <td>0.412533</td>\n",
       "      <td>0.447170</td>\n",
       "      <td>0.429153</td>\n",
       "      <td>1060.000000</td>\n",
       "      <td>43307</td>\n",
       "      <td>1149</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 15 of 23 for (15, 'Repetition') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/9247 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 9247/9247 [00:02<00:00, 3142.86 examples/s]\n",
      "Map: 100%|██████████| 445/445 [00:00<00:00, 5524.88 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10404' max='11560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10404/11560 52:12 < 05:48, 3.32 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Repetition Precision</th>\n",
       "      <th>Repetition Recall</th>\n",
       "      <th>Repetition F1-score</th>\n",
       "      <th>Repetition Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-repetition Support</th>\n",
       "      <th>I-repetition Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.535800</td>\n",
       "      <td>0.540047</td>\n",
       "      <td>0.920983</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.326848</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.326848</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.326848</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.326848</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.615932</td>\n",
       "      <td>0.930684</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.567797</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.636303</td>\n",
       "      <td>0.931936</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.573427</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.573427</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.573427</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.573427</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.682890</td>\n",
       "      <td>0.932405</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.787172</td>\n",
       "      <td>0.935378</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.626761</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.626761</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.626761</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>0.577922</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>0.626761</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.760790</td>\n",
       "      <td>0.937725</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.623377</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.941836</td>\n",
       "      <td>0.939133</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.975757</td>\n",
       "      <td>0.934283</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.640523</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.640523</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.640523</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.640523</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>1.025168</td>\n",
       "      <td>0.936317</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.668896</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.668896</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.668896</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.668896</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>5646</td>\n",
       "      <td>154</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 16 of 23 for (16, 'Exaggeration-Minimisation') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/7837 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 7837/7837 [00:02<00:00, 3395.85 examples/s]\n",
      "Map: 100%|██████████| 685/685 [00:00<00:00, 4760.09 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6860' max='9800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6860/9800 33:48 < 14:29, 3.38 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Exaggeration-minimisation Precision</th>\n",
       "      <th>Exaggeration-minimisation Recall</th>\n",
       "      <th>Exaggeration-minimisation F1-score</th>\n",
       "      <th>Exaggeration-minimisation Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-exaggeration-minimisation Support</th>\n",
       "      <th>I-exaggeration-minimisation Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.816200</td>\n",
       "      <td>0.933708</td>\n",
       "      <td>0.826865</td>\n",
       "      <td>0.024272</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.024272</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.024272</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.024272</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432100</td>\n",
       "      <td>0.933678</td>\n",
       "      <td>0.874673</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.295302</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.295302</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.295302</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0.213592</td>\n",
       "      <td>0.295302</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>1.189770</td>\n",
       "      <td>0.871639</td>\n",
       "      <td>0.276699</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.307278</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.276699</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.307278</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.276699</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.307278</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>0.276699</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.307278</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>1.488383</td>\n",
       "      <td>0.872999</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.352144</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.352144</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.352144</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>0.378641</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.352144</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>1.429829</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>1.739597</td>\n",
       "      <td>0.876347</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.395294</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.395294</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.395294</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.395294</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.814231</td>\n",
       "      <td>0.875196</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.398104</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.398104</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.398104</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.398104</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>7496</td>\n",
       "      <td>206</td>\n",
       "      <td>1857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 17 of 23 for (17, 'Obfuscation-Vagueness-Confusion') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/788 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 788/788 [00:00<00:00, 4250.68 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 4268.11 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [990/990 05:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Obfuscation-vagueness-confusion Precision</th>\n",
       "      <th>Obfuscation-vagueness-confusion Recall</th>\n",
       "      <th>Obfuscation-vagueness-confusion F1-score</th>\n",
       "      <th>Obfuscation-vagueness-confusion Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-obfuscation-vagueness-confusion Support</th>\n",
       "      <th>I-obfuscation-vagueness-confusion Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.390700</td>\n",
       "      <td>1.706384</td>\n",
       "      <td>0.639079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>1.205768</td>\n",
       "      <td>0.577865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.598200</td>\n",
       "      <td>1.994315</td>\n",
       "      <td>0.608717</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>2.719798</td>\n",
       "      <td>0.612635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>2.340461</td>\n",
       "      <td>0.700294</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>2.855759</td>\n",
       "      <td>0.663565</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>3.410347</td>\n",
       "      <td>0.659647</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>3.489111</td>\n",
       "      <td>0.670421</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>3.755701</td>\n",
       "      <td>0.659158</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>3.745851</td>\n",
       "      <td>0.675318</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>1138</td>\n",
       "      <td>47</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 18 of 23 for (18, 'Name_Calling-Labeling') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/20141 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 20141/20141 [00:06<00:00, 3054.48 examples/s]\n",
      "Map: 100%|██████████| 2205/2205 [00:00<00:00, 4209.96 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25180' max='25180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25180/25180 1:41:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Name Calling-labeling Precision</th>\n",
       "      <th>Name Calling-labeling Recall</th>\n",
       "      <th>Name Calling-labeling F1-score</th>\n",
       "      <th>Name Calling-labeling Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-name Calling-labeling Support</th>\n",
       "      <th>I-name Calling-labeling Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.339445</td>\n",
       "      <td>0.924685</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.052925</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.052925</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.052925</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.052925</td>\n",
       "      <td>0.032340</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.401619</td>\n",
       "      <td>0.929431</td>\n",
       "      <td>0.106618</td>\n",
       "      <td>0.279743</td>\n",
       "      <td>0.154392</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>0.106618</td>\n",
       "      <td>0.279743</td>\n",
       "      <td>0.154392</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>0.106618</td>\n",
       "      <td>0.279743</td>\n",
       "      <td>0.154392</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>0.106618</td>\n",
       "      <td>0.279743</td>\n",
       "      <td>0.154392</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.364319</td>\n",
       "      <td>0.947035</td>\n",
       "      <td>0.324755</td>\n",
       "      <td>0.419304</td>\n",
       "      <td>0.366022</td>\n",
       "      <td>632.000000</td>\n",
       "      <td>0.324755</td>\n",
       "      <td>0.419304</td>\n",
       "      <td>0.366022</td>\n",
       "      <td>632.000000</td>\n",
       "      <td>0.324755</td>\n",
       "      <td>0.419304</td>\n",
       "      <td>0.366022</td>\n",
       "      <td>632.000000</td>\n",
       "      <td>0.324755</td>\n",
       "      <td>0.419304</td>\n",
       "      <td>0.366022</td>\n",
       "      <td>632.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.945351</td>\n",
       "      <td>0.318627</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>0.373295</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>0.318627</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>0.373295</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>0.318627</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>0.373295</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>0.318627</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>0.373295</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.417791</td>\n",
       "      <td>0.950586</td>\n",
       "      <td>0.394608</td>\n",
       "      <td>0.477037</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>675.000000</td>\n",
       "      <td>0.394608</td>\n",
       "      <td>0.477037</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>675.000000</td>\n",
       "      <td>0.394608</td>\n",
       "      <td>0.477037</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>675.000000</td>\n",
       "      <td>0.394608</td>\n",
       "      <td>0.477037</td>\n",
       "      <td>0.431925</td>\n",
       "      <td>675.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.531878</td>\n",
       "      <td>0.950341</td>\n",
       "      <td>0.403186</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>0.459497</td>\n",
       "      <td>616.000000</td>\n",
       "      <td>0.403186</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>0.459497</td>\n",
       "      <td>616.000000</td>\n",
       "      <td>0.403186</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>0.459497</td>\n",
       "      <td>616.000000</td>\n",
       "      <td>0.403186</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>0.459497</td>\n",
       "      <td>616.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.575605</td>\n",
       "      <td>0.949668</td>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.481976</td>\n",
       "      <td>0.461342</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.481976</td>\n",
       "      <td>0.461342</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.481976</td>\n",
       "      <td>0.461342</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>0.442402</td>\n",
       "      <td>0.481976</td>\n",
       "      <td>0.461342</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.652662</td>\n",
       "      <td>0.951566</td>\n",
       "      <td>0.463235</td>\n",
       "      <td>0.519945</td>\n",
       "      <td>0.489955</td>\n",
       "      <td>727.000000</td>\n",
       "      <td>0.463235</td>\n",
       "      <td>0.519945</td>\n",
       "      <td>0.489955</td>\n",
       "      <td>727.000000</td>\n",
       "      <td>0.463235</td>\n",
       "      <td>0.519945</td>\n",
       "      <td>0.489955</td>\n",
       "      <td>727.000000</td>\n",
       "      <td>0.463235</td>\n",
       "      <td>0.519945</td>\n",
       "      <td>0.489955</td>\n",
       "      <td>727.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.675231</td>\n",
       "      <td>0.951535</td>\n",
       "      <td>0.448529</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>0.448529</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>0.448529</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>0.448529</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.478431</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.733243</td>\n",
       "      <td>0.952209</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.523745</td>\n",
       "      <td>0.497102</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.523745</td>\n",
       "      <td>0.497102</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.523745</td>\n",
       "      <td>0.497102</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>0.473039</td>\n",
       "      <td>0.523745</td>\n",
       "      <td>0.497102</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>29630</td>\n",
       "      <td>816</td>\n",
       "      <td>2217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model no. 19 of 23 for (19, 'Doubt') persuasion technique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/checkthat/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/13259 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 13259/13259 [00:03<00:00, 3317.35 examples/s]\n",
      "Map: 100%|██████████| 1731/1731 [00:00<00:00, 4857.68 examples/s]\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4975' max='16580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4975/16580 18:15 < 42:36, 4.54 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Doubt Precision</th>\n",
       "      <th>Doubt Recall</th>\n",
       "      <th>Doubt F1-score</th>\n",
       "      <th>Doubt Support</th>\n",
       "      <th>Micro avg Precision</th>\n",
       "      <th>Micro avg Recall</th>\n",
       "      <th>Micro avg F1-score</th>\n",
       "      <th>Micro avg Support</th>\n",
       "      <th>Macro avg Precision</th>\n",
       "      <th>Macro avg Recall</th>\n",
       "      <th>Macro avg F1-score</th>\n",
       "      <th>Macro avg Support</th>\n",
       "      <th>Weighted avg Precision</th>\n",
       "      <th>Weighted avg Recall</th>\n",
       "      <th>Weighted avg F1-score</th>\n",
       "      <th>Weighted avg Support</th>\n",
       "      <th>O Support</th>\n",
       "      <th>B-doubt Support</th>\n",
       "      <th>I-doubt Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.161200</td>\n",
       "      <td>0.976914</td>\n",
       "      <td>0.639543</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>17252</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.740100</td>\n",
       "      <td>1.176096</td>\n",
       "      <td>0.722396</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.113487</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.113487</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.113487</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.113487</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>17252</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>1.276056</td>\n",
       "      <td>0.757573</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.171014</td>\n",
       "      <td>0.167614</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.171014</td>\n",
       "      <td>0.167614</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.171014</td>\n",
       "      <td>0.167614</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.171014</td>\n",
       "      <td>0.167614</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>17252</td>\n",
       "      <td>718</td>\n",
       "      <td>11709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift = 0\n",
    "for i, tt in enumerate(target_tags):\n",
    "    if i < shift:\n",
    "        continue\n",
    "    print(f'Training model no. {i} of {len(target_tags)} for {tt} persuasion technique...')\n",
    "    labels_model = LabelSet(labels=[tt[1]])\n",
    "    \n",
    "    token_columns = ['id', 'ner_tags', 'tokens', \n",
    "                     #'lang'\n",
    "                     ]\n",
    "\n",
    "    df_binary_subsampled_gold = make_binary_balanced_df(balanced_df_gold, target_tag=tt[1], labels_model=labels_model)\n",
    "    binary_dataset_gold = Dataset.from_pandas(df_binary_subsampled_gold[token_columns])\n",
    "\n",
    "    df_binary_subsampled_aug = make_binary_balanced_df(balanced_df_aug, target_tag=tt[1], labels_model=labels_model)\n",
    "    binary_dataset_aug = Dataset.from_pandas(df_binary_subsampled_aug[token_columns])\n",
    "    \n",
    "    split_ratio = 0.2\n",
    "    split_seed = 42\n",
    "    datadict = binary_dataset_gold.train_test_split(split_ratio, seed=split_seed)\n",
    "\n",
    "    #model_name = 'bert-base-multilingual-cased'\n",
    "    #model_name = 'xlm-roberta-base'\n",
    "    model_name = 'microsoft/mdeberta-v3-base'\n",
    "    #model_name = 'FacebookAI/xlm-roberta-large'\n",
    "    model_name_simple = model_name.split('/')[-1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    batch_size = 16\n",
    "    datadict['train'] = concatenate_datasets([datadict['train'], binary_dataset_aug]) # this is where we merge english gold data with aug data\n",
    "    datadict = datadict.map(lambda x: tokenize_token_classification(x, tokenizer), batched=True, batch_size=None)\n",
    "\n",
    "    columns = [\n",
    "                'input_ids',\n",
    "                'token_type_ids',\n",
    "                'attention_mask',\n",
    "                'labels'\n",
    "                ]\n",
    "\n",
    "    datadict.set_format('torch', columns = columns)\n",
    "\n",
    "    train_data = datadict['train']\n",
    "    val_data = datadict['test']\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding='longest')\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                                num_labels=len(labels_model.ids_to_label.values()),\n",
    "                                                                label2id=labels_model.labels_to_id,\n",
    "                                                                id2label=labels_model.ids_to_label,\n",
    "                                                                )\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=f'/home/lgiordano/LUCA/checkthat_GITHUB/models/M2/RUN_OTTOBRE/weights_and_results/{date_time}_aug_cw_ts0.9/mdeberta-v3-base-NEW_aug_{i}_{tt[1]}',\n",
    "                                  save_total_limit=2,\n",
    "                                  save_strategy='epoch',\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_only_model=True,\n",
    "                                  metric_for_best_model='eval_macro avg_f1-score',\n",
    "                                  logging_strategy='epoch',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=5e-5,\n",
    "                                  optim='adamw_torch',\n",
    "                                  num_train_epochs=10)\n",
    "    \n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "    ###CustomTrainer per class weighting, threshold=0.9 per threshold\n",
    "    trainer = CustomTrainer(model,\n",
    "                      training_args,\n",
    "                      train_dataset=train_data,\n",
    "                      eval_dataset=val_data,\n",
    "                      data_collator=data_collator,\n",
    "                      tokenizer=tokenizer,\n",
    "                      callbacks=[early_stopping],\n",
    "                      compute_metrics=compute_metrics_wrapper(\n",
    "                          label_list=[i for i in labels_model.ids_to_label.values()],\n",
    "                          pt=tt[1],\n",
    "                          model_name_simple=model_name_simple,\n",
    "                          date_time=date_time,\n",
    "                          threshold=0.9\n",
    "                          ),\n",
    "                      )\n",
    "    \n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
