{
    "results": [
        {
            "micro_f1": 0.669971671388102,
            "Appeal_to_Authority_precision": 0.05128205128205128,
            "Appeal_to_Authority_recall": 0.014742014742014743,
            "Appeal_to_Authority_f1-score": 0.022900763358778626,
            "Appeal_to_Authority_support": 407,
            "micro avg_precision": 0.05128205128205128,
            "micro avg_recall": 0.014742014742014743,
            "micro avg_f1-score": 0.022900763358778626,
            "micro avg_support": 407,
            "macro avg_precision": 0.05128205128205128,
            "macro avg_recall": 0.014742014742014743,
            "macro avg_f1-score": 0.022900763358778626,
            "macro avg_support": 407,
            "weighted avg_precision": 0.05128205128205128,
            "weighted avg_recall": 0.014742014742014743,
            "weighted avg_f1-score": 0.022900763358778626,
            "weighted avg_support": 407,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 0
        },
        {
            "micro_f1": 0.6533791987049777,
            "Appeal_to_Authority_precision": 0.1111111111111111,
            "Appeal_to_Authority_recall": 0.033854166666666664,
            "Appeal_to_Authority_f1-score": 0.05189620758483034,
            "Appeal_to_Authority_support": 384,
            "micro avg_precision": 0.1111111111111111,
            "micro avg_recall": 0.033854166666666664,
            "micro avg_f1-score": 0.05189620758483034,
            "micro avg_support": 384,
            "macro avg_precision": 0.1111111111111111,
            "macro avg_recall": 0.033854166666666664,
            "macro avg_f1-score": 0.05189620758483034,
            "macro avg_support": 384,
            "weighted avg_precision": 0.1111111111111111,
            "weighted avg_recall": 0.033854166666666664,
            "weighted avg_f1-score": 0.051896207584830344,
            "weighted avg_support": 384,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 1
        },
        {
            "micro_f1": 0.7403885066774585,
            "Appeal_to_Authority_precision": 0.10256410256410256,
            "Appeal_to_Authority_recall": 0.06349206349206349,
            "Appeal_to_Authority_f1-score": 0.0784313725490196,
            "Appeal_to_Authority_support": 189,
            "micro avg_precision": 0.10256410256410256,
            "micro avg_recall": 0.06349206349206349,
            "micro avg_f1-score": 0.0784313725490196,
            "micro avg_support": 189,
            "macro avg_precision": 0.10256410256410256,
            "macro avg_recall": 0.06349206349206349,
            "macro avg_f1-score": 0.0784313725490196,
            "macro avg_support": 189,
            "weighted avg_precision": 0.10256410256410256,
            "weighted avg_recall": 0.06349206349206349,
            "weighted avg_f1-score": 0.0784313725490196,
            "weighted avg_support": 189,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 2
        },
        {
            "micro_f1": 0.707405908539053,
            "Appeal_to_Authority_precision": 0.18803418803418803,
            "Appeal_to_Authority_recall": 0.08906882591093117,
            "Appeal_to_Authority_f1-score": 0.12087912087912087,
            "Appeal_to_Authority_support": 247,
            "micro avg_precision": 0.18803418803418803,
            "micro avg_recall": 0.08906882591093117,
            "micro avg_f1-score": 0.12087912087912087,
            "micro avg_support": 247,
            "macro avg_precision": 0.18803418803418803,
            "macro avg_recall": 0.08906882591093117,
            "macro avg_f1-score": 0.12087912087912087,
            "macro avg_support": 247,
            "weighted avg_precision": 0.18803418803418803,
            "weighted avg_recall": 0.08906882591093117,
            "weighted avg_f1-score": 0.12087912087912087,
            "weighted avg_support": 247,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 3
        },
        {
            "micro_f1": 0.7061918251719952,
            "Appeal_to_Authority_precision": 0.18803418803418803,
            "Appeal_to_Authority_recall": 0.11055276381909548,
            "Appeal_to_Authority_f1-score": 0.13924050632911392,
            "Appeal_to_Authority_support": 199,
            "micro avg_precision": 0.18803418803418803,
            "micro avg_recall": 0.11055276381909548,
            "micro avg_f1-score": 0.13924050632911392,
            "micro avg_support": 199,
            "macro avg_precision": 0.18803418803418803,
            "macro avg_recall": 0.11055276381909548,
            "macro avg_f1-score": 0.13924050632911392,
            "macro avg_support": 199,
            "weighted avg_precision": 0.18803418803418803,
            "weighted avg_recall": 0.11055276381909548,
            "weighted avg_f1-score": 0.13924050632911392,
            "weighted avg_support": 199,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 4
        },
        {
            "micro_f1": 0.6616754350465399,
            "Appeal_to_Authority_precision": 0.21367521367521367,
            "Appeal_to_Authority_recall": 0.11848341232227488,
            "Appeal_to_Authority_f1-score": 0.15243902439024387,
            "Appeal_to_Authority_support": 211,
            "micro avg_precision": 0.21367521367521367,
            "micro avg_recall": 0.11848341232227488,
            "micro avg_f1-score": 0.15243902439024387,
            "micro avg_support": 211,
            "macro avg_precision": 0.21367521367521367,
            "macro avg_recall": 0.11848341232227488,
            "macro avg_f1-score": 0.15243902439024387,
            "macro avg_support": 211,
            "weighted avg_precision": 0.21367521367521367,
            "weighted avg_recall": 0.11848341232227488,
            "weighted avg_f1-score": 0.15243902439024387,
            "weighted avg_support": 211,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 5
        },
        {
            "micro_f1": 0.7341157426143262,
            "Appeal_to_Authority_precision": 0.23076923076923078,
            "Appeal_to_Authority_recall": 0.13636363636363635,
            "Appeal_to_Authority_f1-score": 0.1714285714285714,
            "Appeal_to_Authority_support": 198,
            "micro avg_precision": 0.23076923076923078,
            "micro avg_recall": 0.13636363636363635,
            "micro avg_f1-score": 0.1714285714285714,
            "micro avg_support": 198,
            "macro avg_precision": 0.23076923076923078,
            "macro avg_recall": 0.13636363636363635,
            "macro avg_f1-score": 0.1714285714285714,
            "macro avg_support": 198,
            "weighted avg_precision": 0.23076923076923078,
            "weighted avg_recall": 0.13636363636363635,
            "weighted avg_f1-score": 0.1714285714285714,
            "weighted avg_support": 198,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 6
        },
        {
            "micro_f1": 0.7023472278429785,
            "Appeal_to_Authority_precision": 0.23931623931623933,
            "Appeal_to_Authority_recall": 0.12962962962962962,
            "Appeal_to_Authority_f1-score": 0.16816816816816815,
            "Appeal_to_Authority_support": 216,
            "micro avg_precision": 0.23931623931623933,
            "micro avg_recall": 0.12962962962962962,
            "micro avg_f1-score": 0.16816816816816815,
            "micro avg_support": 216,
            "macro avg_precision": 0.23931623931623933,
            "macro avg_recall": 0.12962962962962962,
            "macro avg_f1-score": 0.16816816816816815,
            "macro avg_support": 216,
            "weighted avg_precision": 0.23931623931623933,
            "weighted avg_recall": 0.12962962962962962,
            "weighted avg_f1-score": 0.16816816816816815,
            "weighted avg_support": 216,
            "O_support": 2897,
            "B-Appeal_to_Authority_support": 117,
            "I-Appeal_to_Authority_support": 1928,
            "epoch": 7
        }
    ],
    "best_epoch": 7,
    "train_data_path": "./data/formatted/train_sentences_ls.json"
}